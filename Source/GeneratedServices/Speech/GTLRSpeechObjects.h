// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Google Cloud Speech API (speech/v1beta1)
// Description:
//   Google Cloud Speech API.
// Documentation:
//   https://cloud.google.com/speech/

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRSpeech_Context;
@class GTLRSpeech_Operation;
@class GTLRSpeech_OperationMetadata;
@class GTLRSpeech_OperationResponse;
@class GTLRSpeech_RecognitionAlternative;
@class GTLRSpeech_RecognitionAudio;
@class GTLRSpeech_RecognitionConfig;
@class GTLRSpeech_RecognitionResult;
@class GTLRSpeech_Status;
@class GTLRSpeech_StatusDetailsItem;

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionConfig.encoding

/**
 *  Adaptive Multi-Rate Narrowband codec. `sample_rate` must be 8000 Hz.
 *
 *  Value: "AMR"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Amr;
/**
 *  Adaptive Multi-Rate Wideband codec. `sample_rate` must be 16000 Hz.
 *
 *  Value: "AMR_WB"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_AmrWb;
/**
 *  Not specified. Will return result google.rpc.Code.INVALID_ARGUMENT.
 *
 *  Value: "ENCODING_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified;
/**
 *  This is the recommended encoding for `SyncRecognize` and
 *  `StreamingRecognize` because it uses lossless compression; therefore
 *  recognition accuracy is not compromised by a lossy codec.
 *  The stream FLAC (Free Lossless Audio Codec) encoding is specified at:
 *  http://flac.sourceforge.net/documentation.html.
 *  16-bit and 24-bit samples are supported.
 *  Not all fields in STREAMINFO are supported.
 *
 *  Value: "FLAC"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Flac;
/**
 *  Uncompressed 16-bit signed little-endian samples (Linear PCM).
 *  This is the only encoding that may be used by `AsyncRecognize`.
 *
 *  Value: "LINEAR16"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Linear16;
/**
 *  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
 *
 *  Value: "MULAW"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Mulaw;

/**
 *  `AsyncRecognizeRequest` is the top-level message sent by the client for
 *  the `AsyncRecognize` method.
 */
@interface GTLRSpeech_AsyncRecognizeRequest : GTLRObject

/** [Required] The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  [Required] The `config` message provides information to the recognizer
 *  that specifies how to process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The request message for Operations.CancelOperation.
 */
@interface GTLRSpeech_CancelOperationRequest : GTLRObject
@end


/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases
 *  in the results.
 */
@interface GTLRSpeech_Context : GTLRObject

/**
 *  [Optional] A list of strings containing words and phrases "hints" so that
 *  the speech recognition is more likely to recognize them. This can be used
 *  to improve the accuracy for specific words and phrases, for example, if
 *  specific commands are typically spoken by the user. This can also be used
 *  to add additional words to the vocabulary of the recognizer. See
 *  [usage limits](https://cloud.google.com/speech/limits#content).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  A generic empty message that you can re-use to avoid defining duplicated
 *  empty messages in your APIs. A typical example is to use it as the request
 *  or the response type of an API method. For instance:
 *  service Foo {
 *  rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);
 *  }
 *  The JSON representation for `Empty` is empty JSON object `{}`.
 */
@interface GTLRSpeech_Empty : GTLRObject
@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Operation *> *operations;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRSpeech_Operation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress.
 *  If true, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRSpeech_Status *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_OperationMetadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the
 *  `name` should have the format of `operations/some/unique/name`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_OperationResponse *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_OperationMetadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_OperationResponse : GTLRObject
@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRSpeech_RecognitionAlternative : GTLRObject

/**
 *  [Output-only] The confidence estimate between 0.0 and 1.0. A higher number
 *  means the system is more confident that the recognition is correct.
 *  This field is typically provided only for the top hypothesis, and only for
 *  `is_final=true` results.
 *  The default of 0.0 is a sentinel value indicating confidence was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  [Output-only] Transcript text representing the words that the user spoke.
 */
@property(nonatomic, copy, nullable) NSString *transcript;

@end


/**
 *  Contains audio data in the encoding specified in the `RecognitionConfig`.
 *  Either `content` or `uri` must be supplied. Supplying both or neither
 *  returns google.rpc.Code.INVALID_ARGUMENT. See
 *  [audio limits](https://cloud.google.com/speech/limits#content).
 */
@interface GTLRSpeech_RecognitionAudio : GTLRObject

/**
 *  The audio data bytes encoded as specified in
 *  `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 *  pure binary representation, whereas JSON representations use base64.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *content;

/**
 *  URI that points to a file that contains audio data bytes as specified in
 *  `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
 *  supported, which must be specified in the following format:
 *  `gs://bucket_name/object_name` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  The `RecognitionConfig` message provides information to the recognizer
 *  that specifies how to process the request.
 */
@interface GTLRSpeech_RecognitionConfig : GTLRObject

/**
 *  [Required] Encoding of audio data sent in all `RecognitionAudio` messages.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Amr Adaptive Multi-Rate
 *        Narrowband codec. `sample_rate` must be 8000 Hz. (Value: "AMR")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_AmrWb Adaptive Multi-Rate
 *        Wideband codec. `sample_rate` must be 16000 Hz. (Value: "AMR_WB")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified Not
 *        specified. Will return result google.rpc.Code.INVALID_ARGUMENT.
 *        (Value: "ENCODING_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Flac This is the
 *        recommended encoding for `SyncRecognize` and
 *        `StreamingRecognize` because it uses lossless compression; therefore
 *        recognition accuracy is not compromised by a lossy codec.
 *        The stream FLAC (Free Lossless Audio Codec) encoding is specified at:
 *        http://flac.sourceforge.net/documentation.html.
 *        16-bit and 24-bit samples are supported.
 *        Not all fields in STREAMINFO are supported. (Value: "FLAC")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Linear16 Uncompressed
 *        16-bit signed little-endian samples (Linear PCM).
 *        This is the only encoding that may be used by `AsyncRecognize`.
 *        (Value: "LINEAR16")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Mulaw 8-bit samples that
 *        compand 14-bit audio samples using G.711 PCMU/mu-law. (Value: "MULAW")
 */
@property(nonatomic, copy, nullable) NSString *encoding;

/**
 *  [Optional] The language of the supplied audio as a BCP-47 language tag.
 *  Example: "en-GB" https://www.rfc-editor.org/rfc/bcp/bcp47.txt
 *  If omitted, defaults to "en-US". See
 *  [Language Support](https://cloud.google.com/speech/docs/languages)
 *  for a list of the currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  [Optional] Maximum number of recognition hypotheses to be returned.
 *  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 *  within each `SpeechRecognitionResult`.
 *  The server may return fewer than `max_alternatives`.
 *  Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
 *  `1`. If omitted, defaults to `1`.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/**
 *  [Optional] If set to `true`, the server will attempt to filter out
 *  profanities, replacing all but the initial character in each filtered word
 *  with asterisks, e.g. "f***". If set to `false` or omitted, profanities
 *  won't be filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *profanityFilter;

/**
 *  [Required] Sample rate in Hertz of the audio data sent in all
 *  `RecognitionAudio` messages. Valid values are: 8000-48000.
 *  16000 is optimal. For best results, set the sampling rate of the audio
 *  source to 16000 Hz. If that's not possible, use the native sample rate of
 *  the audio source (instead of re-sampling).
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *sampleRate;

/** [Optional] A means to provide context to assist the speech recognition. */
@property(nonatomic, strong, nullable) GTLRSpeech_Context *speechContext;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRSpeech_RecognitionResult : GTLRObject

/**
 *  [Output-only] May contain one or more recognition hypotheses (up to the
 *  maximum specified in `max_alternatives`).
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionAlternative *> *alternatives;

@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different
 *  programming environments, including REST APIs and RPC APIs. It is used by
 *  [gRPC](https://github.com/grpc). The error model is designed to be:
 *  - Simple to use and understand for most users
 *  - Flexible enough to meet unexpected needs
 *  # Overview
 *  The `Status` message contains three pieces of data: error code, error
 *  message,
 *  and error details. The error code should be an enum value of
 *  google.rpc.Code, but it may accept additional error codes if needed. The
 *  error message should be a developer-facing English message that helps
 *  developers *understand* and *resolve* the error. If a localized user-facing
 *  error message is needed, put the localized message in the error details or
 *  localize it in the client. The optional error details may contain arbitrary
 *  information about the error. There is a predefined set of error detail types
 *  in the package `google.rpc` which can be used for common error conditions.
 *  # Language mapping
 *  The `Status` message is the logical representation of the error model, but
 *  it
 *  is not necessarily the actual wire format. When the `Status` message is
 *  exposed in different client libraries and different wire protocols, it can
 *  be
 *  mapped differently. For example, it will likely be mapped to some exceptions
 *  in Java, but more likely mapped to some error codes in C.
 *  # Other uses
 *  The error model and the `Status` message can be used in a variety of
 *  environments, either with or without APIs, to provide a
 *  consistent developer experience across different environments.
 *  Example uses of this error model include:
 *  - Partial errors. If a service needs to return partial errors to the client,
 *  it may embed the `Status` in the normal response to indicate the partial
 *  errors.
 *  - Workflow errors. A typical workflow has multiple steps. Each step may
 *  have a `Status` message for error reporting purpose.
 *  - Batch operations. If a client uses batch request and batch response, the
 *  `Status` message should be used directly inside batch response, one for
 *  each error sub-response.
 *  - Asynchronous operations. If an API call embeds asynchronous operation
 *  results in its response, the status of those operations should be
 *  represented directly using the `Status` message.
 *  - Logging. If some API errors are stored in logs, the message `Status` could
 *  be used directly after any stripping needed for security/privacy reasons.
 */
@interface GTLRSpeech_Status : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There will be a
 *  common set of message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_StatusDetailsItem *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRSpeech_StatusDetailsItem
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_StatusDetailsItem : GTLRObject
@end


/**
 *  `SyncRecognizeRequest` is the top-level message sent by the client for
 *  the `SyncRecognize` method.
 */
@interface GTLRSpeech_SyncRecognizeRequest : GTLRObject

/** [Required] The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  [Required] The `config` message provides information to the recognizer
 *  that specifies how to process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  `SyncRecognizeResponse` is the only message returned to the client by
 *  `SyncRecognize`. It contains the result as zero or more sequential
 *  `SpeechRecognitionResult` messages.
 */
@interface GTLRSpeech_SyncRecognizeResponse : GTLRObject

/**
 *  [Output-only] Sequential list of transcription results corresponding to
 *  sequential portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

@end

NS_ASSUME_NONNULL_END
