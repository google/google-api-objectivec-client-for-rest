// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Cloud Speech-to-Text API (speech/v1)
// Description:
//   Converts audio to text by applying powerful neural network models.
// Documentation:
//   https://cloud.google.com/speech-to-text/docs/quickstart-protocol

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRSpeech_Context;
@class GTLRSpeech_Operation;
@class GTLRSpeech_Operation_Metadata;
@class GTLRSpeech_Operation_Response;
@class GTLRSpeech_RecognitionAlternative;
@class GTLRSpeech_RecognitionAudio;
@class GTLRSpeech_RecognitionConfig;
@class GTLRSpeech_RecognitionMetadata;
@class GTLRSpeech_RecognitionResult;
@class GTLRSpeech_SpeakerDiarizationConfig;
@class GTLRSpeech_Status;
@class GTLRSpeech_Status_Details_Item;
@class GTLRSpeech_WordInfo;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionConfig.encoding

/**
 *  Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
 *
 *  Value: "AMR"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Amr;
/**
 *  Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
 *
 *  Value: "AMR_WB"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_AmrWb;
/**
 *  Not specified.
 *
 *  Value: "ENCODING_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified;
/**
 *  `FLAC` (Free Lossless Audio
 *  Codec) is the recommended encoding because it is
 *  lossless--therefore recognition is not compromised--and
 *  requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
 *  encoding supports 16-bit and 24-bit samples, however, not all fields in
 *  `STREAMINFO` are supported.
 *
 *  Value: "FLAC"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Flac;
/**
 *  Uncompressed 16-bit signed little-endian samples (Linear PCM).
 *
 *  Value: "LINEAR16"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Linear16;
/**
 *  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
 *
 *  Value: "MULAW"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Mulaw;
/**
 *  Opus encoded audio frames in Ogg container
 *  ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *  `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
 *
 *  Value: "OGG_OPUS"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_OggOpus;
/**
 *  Although the use of lossy encodings is not recommended, if a very low
 *  bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *  Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *  Cloud Speech API has a header byte in each block, as in MIME type
 *  `audio/x-speex-with-header-byte`.
 *  It is a variant of the RTP Speex encoding defined in
 *  [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *  The stream is a sequence of blocks, one block per RTP packet. Each block
 *  starts with a byte containing the length of the block, in bytes, followed
 *  by one or more frames of Speex data, padded to an integral number of
 *  bytes (octets) as specified in RFC 5574. In other words, each RTP header
 *  is replaced with a single byte containing the block length. Only Speex
 *  wideband is supported. `sample_rate_hertz` must be 16000.
 *
 *  Value: "SPEEX_WITH_HEADER_BYTE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.interactionType

/**
 *  Transcribe speech to text to create a written document, such as a
 *  text-message, email or report.
 *
 *  Value: "DICTATION"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Dictation;
/**
 *  Multiple people in a conversation or discussion. For example in a
 *  meeting with two or more people actively participating. Typically
 *  all the primary people speaking would be in the same room (if not,
 *  see PHONE_CALL)
 *
 *  Value: "DISCUSSION"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Discussion;
/**
 *  Use case is either unknown or is something other than one of the other
 *  values below.
 *
 *  Value: "INTERACTION_TYPE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_InteractionTypeUnspecified;
/**
 *  A phone-call or video-conference in which two or more people, who are
 *  not in the same room, are actively participating.
 *
 *  Value: "PHONE_CALL"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_PhoneCall;
/**
 *  One or more persons lecturing or presenting to others, mostly
 *  uninterrupted.
 *
 *  Value: "PRESENTATION"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Presentation;
/**
 *  Professionally produced audio (eg. TV Show, Podcast).
 *
 *  Value: "PROFESSIONALLY_PRODUCED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_ProfessionallyProduced;
/**
 *  Transcribe voice commands, such as for controlling a device.
 *
 *  Value: "VOICE_COMMAND"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceCommand;
/**
 *  A recorded message intended for another person to listen to.
 *
 *  Value: "VOICEMAIL"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Voicemail;
/**
 *  Transcribe spoken questions and queries into text.
 *
 *  Value: "VOICE_SEARCH"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceSearch;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.microphoneDistance

/**
 *  The speaker is more than 3 meters away from the microphone.
 *
 *  Value: "FARFIELD"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Farfield;
/**
 *  Audio type is not known.
 *
 *  Value: "MICROPHONE_DISTANCE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified;
/**
 *  The speaker if within 3 meters of the microphone.
 *
 *  Value: "MIDFIELD"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Midfield;
/**
 *  The audio was captured from a closely placed microphone. Eg. phone,
 *  dictaphone, or handheld microphone. Generally if there speaker is within
 *  1 meter of the microphone.
 *
 *  Value: "NEARFIELD"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Nearfield;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.originalMediaType

/**
 *  The speech data is an audio recording.
 *
 *  Value: "AUDIO"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Audio;
/**
 *  Unknown original media type.
 *
 *  Value: "ORIGINAL_MEDIA_TYPE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified;
/**
 *  The speech data originally recorded on a video.
 *
 *  Value: "VIDEO"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Video;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.recordingDeviceType

/**
 *  Speech was recorded indoors.
 *
 *  Value: "OTHER_INDOOR_DEVICE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice;
/**
 *  Speech was recorded outdoors.
 *
 *  Value: "OTHER_OUTDOOR_DEVICE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice;
/**
 *  Speech was recorded using a personal computer or tablet.
 *
 *  Value: "PC"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Pc;
/**
 *  Speech was recorded over a phone line.
 *
 *  Value: "PHONE_LINE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_PhoneLine;
/**
 *  The recording device is unknown.
 *
 *  Value: "RECORDING_DEVICE_TYPE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified;
/**
 *  Speech was recorded on a smartphone.
 *
 *  Value: "SMARTPHONE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Smartphone;
/**
 *  Speech was recorded in a vehicle.
 *
 *  Value: "VEHICLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Vehicle;

/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases
 *  in the results.
 */
@interface GTLRSpeech_Context : GTLRObject

/**
 *  A list of strings containing words and phrases "hints" so that
 *  the speech recognition is more likely to recognize them. This can be used
 *  to improve the accuracy for specific words and phrases, for example, if
 *  specific commands are typically spoken by the user. This can also be used
 *  to add additional words to the vocabulary of the recognizer. See
 *  [usage limits](https://cloud.google.com/speech-to-text/quotas#content).
 *  List items can also be set to classes for groups of words that represent
 *  common concepts that occur in natural language. For example, rather than
 *  providing phrase hints for every month of the year, using the $MONTH class
 *  improves the likelihood of correctly transcribing audio that includes
 *  months.
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Operation *> *operations;

@end


/**
 *  Describes the progress of a long-running `LongRunningRecognize` call. It is
 *  included in the `metadata` field of the `Operation` returned by the
 *  `GetOperation` call of the `google::longrunning::Operations` service.
 */
@interface GTLRSpeech_LongRunningRecognizeMetadata : GTLRObject

/** Time of the most recent processing update. */
@property(nonatomic, strong, nullable) GTLRDateTime *lastUpdateTime;

/**
 *  Approximate percentage of audio processed thus far. Guaranteed to be 100
 *  when the audio is fully processed and the results are available.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/**
 *  Output only. The URI of the audio file being transcribed. Empty if the audio
 *  was sent
 *  as byte content.
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  The top-level message sent by the client for the `LongRunningRecognize`
 *  method.
 */
@interface GTLRSpeech_LongRunningRecognizeRequest : GTLRObject

/** Required. The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  Required. Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The only message returned to the client by the `LongRunningRecognize`
 *  method.
 *  It contains the result as zero or more sequential `SpeechRecognitionResult`
 *  messages. It is included in the `result.response` field of the `Operation`
 *  returned by the `GetOperation` call of the `google::longrunning::Operations`
 *  service.
 */
@interface GTLRSpeech_LongRunningRecognizeResponse : GTLRObject

/**
 *  Sequential list of transcription results corresponding to
 *  sequential portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRSpeech_Operation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress.
 *  If `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRSpeech_Status *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the
 *  `name` should be a resource name ending with `operations/{unique_id}`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Metadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Response : GTLRObject
@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRSpeech_RecognitionAlternative : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative of a non-streaming
 *  result or, of a streaming result where `is_final=true`.
 *  This field is not guaranteed to be accurate and users should not rely on it
 *  to be always provided.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is true, you will see all the words
 *  from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_WordInfo *> *words;

@end


/**
 *  Contains audio data in the encoding specified in the `RecognitionConfig`.
 *  Either `content` or `uri` must be supplied. Supplying both or neither
 *  returns google.rpc.Code.INVALID_ARGUMENT. See
 *  [content limits](https://cloud.google.com/speech-to-text/quotas#content).
 */
@interface GTLRSpeech_RecognitionAudio : GTLRObject

/**
 *  The audio data bytes encoded as specified in
 *  `RecognitionConfig`. Note: as with all bytes fields, proto buffers use a
 *  pure binary representation, whereas JSON representations use base64.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *content;

/**
 *  URI that points to a file that contains audio data bytes as specified in
 *  `RecognitionConfig`. The file must not be compressed (for example, gzip).
 *  Currently, only Google Cloud Storage URIs are
 *  supported, which must be specified in the following format:
 *  `gs://bucket_name/object_name` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  Provides information to the recognizer that specifies how to process the
 *  request.
 */
@interface GTLRSpeech_RecognitionConfig : GTLRObject

/**
 *  The number of channels in the input audio data.
 *  ONLY set this for MULTI-CHANNEL recognition.
 *  Valid values for LINEAR16 and FLAC are `1`-`8`.
 *  Valid values for OGG_OPUS are '1'-'254'.
 *  Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only `1`.
 *  If `0` or omitted, defaults to one channel (mono).
 *  Note: We only recognize the first channel by default.
 *  To perform independent recognition on each channel set
 *  `enable_separate_recognition_per_channel` to 'true'.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *audioChannelCount;

/**
 *  Config to enable speaker diarization and set additional
 *  parameters to make diarization better suited for your application.
 *  Note: When this is enabled, we send all the words from the beginning of the
 *  audio for the top alternative in every consecutive STREAMING responses.
 *  This is done in order to improve our speaker tags as our models learn to
 *  identify the speakers in the conversation over time.
 *  For non-streaming requests, the diarization results will be provided only
 *  in the top alternative of the FINAL SpeechRecognitionResult.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_SpeakerDiarizationConfig *diarizationConfig;

/**
 *  If 'true', adds punctuation to recognition result hypotheses.
 *  This feature is only available in select languages. Setting this for
 *  requests in other languages has no effect at all.
 *  The default 'false' value does not add punctuation to result hypotheses.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableAutomaticPunctuation;

/**
 *  This needs to be set to `true` explicitly and `audio_channel_count` > 1
 *  to get each channel recognized separately. The recognition result will
 *  contain a `channel_tag` field to state which channel that result belongs
 *  to. If this is not true, we will only recognize the first channel. The
 *  request is billed cumulatively for all channels recognized:
 *  `audio_channel_count` multiplied by the length of the audio.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSeparateRecognitionPerChannel;

/**
 *  If `true`, the top result includes a list of words and
 *  the start and end time offsets (timestamps) for those words. If
 *  `false`, no word-level time offset information is returned. The default is
 *  `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordTimeOffsets;

/**
 *  Encoding of audio data sent in all `RecognitionAudio` messages.
 *  This field is optional for `FLAC` and `WAV` audio files and required
 *  for all other audio formats. For details, see AudioEncoding.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Amr Adaptive Multi-Rate
 *        Narrowband codec. `sample_rate_hertz` must be 8000. (Value: "AMR")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_AmrWb Adaptive Multi-Rate
 *        Wideband codec. `sample_rate_hertz` must be 16000. (Value: "AMR_WB")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified Not
 *        specified. (Value: "ENCODING_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Flac `FLAC` (Free Lossless
 *        Audio
 *        Codec) is the recommended encoding because it is
 *        lossless--therefore recognition is not compromised--and
 *        requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
 *        encoding supports 16-bit and 24-bit samples, however, not all fields
 *        in
 *        `STREAMINFO` are supported. (Value: "FLAC")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Linear16 Uncompressed
 *        16-bit signed little-endian samples (Linear PCM). (Value: "LINEAR16")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Mulaw 8-bit samples that
 *        compand 14-bit audio samples using G.711 PCMU/mu-law. (Value: "MULAW")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_OggOpus Opus encoded audio
 *        frames in Ogg container
 *        ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *        `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or
 *        48000. (Value: "OGG_OPUS")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte
 *        Although the use of lossy encodings is not recommended, if a very low
 *        bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *        Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *        Cloud Speech API has a header byte in each block, as in MIME type
 *        `audio/x-speex-with-header-byte`.
 *        It is a variant of the RTP Speex encoding defined in
 *        [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *        The stream is a sequence of blocks, one block per RTP packet. Each
 *        block
 *        starts with a byte containing the length of the block, in bytes,
 *        followed
 *        by one or more frames of Speex data, padded to an integral number of
 *        bytes (octets) as specified in RFC 5574. In other words, each RTP
 *        header
 *        is replaced with a single byte containing the block length. Only Speex
 *        wideband is supported. `sample_rate_hertz` must be 16000. (Value:
 *        "SPEEX_WITH_HEADER_BYTE")
 */
@property(nonatomic, copy, nullable) NSString *encoding;

/**
 *  Required. The language of the supplied audio as a
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 *  Example: "en-US".
 *  See [Language
 *  Support](https://cloud.google.com/speech-to-text/docs/languages) for a list
 *  of the currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  Maximum number of recognition hypotheses to be returned.
 *  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 *  within each `SpeechRecognitionResult`.
 *  The server may return fewer than `max_alternatives`.
 *  Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
 *  one. If omitted, will return a maximum of one.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/** Metadata regarding this request. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionMetadata *metadata;

/**
 *  Which model to select for the given request. Select the model
 *  best suited to your domain to get best results. If a model is not
 *  explicitly specified, then we auto-select a model based on the parameters
 *  in the RecognitionConfig.
 *  <table>
 *  <tr>
 *  <td><b>Model</b></td>
 *  <td><b>Description</b></td>
 *  </tr>
 *  <tr>
 *  <td><code>command_and_search</code></td>
 *  <td>Best for short queries such as voice commands or voice search.</td>
 *  </tr>
 *  <tr>
 *  <td><code>phone_call</code></td>
 *  <td>Best for audio that originated from a phone call (typically
 *  recorded at an 8khz sampling rate).</td>
 *  </tr>
 *  <tr>
 *  <td><code>video</code></td>
 *  <td>Best for audio that originated from from video or includes multiple
 *  speakers. Ideally the audio is recorded at a 16khz or greater
 *  sampling rate. This is a premium model that costs more than the
 *  standard rate.</td>
 *  </tr>
 *  <tr>
 *  <td><code>default</code></td>
 *  <td>Best for audio that is not one of the specific audio models.
 *  For example, long-form audio. Ideally the audio is high-fidelity,
 *  recorded at a 16khz or greater sampling rate.</td>
 *  </tr>
 *  </table>
 */
@property(nonatomic, copy, nullable) NSString *model;

/**
 *  If set to `true`, the server will attempt to filter out
 *  profanities, replacing all but the initial character in each filtered word
 *  with asterisks, e.g. "f***". If set to `false` or omitted, profanities
 *  won't be filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *profanityFilter;

/**
 *  Sample rate in Hertz of the audio data sent in all
 *  `RecognitionAudio` messages. Valid values are: 8000-48000.
 *  16000 is optimal. For best results, set the sampling rate of the audio
 *  source to 16000 Hz. If that's not possible, use the native sample rate of
 *  the audio source (instead of re-sampling).
 *  This field is optional for FLAC and WAV audio files, but is
 *  required for all other audio formats. For details, see AudioEncoding.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *sampleRateHertz;

/**
 *  Array of SpeechContext.
 *  A means to provide context to assist the speech recognition. For more
 *  information, see
 *  [speech
 *  adaptation](https://cloud.google.com/speech-to-text/docs/context-strength).
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Context *> *speechContexts;

/**
 *  Set to true to use an enhanced model for speech recognition.
 *  If `use_enhanced` is set to true and the `model` field is not set, then
 *  an appropriate enhanced model is chosen if an enhanced model exists for
 *  the audio.
 *  If `use_enhanced` is true and an enhanced version of the specified model
 *  does not exist, then the speech is recognized using the standard version
 *  of the specified model.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *useEnhanced;

@end


/**
 *  Description of audio data to be recognized.
 */
@interface GTLRSpeech_RecognitionMetadata : GTLRObject

/**
 *  Description of the content. Eg. "Recordings of federal supreme court
 *  hearings from 2012".
 */
@property(nonatomic, copy, nullable) NSString *audioTopic;

/**
 *  The industry vertical to which this speech recognition request most
 *  closely applies. This is most indicative of the topics contained
 *  in the audio. Use the 6-digit NAICS code to identify the industry
 *  vertical - see https://www.naics.com/search/.
 *
 *  Uses NSNumber of unsignedIntValue.
 */
@property(nonatomic, strong, nullable) NSNumber *industryNaicsCodeOfAudio;

/**
 *  The use case most closely describing the audio content to be recognized.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Dictation
 *        Transcribe speech to text to create a written document, such as a
 *        text-message, email or report. (Value: "DICTATION")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Discussion
 *        Multiple people in a conversation or discussion. For example in a
 *        meeting with two or more people actively participating. Typically
 *        all the primary people speaking would be in the same room (if not,
 *        see PHONE_CALL) (Value: "DISCUSSION")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_InteractionTypeUnspecified
 *        Use case is either unknown or is something other than one of the other
 *        values below. (Value: "INTERACTION_TYPE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_PhoneCall A
 *        phone-call or video-conference in which two or more people, who are
 *        not in the same room, are actively participating. (Value:
 *        "PHONE_CALL")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Presentation One
 *        or more persons lecturing or presenting to others, mostly
 *        uninterrupted. (Value: "PRESENTATION")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_ProfessionallyProduced
 *        Professionally produced audio (eg. TV Show, Podcast). (Value:
 *        "PROFESSIONALLY_PRODUCED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceCommand
 *        Transcribe voice commands, such as for controlling a device. (Value:
 *        "VOICE_COMMAND")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Voicemail A
 *        recorded message intended for another person to listen to. (Value:
 *        "VOICEMAIL")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceSearch
 *        Transcribe spoken questions and queries into text. (Value:
 *        "VOICE_SEARCH")
 */
@property(nonatomic, copy, nullable) NSString *interactionType;

/**
 *  The audio type that most closely describes the audio being recognized.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Farfield The
 *        speaker is more than 3 meters away from the microphone. (Value:
 *        "FARFIELD")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified
 *        Audio type is not known. (Value: "MICROPHONE_DISTANCE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Midfield The
 *        speaker if within 3 meters of the microphone. (Value: "MIDFIELD")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Nearfield The
 *        audio was captured from a closely placed microphone. Eg. phone,
 *        dictaphone, or handheld microphone. Generally if there speaker is
 *        within
 *        1 meter of the microphone. (Value: "NEARFIELD")
 */
@property(nonatomic, copy, nullable) NSString *microphoneDistance;

/**
 *  The original media the speech was recorded on.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Audio The speech
 *        data is an audio recording. (Value: "AUDIO")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified
 *        Unknown original media type. (Value:
 *        "ORIGINAL_MEDIA_TYPE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Video The speech
 *        data originally recorded on a video. (Value: "VIDEO")
 */
@property(nonatomic, copy, nullable) NSString *originalMediaType;

/**
 *  Mime type of the original audio file. For example `audio/m4a`,
 *  `audio/x-alaw-basic`, `audio/mp3`, `audio/3gpp`.
 *  A list of possible audio mime types is maintained at
 *  http://www.iana.org/assignments/media-types/media-types.xhtml#audio
 */
@property(nonatomic, copy, nullable) NSString *originalMimeType;

/**
 *  The device used to make the recording. Examples 'Nexus 5X' or
 *  'Polycom SoundStation IP 6000' or 'POTS' or 'VoIP' or
 *  'Cardioid Microphone'.
 */
@property(nonatomic, copy, nullable) NSString *recordingDeviceName;

/**
 *  The type of device the speech was recorded with.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice
 *        Speech was recorded indoors. (Value: "OTHER_INDOOR_DEVICE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice
 *        Speech was recorded outdoors. (Value: "OTHER_OUTDOOR_DEVICE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Pc Speech was
 *        recorded using a personal computer or tablet. (Value: "PC")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_PhoneLine
 *        Speech was recorded over a phone line. (Value: "PHONE_LINE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified
 *        The recording device is unknown. (Value:
 *        "RECORDING_DEVICE_TYPE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Smartphone
 *        Speech was recorded on a smartphone. (Value: "SMARTPHONE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Vehicle Speech
 *        was recorded in a vehicle. (Value: "VEHICLE")
 */
@property(nonatomic, copy, nullable) NSString *recordingDeviceType;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRSpeech_RecognitionResult : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the
 *  maximum specified in `max_alternatives`).
 *  These alternatives are ordered in terms of accuracy, with the top (first)
 *  alternative being the most probable, as ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionAlternative *> *alternatives;

/**
 *  For multi-channel audio, this is the channel number corresponding to the
 *  recognized result for the audio from that channel.
 *  For audio_channel_count = N, its output values can range from '1' to 'N'.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *channelTag;

@end


/**
 *  The top-level message sent by the client for the `Recognize` method.
 */
@interface GTLRSpeech_RecognizeRequest : GTLRObject

/** Required. The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  Required. Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The only message returned to the client by the `Recognize` method. It
 *  contains the result as zero or more sequential `SpeechRecognitionResult`
 *  messages.
 */
@interface GTLRSpeech_RecognizeResponse : GTLRObject

/**
 *  Sequential list of transcription results corresponding to
 *  sequential portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

@end


/**
 *  Config to enable speaker diarization.
 */
@interface GTLRSpeech_SpeakerDiarizationConfig : GTLRObject

/**
 *  If 'true', enables speaker detection for each recognized word in
 *  the top alternative of the recognition result using a speaker_tag provided
 *  in the WordInfo.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSpeakerDiarization;

/**
 *  Maximum number of speakers in the conversation. This range gives you more
 *  flexibility by allowing the system to automatically determine the correct
 *  number of speakers. If not set, the default value is 6.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxSpeakerCount;

/**
 *  Minimum number of speakers in the conversation. This range gives you more
 *  flexibility by allowing the system to automatically determine the correct
 *  number of speakers. If not set, the default value is 2.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *minSpeakerCount;

/**
 *  Output only. Unused.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different programming environments, including REST APIs and RPC APIs. It is
 *  used by [gRPC](https://github.com/grpc). Each `Status` message contains
 *  three pieces of data: error code, error message, and error details.
 *  You can find out more about this error model and how to work with it in the
 *  [API Design Guide](https://cloud.google.com/apis/design/errors).
 */
@interface GTLRSpeech_Status : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Status_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRSpeech_Status_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Status_Details_Item : GTLRObject
@end


/**
 *  Word-specific information for recognized words.
 */
@interface GTLRSpeech_WordInfo : GTLRObject

/**
 *  Time offset relative to the beginning of the audio,
 *  and corresponding to the end of the spoken word.
 *  This field is only set if `enable_word_time_offsets=true` and only
 *  in the top hypothesis.
 *  This is an experimental feature and the accuracy of the time offset can
 *  vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from '1' to diarization_speaker_count.
 *  speaker_tag is set if enable_speaker_diarization = 'true' and only in the
 *  top alternative.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio,
 *  and corresponding to the start of the spoken word.
 *  This field is only set if `enable_word_time_offsets=true` and only
 *  in the top hypothesis.
 *  This is an experimental feature and the accuracy of the time offset can
 *  vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
