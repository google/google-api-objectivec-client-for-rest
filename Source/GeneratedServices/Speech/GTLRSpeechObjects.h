// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Google Cloud Speech API (speech/v1)
// Description:
//   Converts audio to text by applying powerful neural network models.
// Documentation:
//   https://cloud.google.com/speech/

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRSpeech_Context;
@class GTLRSpeech_Operation;
@class GTLRSpeech_Operation_Metadata;
@class GTLRSpeech_Operation_Response;
@class GTLRSpeech_RecognitionAlternative;
@class GTLRSpeech_RecognitionAudio;
@class GTLRSpeech_RecognitionConfig;
@class GTLRSpeech_RecognitionResult;
@class GTLRSpeech_Status;
@class GTLRSpeech_Status_Details_Item;
@class GTLRSpeech_WordInfo;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionConfig.encoding

/**
 *  Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
 *
 *  Value: "AMR"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Amr;
/**
 *  Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
 *
 *  Value: "AMR_WB"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_AmrWb;
/**
 *  Not specified.
 *
 *  Value: "ENCODING_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified;
/**
 *  [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
 *  Codec) is the recommended encoding because it is
 *  lossless--therefore recognition is not compromised--and
 *  requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
 *  encoding supports 16-bit and 24-bit samples, however, not all fields in
 *  `STREAMINFO` are supported.
 *
 *  Value: "FLAC"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Flac;
/**
 *  Uncompressed 16-bit signed little-endian samples (Linear PCM).
 *
 *  Value: "LINEAR16"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Linear16;
/**
 *  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
 *
 *  Value: "MULAW"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Mulaw;
/**
 *  Opus encoded audio frames in Ogg container
 *  ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *  `sample_rate_hertz` must be 16000.
 *
 *  Value: "OGG_OPUS"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_OggOpus;
/**
 *  Although the use of lossy encodings is not recommended, if a very low
 *  bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *  Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *  Cloud Speech API has a header byte in each block, as in MIME type
 *  `audio/x-speex-with-header-byte`.
 *  It is a variant of the RTP Speex encoding defined in
 *  [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *  The stream is a sequence of blocks, one block per RTP packet. Each block
 *  starts with a byte containing the length of the block, in bytes, followed
 *  by one or more frames of Speex data, padded to an integral number of
 *  bytes (octets) as specified in RFC 5574. In other words, each RTP header
 *  is replaced with a single byte containing the block length. Only Speex
 *  wideband is supported. `sample_rate_hertz` must be 16000.
 *
 *  Value: "SPEEX_WITH_HEADER_BYTE"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte;

/**
 *  The request message for Operations.CancelOperation.
 */
@interface GTLRSpeech_CancelOperationRequest : GTLRObject
@end


/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases
 *  in the results.
 */
@interface GTLRSpeech_Context : GTLRObject

/**
 *  *Optional* A list of strings containing words and phrases "hints" so that
 *  the speech recognition is more likely to recognize them. This can be used
 *  to improve the accuracy for specific words and phrases, for example, if
 *  specific commands are typically spoken by the user. This can also be used
 *  to add additional words to the vocabulary of the recognizer. See
 *  [usage limits](https://cloud.google.com/speech/limits#content).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  A generic empty message that you can re-use to avoid defining duplicated
 *  empty messages in your APIs. A typical example is to use it as the request
 *  or the response type of an API method. For instance:
 *  service Foo {
 *  rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);
 *  }
 *  The JSON representation for `Empty` is empty JSON object `{}`.
 */
@interface GTLRSpeech_Empty : GTLRObject
@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Operation *> *operations;

@end


/**
 *  The top-level message sent by the client for the `LongRunningRecognize`
 *  method.
 */
@interface GTLRSpeech_LongRunningRecognizeRequest : GTLRObject

/** *Required* The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  *Required* Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRSpeech_Operation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress.
 *  If `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRSpeech_Status *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the
 *  `name` should have the format of `operations/some/unique/name`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Metadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Response : GTLRObject
@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRSpeech_RecognitionAlternative : GTLRObject

/**
 *  *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is typically provided only for the top hypothesis, and
 *  only for `is_final=true` results. Clients should not rely on the
 *  `confidence` field as it is not guaranteed to be accurate, or even set, in
 *  any of the results.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  *Output-only* Transcript text representing the words that the user spoke.
 */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  *Output-only* A list of word-specific information for each recognized word.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_WordInfo *> *words;

@end


/**
 *  Contains audio data in the encoding specified in the `RecognitionConfig`.
 *  Either `content` or `uri` must be supplied. Supplying both or neither
 *  returns google.rpc.Code.INVALID_ARGUMENT. See
 *  [audio limits](https://cloud.google.com/speech/limits#content).
 */
@interface GTLRSpeech_RecognitionAudio : GTLRObject

/**
 *  The audio data bytes encoded as specified in
 *  `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 *  pure binary representation, whereas JSON representations use base64.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *content;

/**
 *  URI that points to a file that contains audio data bytes as specified in
 *  `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
 *  supported, which must be specified in the following format:
 *  `gs://bucket_name/object_name` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  Provides information to the recognizer that specifies how to process the
 *  request.
 */
@interface GTLRSpeech_RecognitionConfig : GTLRObject

/**
 *  *Optional* If `true`, the top result includes a list of words and
 *  the start and end time offsets (timestamps) for those words. If
 *  `false`, no word-level time offset information is returned. The default is
 *  `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordTimeOffsets;

/**
 *  *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Amr Adaptive Multi-Rate
 *        Narrowband codec. `sample_rate_hertz` must be 8000. (Value: "AMR")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_AmrWb Adaptive Multi-Rate
 *        Wideband codec. `sample_rate_hertz` must be 16000. (Value: "AMR_WB")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified Not
 *        specified. (Value: "ENCODING_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Flac
 *        [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless
 *        Audio
 *        Codec) is the recommended encoding because it is
 *        lossless--therefore recognition is not compromised--and
 *        requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
 *        encoding supports 16-bit and 24-bit samples, however, not all fields
 *        in
 *        `STREAMINFO` are supported. (Value: "FLAC")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Linear16 Uncompressed
 *        16-bit signed little-endian samples (Linear PCM). (Value: "LINEAR16")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Mulaw 8-bit samples that
 *        compand 14-bit audio samples using G.711 PCMU/mu-law. (Value: "MULAW")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_OggOpus Opus encoded audio
 *        frames in Ogg container
 *        ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *        `sample_rate_hertz` must be 16000. (Value: "OGG_OPUS")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte
 *        Although the use of lossy encodings is not recommended, if a very low
 *        bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *        Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *        Cloud Speech API has a header byte in each block, as in MIME type
 *        `audio/x-speex-with-header-byte`.
 *        It is a variant of the RTP Speex encoding defined in
 *        [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *        The stream is a sequence of blocks, one block per RTP packet. Each
 *        block
 *        starts with a byte containing the length of the block, in bytes,
 *        followed
 *        by one or more frames of Speex data, padded to an integral number of
 *        bytes (octets) as specified in RFC 5574. In other words, each RTP
 *        header
 *        is replaced with a single byte containing the block length. Only Speex
 *        wideband is supported. `sample_rate_hertz` must be 16000. (Value:
 *        "SPEEX_WITH_HEADER_BYTE")
 */
@property(nonatomic, copy, nullable) NSString *encoding;

/**
 *  *Required* The language of the supplied audio as a
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 *  Example: "en-US".
 *  See [Language Support](https://cloud.google.com/speech/docs/languages)
 *  for a list of the currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  *Optional* Maximum number of recognition hypotheses to be returned.
 *  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 *  within each `SpeechRecognitionResult`.
 *  The server may return fewer than `max_alternatives`.
 *  Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
 *  one. If omitted, will return a maximum of one.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/**
 *  *Optional* If set to `true`, the server will attempt to filter out
 *  profanities, replacing all but the initial character in each filtered word
 *  with asterisks, e.g. "f***". If set to `false` or omitted, profanities
 *  won't be filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *profanityFilter;

/**
 *  *Required* Sample rate in Hertz of the audio data sent in all
 *  `RecognitionAudio` messages. Valid values are: 8000-48000.
 *  16000 is optimal. For best results, set the sampling rate of the audio
 *  source to 16000 Hz. If that's not possible, use the native sample rate of
 *  the audio source (instead of re-sampling).
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *sampleRateHertz;

/** *Optional* A means to provide context to assist the speech recognition. */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Context *> *speechContexts;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRSpeech_RecognitionResult : GTLRObject

/**
 *  *Output-only* May contain one or more recognition hypotheses (up to the
 *  maximum specified in `max_alternatives`).
 *  These alternatives are ordered in terms of accuracy, with the top (first)
 *  alternative being the most probable, as ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionAlternative *> *alternatives;

/**
 *  For multi-channel audio, this is the channel number corresponding to the
 *  recognized result for the audio from that channel.
 *  For audio_channel_count = N, its output values can range from '0' to 'N-1'.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *channelTag;

@end


/**
 *  The top-level message sent by the client for the `Recognize` method.
 */
@interface GTLRSpeech_RecognizeRequest : GTLRObject

/** *Required* The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  *Required* Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The only message returned to the client by the `Recognize` method. It
 *  contains the result as zero or more sequential `SpeechRecognitionResult`
 *  messages.
 */
@interface GTLRSpeech_RecognizeResponse : GTLRObject

/**
 *  *Output-only* Sequential list of transcription results corresponding to
 *  sequential portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different
 *  programming environments, including REST APIs and RPC APIs. It is used by
 *  [gRPC](https://github.com/grpc). The error model is designed to be:
 *  - Simple to use and understand for most users
 *  - Flexible enough to meet unexpected needs
 *  # Overview
 *  The `Status` message contains three pieces of data: error code, error
 *  message,
 *  and error details. The error code should be an enum value of
 *  google.rpc.Code, but it may accept additional error codes if needed. The
 *  error message should be a developer-facing English message that helps
 *  developers *understand* and *resolve* the error. If a localized user-facing
 *  error message is needed, put the localized message in the error details or
 *  localize it in the client. The optional error details may contain arbitrary
 *  information about the error. There is a predefined set of error detail types
 *  in the package `google.rpc` that can be used for common error conditions.
 *  # Language mapping
 *  The `Status` message is the logical representation of the error model, but
 *  it
 *  is not necessarily the actual wire format. When the `Status` message is
 *  exposed in different client libraries and different wire protocols, it can
 *  be
 *  mapped differently. For example, it will likely be mapped to some exceptions
 *  in Java, but more likely mapped to some error codes in C.
 *  # Other uses
 *  The error model and the `Status` message can be used in a variety of
 *  environments, either with or without APIs, to provide a
 *  consistent developer experience across different environments.
 *  Example uses of this error model include:
 *  - Partial errors. If a service needs to return partial errors to the client,
 *  it may embed the `Status` in the normal response to indicate the partial
 *  errors.
 *  - Workflow errors. A typical workflow has multiple steps. Each step may
 *  have a `Status` message for error reporting.
 *  - Batch operations. If a client uses batch request and batch response, the
 *  `Status` message should be used directly inside batch response, one for
 *  each error sub-response.
 *  - Asynchronous operations. If an API call embeds asynchronous operation
 *  results in its response, the status of those operations should be
 *  represented directly using the `Status` message.
 *  - Logging. If some API errors are stored in logs, the message `Status` could
 *  be used directly after any stripping needed for security/privacy reasons.
 */
@interface GTLRSpeech_Status : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Status_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRSpeech_Status_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Status_Details_Item : GTLRObject
@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such
 *  as `enable_word_time_offsets`.
 */
@interface GTLRSpeech_WordInfo : GTLRObject

/**
 *  *Output-only* Time offset relative to the beginning of the audio,
 *  and corresponding to the end of the spoken word.
 *  This field is only set if `enable_word_time_offsets=true` and only
 *  in the top hypothesis.
 *  This is an experimental feature and the accuracy of the time offset can
 *  vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  *Output-only* Time offset relative to the beginning of the audio,
 *  and corresponding to the start of the spoken word.
 *  This field is only set if `enable_word_time_offsets=true` and only
 *  in the top hypothesis.
 *  This is an experimental feature and the accuracy of the time offset can
 *  vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** *Output-only* The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
