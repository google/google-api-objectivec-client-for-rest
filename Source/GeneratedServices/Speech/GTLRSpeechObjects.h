// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Cloud Speech API (speech/v1)
// Description:
//   Converts audio to text by applying powerful neural network models.
// Documentation:
//   https://cloud.google.com/speech-to-text/docs/quickstart-protocol

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRSpeech_Context;
@class GTLRSpeech_Operation;
@class GTLRSpeech_Operation_Metadata;
@class GTLRSpeech_Operation_Response;
@class GTLRSpeech_RecognitionAlternative;
@class GTLRSpeech_RecognitionAudio;
@class GTLRSpeech_RecognitionConfig;
@class GTLRSpeech_RecognitionResult;
@class GTLRSpeech_Status;
@class GTLRSpeech_Status_Details_Item;
@class GTLRSpeech_WordInfo;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionConfig.encoding

/**
 *  Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
 *
 *  Value: "AMR"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Amr;
/**
 *  Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
 *
 *  Value: "AMR_WB"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_AmrWb;
/**
 *  Not specified.
 *
 *  Value: "ENCODING_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified;
/**
 *  `FLAC` (Free Lossless Audio
 *  Codec) is the recommended encoding because it is
 *  lossless--therefore recognition is not compromised--and
 *  requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
 *  encoding supports 16-bit and 24-bit samples, however, not all fields in
 *  `STREAMINFO` are supported.
 *
 *  Value: "FLAC"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Flac;
/**
 *  Uncompressed 16-bit signed little-endian samples (Linear PCM).
 *
 *  Value: "LINEAR16"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Linear16;
/**
 *  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
 *
 *  Value: "MULAW"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Mulaw;
/**
 *  Opus encoded audio frames in Ogg container
 *  ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *  `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
 *
 *  Value: "OGG_OPUS"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_OggOpus;
/**
 *  Although the use of lossy encodings is not recommended, if a very low
 *  bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *  Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *  Cloud Speech API has a header byte in each block, as in MIME type
 *  `audio/x-speex-with-header-byte`.
 *  It is a variant of the RTP Speex encoding defined in
 *  [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *  The stream is a sequence of blocks, one block per RTP packet. Each block
 *  starts with a byte containing the length of the block, in bytes, followed
 *  by one or more frames of Speex data, padded to an integral number of
 *  bytes (octets) as specified in RFC 5574. In other words, each RTP header
 *  is replaced with a single byte containing the block length. Only Speex
 *  wideband is supported. `sample_rate_hertz` must be 16000.
 *
 *  Value: "SPEEX_WITH_HEADER_BYTE"
 */
GTLR_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte;

/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases
 *  in the results.
 */
@interface GTLRSpeech_Context : GTLRObject

/**
 *  *Optional* A list of strings containing words and phrases "hints" so that
 *  the speech recognition is more likely to recognize them. This can be used
 *  to improve the accuracy for specific words and phrases, for example, if
 *  specific commands are typically spoken by the user. This can also be used
 *  to add additional words to the vocabulary of the recognizer. See
 *  [usage limits](/speech-to-text/quotas#content).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Operation *> *operations;

@end


/**
 *  Describes the progress of a long-running `LongRunningRecognize` call. It is
 *  included in the `metadata` field of the `Operation` returned by the
 *  `GetOperation` call of the `google::longrunning::Operations` service.
 */
@interface GTLRSpeech_LongRunningRecognizeMetadata : GTLRObject

/** Time of the most recent processing update. */
@property(nonatomic, strong, nullable) GTLRDateTime *lastUpdateTime;

/**
 *  Approximate percentage of audio processed thus far. Guaranteed to be 100
 *  when the audio is fully processed and the results are available.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

@end


/**
 *  The top-level message sent by the client for the `LongRunningRecognize`
 *  method.
 */
@interface GTLRSpeech_LongRunningRecognizeRequest : GTLRObject

/** *Required* The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  *Required* Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The only message returned to the client by the `LongRunningRecognize`
 *  method.
 *  It contains the result as zero or more sequential `SpeechRecognitionResult`
 *  messages. It is included in the `result.response` field of the `Operation`
 *  returned by the `GetOperation` call of the `google::longrunning::Operations`
 *  service.
 */
@interface GTLRSpeech_LongRunningRecognizeResponse : GTLRObject

/**
 *  Output only. Sequential list of transcription results corresponding to
 *  sequential portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRSpeech_Operation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress.
 *  If `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRSpeech_Status *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the
 *  `name` should have the format of `operations/some/unique/name`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Metadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Response : GTLRObject
@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRSpeech_RecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative of a non-streaming
 *  result or, of a streaming result where `is_final=true`.
 *  This field is not guaranteed to be accurate and users should not rely on it
 *  to be always provided.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Output only. Transcript text representing the words that the user spoke.
 */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is true, you will see all the words
 *  from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_WordInfo *> *words;

@end


/**
 *  Contains audio data in the encoding specified in the `RecognitionConfig`.
 *  Either `content` or `uri` must be supplied. Supplying both or neither
 *  returns google.rpc.Code.INVALID_ARGUMENT. See
 *  [content limits](/speech-to-text/quotas#content).
 */
@interface GTLRSpeech_RecognitionAudio : GTLRObject

/**
 *  The audio data bytes encoded as specified in
 *  `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
 *  pure binary representation, whereas JSON representations use base64.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *content;

/**
 *  URI that points to a file that contains audio data bytes as specified in
 *  `RecognitionConfig`. The file must not be compressed (for example, gzip).
 *  Currently, only Google Cloud Storage URIs are
 *  supported, which must be specified in the following format:
 *  `gs://bucket_name/object_name` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  Provides information to the recognizer that specifies how to process the
 *  request.
 */
@interface GTLRSpeech_RecognitionConfig : GTLRObject

/**
 *  *Optional* If 'true', adds punctuation to recognition result hypotheses.
 *  This feature is only available in select languages. Setting this for
 *  requests in other languages has no effect at all.
 *  The default 'false' value does not add punctuation to result hypotheses.
 *  Note: This is currently offered as an experimental service, complimentary
 *  to all users. In the future this may be exclusively available as a
 *  premium feature.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableAutomaticPunctuation;

/**
 *  This needs to be set to `true` explicitly and `audio_channel_count` > 1
 *  to get each channel recognized separately. The recognition result will
 *  contain a `channel_tag` field to state which channel that result belongs
 *  to. If this is not true, we will only recognize the first channel. The
 *  request is billed cumulatively for all channels recognized:
 *  `audio_channel_count` multiplied by the length of the audio.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSeparateRecognitionPerChannel;

/**
 *  *Optional* If `true`, the top result includes a list of words and
 *  the start and end time offsets (timestamps) for those words. If
 *  `false`, no word-level time offset information is returned. The default is
 *  `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordTimeOffsets;

/**
 *  Encoding of audio data sent in all `RecognitionAudio` messages.
 *  This field is optional for `FLAC` and `WAV` audio files and required
 *  for all other audio formats. For details, see AudioEncoding.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Amr Adaptive Multi-Rate
 *        Narrowband codec. `sample_rate_hertz` must be 8000. (Value: "AMR")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_AmrWb Adaptive Multi-Rate
 *        Wideband codec. `sample_rate_hertz` must be 16000. (Value: "AMR_WB")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified Not
 *        specified. (Value: "ENCODING_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Flac `FLAC` (Free Lossless
 *        Audio
 *        Codec) is the recommended encoding because it is
 *        lossless--therefore recognition is not compromised--and
 *        requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
 *        encoding supports 16-bit and 24-bit samples, however, not all fields
 *        in
 *        `STREAMINFO` are supported. (Value: "FLAC")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Linear16 Uncompressed
 *        16-bit signed little-endian samples (Linear PCM). (Value: "LINEAR16")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Mulaw 8-bit samples that
 *        compand 14-bit audio samples using G.711 PCMU/mu-law. (Value: "MULAW")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_OggOpus Opus encoded audio
 *        frames in Ogg container
 *        ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *        `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or
 *        48000. (Value: "OGG_OPUS")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte
 *        Although the use of lossy encodings is not recommended, if a very low
 *        bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *        Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *        Cloud Speech API has a header byte in each block, as in MIME type
 *        `audio/x-speex-with-header-byte`.
 *        It is a variant of the RTP Speex encoding defined in
 *        [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *        The stream is a sequence of blocks, one block per RTP packet. Each
 *        block
 *        starts with a byte containing the length of the block, in bytes,
 *        followed
 *        by one or more frames of Speex data, padded to an integral number of
 *        bytes (octets) as specified in RFC 5574. In other words, each RTP
 *        header
 *        is replaced with a single byte containing the block length. Only Speex
 *        wideband is supported. `sample_rate_hertz` must be 16000. (Value:
 *        "SPEEX_WITH_HEADER_BYTE")
 */
@property(nonatomic, copy, nullable) NSString *encoding;

/**
 *  *Required* The language of the supplied audio as a
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 *  Example: "en-US".
 *  See [Language Support](/speech-to-text/docs/languages)
 *  for a list of the currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  *Optional* Maximum number of recognition hypotheses to be returned.
 *  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 *  within each `SpeechRecognitionResult`.
 *  The server may return fewer than `max_alternatives`.
 *  Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
 *  one. If omitted, will return a maximum of one.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/**
 *  *Optional* Which model to select for the given request. Select the model
 *  best suited to your domain to get best results. If a model is not
 *  explicitly specified, then we auto-select a model based on the parameters
 *  in the RecognitionConfig.
 *  <table>
 *  <tr>
 *  <td><b>Model</b></td>
 *  <td><b>Description</b></td>
 *  </tr>
 *  <tr>
 *  <td><code>command_and_search</code></td>
 *  <td>Best for short queries such as voice commands or voice search.</td>
 *  </tr>
 *  <tr>
 *  <td><code>phone_call</code></td>
 *  <td>Best for audio that originated from a phone call (typically
 *  recorded at an 8khz sampling rate).</td>
 *  </tr>
 *  <tr>
 *  <td><code>video</code></td>
 *  <td>Best for audio that originated from from video or includes multiple
 *  speakers. Ideally the audio is recorded at a 16khz or greater
 *  sampling rate. This is a premium model that costs more than the
 *  standard rate.</td>
 *  </tr>
 *  <tr>
 *  <td><code>default</code></td>
 *  <td>Best for audio that is not one of the specific audio models.
 *  For example, long-form audio. Ideally the audio is high-fidelity,
 *  recorded at a 16khz or greater sampling rate.</td>
 *  </tr>
 *  </table>
 */
@property(nonatomic, copy, nullable) NSString *model;

/**
 *  *Optional* If set to `true`, the server will attempt to filter out
 *  profanities, replacing all but the initial character in each filtered word
 *  with asterisks, e.g. "f***". If set to `false` or omitted, profanities
 *  won't be filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *profanityFilter;

/**
 *  Sample rate in Hertz of the audio data sent in all
 *  `RecognitionAudio` messages. Valid values are: 8000-48000.
 *  16000 is optimal. For best results, set the sampling rate of the audio
 *  source to 16000 Hz. If that's not possible, use the native sample rate of
 *  the audio source (instead of re-sampling).
 *  This field is optional for `FLAC` and `WAV` audio files and required
 *  for all other audio formats. For details, see AudioEncoding.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *sampleRateHertz;

/**
 *  *Optional* array of SpeechContext.
 *  A means to provide context to assist the speech recognition. For more
 *  information, see [Phrase Hints](/speech-to-text/docs/basics#phrase-hints).
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Context *> *speechContexts;

/**
 *  *Optional* Set to true to use an enhanced model for speech recognition.
 *  If `use_enhanced` is set to true and the `model` field is not set, then
 *  an appropriate enhanced model is chosen if:
 *  1. project is eligible for requesting enhanced models
 *  2. an enhanced model exists for the audio
 *  If `use_enhanced` is true and an enhanced version of the specified model
 *  does not exist, then the speech is recognized using the standard version
 *  of the specified model.
 *  Enhanced speech models require that you opt-in to data logging using
 *  instructions in the
 *  [documentation](/speech-to-text/docs/enable-data-logging). If you set
 *  `use_enhanced` to true and you have not enabled audio logging, then you
 *  will receive an error.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *useEnhanced;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRSpeech_RecognitionResult : GTLRObject

/**
 *  Output only. May contain one or more recognition hypotheses (up to the
 *  maximum specified in `max_alternatives`).
 *  These alternatives are ordered in terms of accuracy, with the top (first)
 *  alternative being the most probable, as ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionAlternative *> *alternatives;

/**
 *  For multi-channel audio, this is the channel number corresponding to the
 *  recognized result for the audio from that channel.
 *  For audio_channel_count = N, its output values can range from '1' to 'N'.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *channelTag;

@end


/**
 *  The top-level message sent by the client for the `Recognize` method.
 */
@interface GTLRSpeech_RecognizeRequest : GTLRObject

/** *Required* The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  *Required* Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The only message returned to the client by the `Recognize` method. It
 *  contains the result as zero or more sequential `SpeechRecognitionResult`
 *  messages.
 */
@interface GTLRSpeech_RecognizeResponse : GTLRObject

/**
 *  Output only. Sequential list of transcription results corresponding to
 *  sequential portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different
 *  programming environments, including REST APIs and RPC APIs. It is used by
 *  [gRPC](https://github.com/grpc). The error model is designed to be:
 *  - Simple to use and understand for most users
 *  - Flexible enough to meet unexpected needs
 *  # Overview
 *  The `Status` message contains three pieces of data: error code, error
 *  message,
 *  and error details. The error code should be an enum value of
 *  google.rpc.Code, but it may accept additional error codes if needed. The
 *  error message should be a developer-facing English message that helps
 *  developers *understand* and *resolve* the error. If a localized user-facing
 *  error message is needed, put the localized message in the error details or
 *  localize it in the client. The optional error details may contain arbitrary
 *  information about the error. There is a predefined set of error detail types
 *  in the package `google.rpc` that can be used for common error conditions.
 *  # Language mapping
 *  The `Status` message is the logical representation of the error model, but
 *  it
 *  is not necessarily the actual wire format. When the `Status` message is
 *  exposed in different client libraries and different wire protocols, it can
 *  be
 *  mapped differently. For example, it will likely be mapped to some exceptions
 *  in Java, but more likely mapped to some error codes in C.
 *  # Other uses
 *  The error model and the `Status` message can be used in a variety of
 *  environments, either with or without APIs, to provide a
 *  consistent developer experience across different environments.
 *  Example uses of this error model include:
 *  - Partial errors. If a service needs to return partial errors to the client,
 *  it may embed the `Status` in the normal response to indicate the partial
 *  errors.
 *  - Workflow errors. A typical workflow has multiple steps. Each step may
 *  have a `Status` message for error reporting.
 *  - Batch operations. If a client uses batch request and batch response, the
 *  `Status` message should be used directly inside batch response, one for
 *  each error sub-response.
 *  - Asynchronous operations. If an API call embeds asynchronous operation
 *  results in its response, the status of those operations should be
 *  represented directly using the `Status` message.
 *  - Logging. If some API errors are stored in logs, the message `Status` could
 *  be used directly after any stripping needed for security/privacy reasons.
 */
@interface GTLRSpeech_Status : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Status_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRSpeech_Status_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Status_Details_Item : GTLRObject
@end


/**
 *  Word-specific information for recognized words.
 */
@interface GTLRSpeech_WordInfo : GTLRObject

/**
 *  Output only. Time offset relative to the beginning of the audio,
 *  and corresponding to the end of the spoken word.
 *  This field is only set if `enable_word_time_offsets=true` and only
 *  in the top hypothesis.
 *  This is an experimental feature and the accuracy of the time offset can
 *  vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. Time offset relative to the beginning of the audio,
 *  and corresponding to the start of the spoken word.
 *  This field is only set if `enable_word_time_offsets=true` and only
 *  in the top hypothesis.
 *  This is an experimental feature and the accuracy of the time offset can
 *  vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** Output only. The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
