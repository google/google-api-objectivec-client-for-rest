// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Cloud Video Intelligence API (videointelligence/v1)
// Description:
//   Detects objects, explicit content, and scene changes in videos. It also
//   specifies the region for annotation and transcribes speech to text.
// Documentation:
//   https://cloud.google.com/video-intelligence/docs/

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingPoly;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechContext;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscriptionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response;
@class GTLRCloudVideoIntelligence_GoogleRpcStatus;
@class GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest.features

/** Value: "EXPLICIT_CONTENT_DETECTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ExplicitContentDetection;
/** Value: "FEATURE_UNSPECIFIED" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_FeatureUnspecified;
/** Value: "LABEL_DETECTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_LabelDetection;
/** Value: "SHOT_CHANGE_DETECTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ShotChangeDetection;
/** Value: "SPEECH_TRANSCRIPTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_SpeechTranscription;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig.labelDetectionMode

/**
 *  Detect frame-level labels.
 *
 *  Value: "FRAME_MODE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_FrameMode;
/**
 *  Unspecified.
 *
 *  Value: "LABEL_DETECTION_MODE_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_LabelDetectionModeUnspecified;
/**
 *  Detect both shot-level and frame-level labels.
 *
 *  Value: "SHOT_AND_FRAME_MODE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotAndFrameMode;
/**
 *  Detect shot-level labels.
 *
 *  Value: "SHOT_MODE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotMode;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation request.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest : GTLRObject

/** Requested video annotation features. */
@property(nonatomic, strong, nullable) NSArray<NSString *> *features;

/**
 *  The video data bytes.
 *  If unset, the input video(s) should be specified via `input_uri`.
 *  If set, `input_uri` should be unset.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *inputContent;

/**
 *  Input video location. Currently, only
 *  [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
 *  supported, which must be specified in the following format:
 *  `gs://bucket-id/object-id` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](/storage/docs/reference-uris).
 *  A video URI may include wildcards in `object-id`, and thus identify
 *  multiple videos. Supported wildcards: '*' to match 0 or more characters;
 *  '?' to match 1 character. If unset, the input video should be embedded
 *  in the request as `input_content`. If set, `input_content` should be unset.
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Optional cloud region where annotation should take place. Supported cloud
 *  regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
 *  is specified, a region will be determined based on video file location.
 */
@property(nonatomic, copy, nullable) NSString *locationId;

/**
 *  Optional location where the output (in JSON format) should be stored.
 *  Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
 *  URIs are supported, which must be specified in the following format:
 *  `gs://bucket-id/object-id` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *outputUri;

/** Additional video context and/or feature-specific parameters. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext *videoContext;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame *> *frames;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment *> *segments;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechRecognitionAlternative : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is typically provided only for the top hypothesis, and
 *  only for `is_final=true` results. Clients should not rely on the
 *  `confidence` field as it is not guaranteed to be accurate or consistent.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/** A list of word-specific information for each recognized word. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of
 *  accuracy, with the top (first) alternative being the most probable, as
 *  ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
 *  language in this result. This language code was detected to have the most
 *  likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be
 *  100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation *explicitAnnotation;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *shotLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechTranscription *> *speechTranscriptions;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such
 *  as `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative.
 *  This field is not guaranteed to be accurate and users should not rely on it
 *  to be always provided.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame *> *frames;

@end


/**
 *  Config for EXPLICIT_CONTENT_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig : GTLRObject

/**
 *  Model to use for explicit content detection.
 *  Supported values: "builtin/stable" (the default if unset) and
 *  "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment *> *segments;

@end


/**
 *  Config for LABEL_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig : GTLRObject

/**
 *  What labels should be detected with LABEL_DETECTION, in addition to
 *  video-level labels or segment-level labels.
 *  If unspecified, defaults to `SHOT_MODE`.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_FrameMode
 *        Detect frame-level labels. (Value: "FRAME_MODE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_LabelDetectionModeUnspecified
 *        Unspecified. (Value: "LABEL_DETECTION_MODE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotAndFrameMode
 *        Detect both shot-level and frame-level labels. (Value:
 *        "SHOT_AND_FRAME_MODE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotMode
 *        Detect shot-level labels. (Value: "SHOT_MODE")
 */
@property(nonatomic, copy, nullable) NSString *labelDetectionMode;

/**
 *  Model to use for label detection.
 *  Supported values: "builtin/stable" (the default if unset) and
 *  "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

/**
 *  Whether the video has been shot from a stationary (i.e. non-moving) camera.
 *  When set to true, might improve detection accuracy for moving objects.
 *  Should be used with `SHOT_AND_FRAME_MODE` enabled.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *stationaryCamera;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

@end


/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame *> *frames;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment *> *segments;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is typically provided only for the top hypothesis, and
 *  only for `is_final=true` results. Clients should not rely on the
 *  `confidence` field as it is not guaranteed to be accurate or consistent.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/** A list of word-specific information for each recognized word. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of
 *  accuracy, with the top (first) alternative being the most probable, as
 *  ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
 *  language in this result. This language code was detected to have the most
 *  likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be
 *  100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation *explicitAnnotation;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *shotLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription *> *speechTranscriptions;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such
 *  as `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative.
 *  This field is not guaranteed to be accurate and users should not rely on it
 *  to be always provided.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame *> *frames;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelSegment *> *segments;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

@end


/**
 *  Normalized bounding box.
 *  The normalized vertex coordinates are relative to the original image.
 *  Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Normalized bounding polygon for text (that might not be aligned with axis).
 *  Contains list of the corner points in clockwise order starting from
 *  top-left corner. For example, for a rectangular bounding box:
 *  When the text is horizontal it might look like:
 *  0----1
 *  | |
 *  3----2
 *  When it's clockwise rotated 180 degrees around the top-left corner it
 *  becomes:
 *  2----3
 *  | |
 *  1----0
 *  and the vertex order will still be (0, 1, 2, 3). Note that values can be
 *  less
 *  than 0, or greater than 1 due to trignometric calculations for location of
 *  the box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingPoly : GTLRObject

/** Normalized vertices of the bounding polygon. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex *> *vertices;

@end


/**
 *  A vertex represents a 2D point in the image.
 *  NOTE: the normalized vertex coordinates are relative to the original image
 *  and range from 0 to 1.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end


/**
 *  Annotations corresponding to one tracked object.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingAnnotation : GTLRObject

/**
 *  Object category's labeling confidence of this track.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Entity to specify the object category that this track is labeled as. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *entity;

/**
 *  Information corresponding to all frames where this object track appears.
 *  Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
 *  messages in frames.
 *  Streaming mode: it can only be one ObjectTrackingFrame message in frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingFrame *> *frames;

/**
 *  Non-streaming batch mode ONLY.
 *  Each object track corresponds to one video segment where it appears.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

/**
 *  Streaming mode ONLY.
 *  In streaming mode, we do not know the end time of a tracked object
 *  before it is completed. Hence, there is no VideoSegment info returned.
 *  Instead, we provide a unique identifiable integer track_id so that
 *  the customers can correlate the results of the ongoing
 *  ObjectTrackAnnotation of the same track_id over time.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *trackId;

@end


/**
 *  Video frame level annotations for object detection and tracking. This field
 *  stores per frame location, time offset, and confidence.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingFrame : GTLRObject

/**
 *  The normalized bounding box location of this object track for the frame.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox *normalizedBoundingBox;

/** The timestamp of the frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechRecognitionAlternative : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is typically provided only for the top hypothesis, and
 *  only for `is_final=true` results. Clients should not rely on the
 *  `confidence` field as it is not guaranteed to be accurate or consistent.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/** A list of word-specific information for each recognized word. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of
 *  accuracy, with the top (first) alternative being the most probable, as
 *  ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
 *  language in this result. This language code was detected to have the most
 *  likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Annotations related to one detected OCR text snippet. This will contain the
 *  corresponding text, confidence value, and frame level information for each
 *  detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextAnnotation : GTLRObject

/** All video segments where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextSegment *> *segments;

/** The detected text. */
@property(nonatomic, copy, nullable) NSString *text;

@end


/**
 *  Video frame level annotation results for text annotation (OCR).
 *  Contains information regarding timestamp and bounding box locations for the
 *  frames containing detected OCR text snippets.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextFrame : GTLRObject

/** Bounding polygon of the detected text for this frame. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingPoly *rotatedBoundingBox;

/** Timestamp of this frame. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for text detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextSegment : GTLRObject

/**
 *  Confidence for the track of detected text. It is calculated as the highest
 *  over all frames where OCR detected text appears.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Information related to the frames where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextFrame *> *frames;

/** Video segment where a text snippet was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be
 *  100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentAnnotation *explicitAnnotation;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/** Annotations for list of objects detected and tracked in video. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingAnnotation *> *objectAnnotations;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *shotLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechTranscription *> *speechTranscriptions;

/**
 *  OCR text detection and tracking.
 *  Annotations for list of detected text snippets. Each will have list of
 *  frame information associated with it.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextAnnotation *> *textAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such
 *  as `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative.
 *  This field is not guaranteed to be accurate and users should not rely on it
 *  to be always provided.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Config for SHOT_CHANGE_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig : GTLRObject

/**
 *  Model to use for shot change detection.
 *  Supported values: "builtin/stable" (the default if unset) and
 *  "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases
 *  in the results.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechContext : GTLRObject

/**
 *  *Optional* A list of strings containing words and phrases "hints" so that
 *  the speech recognition is more likely to recognize them. This can be used
 *  to improve the accuracy for specific words and phrases, for example, if
 *  specific commands are typically spoken by the user. This can also be used
 *  to add additional words to the vocabulary of the recognizer. See
 *  [usage limits](https://cloud.google.com/speech/limits#content).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechRecognitionAlternative : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is typically provided only for the top hypothesis, and
 *  only for `is_final=true` results. Clients should not rely on the
 *  `confidence` field as it is not guaranteed to be accurate or consistent.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/** A list of word-specific information for each recognized word. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of
 *  accuracy, with the top (first) alternative being the most probable, as
 *  ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
 *  language in this result. This language code was detected to have the most
 *  likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Config for SPEECH_TRANSCRIPTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscriptionConfig : GTLRObject

/**
 *  *Optional* For file formats, such as MXF or MKV, supporting multiple audio
 *  tracks, specify up to two tracks. Default: track 0.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSArray<NSNumber *> *audioTracks;

/**
 *  *Optional*
 *  If set, specifies the estimated number of speakers in the conversation.
 *  If not set, defaults to '2'.
 *  Ignored unless enable_speaker_diarization is set to true.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *diarizationSpeakerCount;

/**
 *  *Optional* If 'true', adds punctuation to recognition result hypotheses.
 *  This feature is only available in select languages. Setting this for
 *  requests in other languages has no effect at all. The default 'false' value
 *  does not add punctuation to result hypotheses. NOTE: "This is currently
 *  offered as an experimental service, complimentary to all users. In the
 *  future this may be exclusively available as a premium feature."
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableAutomaticPunctuation;

/**
 *  *Optional* If 'true', enables speaker detection for each recognized word in
 *  the top alternative of the recognition result using a speaker_tag provided
 *  in the WordInfo.
 *  Note: When this is true, we send all the words from the beginning of the
 *  audio for the top alternative in every consecutive responses.
 *  This is done in order to improve our speaker tags as our models learn to
 *  identify the speakers in the conversation over time.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSpeakerDiarization;

/**
 *  *Optional* If `true`, the top result includes a list of words and the
 *  confidence for those words. If `false`, no word-level confidence
 *  information is returned. The default is `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordConfidence;

/**
 *  *Optional* If set to `true`, the server will attempt to filter out
 *  profanities, replacing all but the initial character in each filtered word
 *  with asterisks, e.g. "f***". If set to `false` or omitted, profanities
 *  won't be filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *filterProfanity;

/**
 *  *Required* The language of the supplied audio as a
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 *  Example: "en-US".
 *  See [Language Support](https://cloud.google.com/speech/docs/languages)
 *  for a list of the currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  *Optional* Maximum number of recognition hypotheses to be returned.
 *  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 *  within each `SpeechTranscription`. The server may return fewer than
 *  `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
 *  return a maximum of one. If omitted, will return a maximum of one.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/** *Optional* A means to provide context to assist the speech recognition. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechContext *> *speechContexts;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be
 *  100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation *explicitAnnotation;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *shotLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscription *> *speechTranscriptions;

@end


/**
 *  Video context and/or feature-specific parameters.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext : GTLRObject

/** Config for EXPLICIT_CONTENT_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig *explicitContentDetectionConfig;

/** Config for LABEL_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig *labelDetectionConfig;

/**
 *  Video segments to annotate. The segments may overlap and are not required
 *  to be contiguous or span the whole video. If unspecified, each video is
 *  treated as a single segment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *segments;

/** Config for SHOT_CHANGE_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig *shotChangeDetectionConfig;

/** Config for SPEECH_TRANSCRIPTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscriptionConfig *speechTranscriptionConfig;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such
 *  as `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative.
 *  This field is not guaranteed to be accurate and users should not rely on it
 *  to be always provided.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and
 *  corresponding to the start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  The request message for Operations.CancelOperation.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningCancelOperationRequest : GTLRObject
@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleLongrunningOperation *> *operations;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress.
 *  If `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the
 *  `name` should have the format of `operations/some/unique/name`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response : GTLRObject
@end


/**
 *  A generic empty message that you can re-use to avoid defining duplicated
 *  empty messages in your APIs. A typical example is to use it as the request
 *  or the response type of an API method. For instance:
 *  service Foo {
 *  rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);
 *  }
 *  The JSON representation for `Empty` is empty JSON object `{}`.
 */
@interface GTLRCloudVideoIntelligence_GoogleProtobufEmpty : GTLRObject
@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different
 *  programming environments, including REST APIs and RPC APIs. It is used by
 *  [gRPC](https://github.com/grpc). The error model is designed to be:
 *  - Simple to use and understand for most users
 *  - Flexible enough to meet unexpected needs
 *  # Overview
 *  The `Status` message contains three pieces of data: error code, error
 *  message,
 *  and error details. The error code should be an enum value of
 *  google.rpc.Code, but it may accept additional error codes if needed. The
 *  error message should be a developer-facing English message that helps
 *  developers *understand* and *resolve* the error. If a localized user-facing
 *  error message is needed, put the localized message in the error details or
 *  localize it in the client. The optional error details may contain arbitrary
 *  information about the error. There is a predefined set of error detail types
 *  in the package `google.rpc` that can be used for common error conditions.
 *  # Language mapping
 *  The `Status` message is the logical representation of the error model, but
 *  it
 *  is not necessarily the actual wire format. When the `Status` message is
 *  exposed in different client libraries and different wire protocols, it can
 *  be
 *  mapped differently. For example, it will likely be mapped to some exceptions
 *  in Java, but more likely mapped to some error codes in C.
 *  # Other uses
 *  The error model and the `Status` message can be used in a variety of
 *  environments, either with or without APIs, to provide a
 *  consistent developer experience across different environments.
 *  Example uses of this error model include:
 *  - Partial errors. If a service needs to return partial errors to the client,
 *  it may embed the `Status` in the normal response to indicate the partial
 *  errors.
 *  - Workflow errors. A typical workflow has multiple steps. Each step may
 *  have a `Status` message for error reporting.
 *  - Batch operations. If a client uses batch request and batch response, the
 *  `Status` message should be used directly inside batch response, one for
 *  each error sub-response.
 *  - Asynchronous operations. If an API call embeds asynchronous operation
 *  results in its response, the status of those operations should be
 *  represented directly using the `Status` message.
 *  - Logging. If some API errors are stored in logs, the message `Status` could
 *  be used directly after any stripping needed for security/privacy reasons.
 */
@interface GTLRCloudVideoIntelligence_GoogleRpcStatus : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item : GTLRObject
@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
