// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Cloud Video Intelligence API (videointelligence/v1)
// Description:
//   Detects objects, explicit content, and scene changes in videos. It also
//   specifies the region for annotation and transcribes speech to text.
//   Supports both asynchronous API and streaming API.
// Documentation:
//   https://cloud.google.com/video-intelligence/docs/

#if SWIFT_PACKAGE || GTLR_USE_MODULAR_IMPORT
  @import GoogleAPIClientForRESTCore;
#elif GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedLandmark;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LogoRecognitionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingPoly;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedVertex;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ObjectTrackingAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ObjectTrackingFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2PersonDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TimestampedObject;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Track;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedLandmark;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LogoRecognitionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingPoly;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedVertex;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedLandmark;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LogoRecognitionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingPoly;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedVertex;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ObjectTrackingAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ObjectTrackingFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1PersonDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TimestampedObject;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Track;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedLandmark;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LogoRecognitionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingPoly;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1PersonDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TimestampedObject;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Track;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Celebrity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1CelebrityRecognitionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1CelebrityTrack;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedLandmark;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LogoRecognitionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingPoly;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedVertex;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1PersonDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1RecognizedCelebrity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1StreamingVideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TimestampedObject;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Track;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1PersonDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1PersonDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechContext;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscriptionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TimestampedObject;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Track;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response;
@class GTLRCloudVideoIntelligence_GoogleRpcStatus;
@class GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest.features

/**
 *  Explicit content detection.
 *
 *  Value: "EXPLICIT_CONTENT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ExplicitContentDetection;
/**
 *  Human face detection.
 *
 *  Value: "FACE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_FaceDetection;
/**
 *  Unspecified.
 *
 *  Value: "FEATURE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_FeatureUnspecified;
/**
 *  Label detection. Detect objects, such as dog or flower.
 *
 *  Value: "LABEL_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_LabelDetection;
/**
 *  Logo detection, tracking, and recognition.
 *
 *  Value: "LOGO_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_LogoRecognition;
/**
 *  Object detection and tracking.
 *
 *  Value: "OBJECT_TRACKING"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ObjectTracking;
/**
 *  Person detection.
 *
 *  Value: "PERSON_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_PersonDetection;
/**
 *  Shot change detection.
 *
 *  Value: "SHOT_CHANGE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ShotChangeDetection;
/**
 *  Speech transcription.
 *
 *  Value: "SPEECH_TRANSCRIPTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_SpeechTranscription;
/**
 *  OCR text detection and tracking.
 *
 *  Value: "TEXT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_TextDetection;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress.feature

/**
 *  Explicit content detection.
 *
 *  Value: "EXPLICIT_CONTENT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_ExplicitContentDetection;
/**
 *  Human face detection.
 *
 *  Value: "FACE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_FaceDetection;
/**
 *  Unspecified.
 *
 *  Value: "FEATURE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_FeatureUnspecified;
/**
 *  Label detection. Detect objects, such as dog or flower.
 *
 *  Value: "LABEL_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_LabelDetection;
/**
 *  Logo detection, tracking, and recognition.
 *
 *  Value: "LOGO_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_LogoRecognition;
/**
 *  Object detection and tracking.
 *
 *  Value: "OBJECT_TRACKING"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_ObjectTracking;
/**
 *  Person detection.
 *
 *  Value: "PERSON_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_PersonDetection;
/**
 *  Shot change detection.
 *
 *  Value: "SHOT_CHANGE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_ShotChangeDetection;
/**
 *  Speech transcription.
 *
 *  Value: "SPEECH_TRANSCRIPTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_SpeechTranscription;
/**
 *  OCR text detection and tracking.
 *
 *  Value: "TEXT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_TextDetection;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig.labelDetectionMode

/**
 *  Detect frame-level labels.
 *
 *  Value: "FRAME_MODE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_FrameMode;
/**
 *  Unspecified.
 *
 *  Value: "LABEL_DETECTION_MODE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_LabelDetectionModeUnspecified;
/**
 *  Detect both shot-level and frame-level labels.
 *
 *  Value: "SHOT_AND_FRAME_MODE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotAndFrameMode;
/**
 *  Detect shot-level labels.
 *
 *  Value: "SHOT_MODE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotMode;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress.feature

/**
 *  Explicit content detection.
 *
 *  Value: "EXPLICIT_CONTENT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_ExplicitContentDetection;
/**
 *  Human face detection.
 *
 *  Value: "FACE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_FaceDetection;
/**
 *  Unspecified.
 *
 *  Value: "FEATURE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_FeatureUnspecified;
/**
 *  Label detection. Detect objects, such as dog or flower.
 *
 *  Value: "LABEL_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_LabelDetection;
/**
 *  Logo detection, tracking, and recognition.
 *
 *  Value: "LOGO_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_LogoRecognition;
/**
 *  Object detection and tracking.
 *
 *  Value: "OBJECT_TRACKING"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_ObjectTracking;
/**
 *  Person detection.
 *
 *  Value: "PERSON_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_PersonDetection;
/**
 *  Shot change detection.
 *
 *  Value: "SHOT_CHANGE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_ShotChangeDetection;
/**
 *  Speech transcription.
 *
 *  Value: "SPEECH_TRANSCRIPTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_SpeechTranscription;
/**
 *  OCR text detection and tracking.
 *
 *  Value: "TEXT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_TextDetection;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress.feature

/**
 *  Explicit content detection.
 *
 *  Value: "EXPLICIT_CONTENT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_ExplicitContentDetection;
/**
 *  Human face detection.
 *
 *  Value: "FACE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_FaceDetection;
/**
 *  Unspecified.
 *
 *  Value: "FEATURE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_FeatureUnspecified;
/**
 *  Label detection. Detect objects, such as dog or flower.
 *
 *  Value: "LABEL_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_LabelDetection;
/**
 *  Logo detection, tracking, and recognition.
 *
 *  Value: "LOGO_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_LogoRecognition;
/**
 *  Object detection and tracking.
 *
 *  Value: "OBJECT_TRACKING"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_ObjectTracking;
/**
 *  Person detection.
 *
 *  Value: "PERSON_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_PersonDetection;
/**
 *  Shot change detection.
 *
 *  Value: "SHOT_CHANGE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_ShotChangeDetection;
/**
 *  Speech transcription.
 *
 *  Value: "SPEECH_TRANSCRIPTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_SpeechTranscription;
/**
 *  OCR text detection and tracking.
 *
 *  Value: "TEXT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_TextDetection;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress.feature

/**
 *  Celebrity recognition.
 *
 *  Value: "CELEBRITY_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_CelebrityRecognition;
/**
 *  Explicit content detection.
 *
 *  Value: "EXPLICIT_CONTENT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_ExplicitContentDetection;
/**
 *  Human face detection.
 *
 *  Value: "FACE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_FaceDetection;
/**
 *  Unspecified.
 *
 *  Value: "FEATURE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_FeatureUnspecified;
/**
 *  Label detection. Detect objects, such as dog or flower.
 *
 *  Value: "LABEL_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_LabelDetection;
/**
 *  Logo detection, tracking, and recognition.
 *
 *  Value: "LOGO_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_LogoRecognition;
/**
 *  Object detection and tracking.
 *
 *  Value: "OBJECT_TRACKING"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_ObjectTracking;
/**
 *  Person detection.
 *
 *  Value: "PERSON_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_PersonDetection;
/**
 *  Shot change detection.
 *
 *  Value: "SHOT_CHANGE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_ShotChangeDetection;
/**
 *  Speech transcription.
 *
 *  Value: "SPEECH_TRANSCRIPTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_SpeechTranscription;
/**
 *  OCR text detection and tracking.
 *
 *  Value: "TEXT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_TextDetection;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress.feature

/**
 *  Explicit content detection.
 *
 *  Value: "EXPLICIT_CONTENT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_ExplicitContentDetection;
/**
 *  Human face detection.
 *
 *  Value: "FACE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_FaceDetection;
/**
 *  Unspecified.
 *
 *  Value: "FEATURE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_FeatureUnspecified;
/**
 *  Label detection. Detect objects, such as dog or flower.
 *
 *  Value: "LABEL_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_LabelDetection;
/**
 *  Logo detection, tracking, and recognition.
 *
 *  Value: "LOGO_RECOGNITION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_LogoRecognition;
/**
 *  Object detection and tracking.
 *
 *  Value: "OBJECT_TRACKING"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_ObjectTracking;
/**
 *  Person detection.
 *
 *  Value: "PERSON_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_PersonDetection;
/**
 *  Shot change detection.
 *
 *  Value: "SHOT_CHANGE_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_ShotChangeDetection;
/**
 *  Speech transcription.
 *
 *  Value: "SPEECH_TRANSCRIPTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_SpeechTranscription;
/**
 *  OCR text detection and tracking.
 *
 *  Value: "TEXT_DETECTION"
 */
FOUNDATION_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_TextDetection;

/**
 *  Video annotation progress. Included in the `metadata` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation request.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest : GTLRObject

/** Required. Requested video annotation features. */
@property(nonatomic, strong, nullable) NSArray<NSString *> *features;

/**
 *  The video data bytes. If unset, the input video(s) should be specified via
 *  the `input_uri`. If set, `input_uri` must be unset.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *inputContent;

/**
 *  Input video location. Currently, only [Cloud
 *  Storage](https://cloud.google.com/storage/) URIs are supported. URIs must be
 *  specified in the following format: `gs://bucket-id/object-id` (other URI
 *  formats return google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](https://cloud.google.com/storage/docs/request-endpoints). To
 *  identify multiple videos, a video URI may include wildcards in the
 *  `object-id`. Supported wildcards: '*' to match 0 or more characters; '?' to
 *  match 1 character. If unset, the input video should be embedded in the
 *  request as `input_content`. If set, `input_content` must be unset.
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Optional. Cloud region where annotation should take place. Supported cloud
 *  regions are: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no
 *  region is specified, the region will be determined based on video file
 *  location.
 */
@property(nonatomic, copy, nullable) NSString *locationId;

/**
 *  Optional. Location where the output (in JSON format) should be stored.
 *  Currently, only [Cloud Storage](https://cloud.google.com/storage/) URIs are
 *  supported. These must be specified in the following format:
 *  `gs://bucket-id/object-id` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see [Request
 *  URIs](https://cloud.google.com/storage/docs/request-endpoints).
 */
@property(nonatomic, copy, nullable) NSString *outputUri;

/** Additional video context and/or feature-specific parameters. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext *videoContext;

@end


/**
 *  Video annotation response. Included in the `response` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Video annotation progress. Included in the `metadata` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults *> *annotationResults;

@end


/**
 *  A generic detected attribute represented by name in string format.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedAttribute : GTLRObject

/**
 *  Detected attribute confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  The name of the attribute, for example, glasses, dark_glasses, mouth_open. A
 *  full list of supported type names will be provided in the document.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  Text value of the detection result. For example, the value for "HairColor"
 *  can be "black", "blonde", etc.
 */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  A generic detected landmark represented by name in string format and a 2D
 *  location.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedLandmark : GTLRObject

/**
 *  The confidence score of the detected landmark. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** The name of this landmark, for example, left_hand, right_shoulder. */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The 2D point of the detected landmark using the normalized image coordindate
 *  system. The normalized coordinates have the range from 0 to 1.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedVertex *point;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity : GTLRObject

/**
 *  Textual description, e.g., `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in [Google Knowledge Graph
 *  Search API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only). If no
 *  explicit content has been detected in a frame, no annotations are present
 *  for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame *> *frames;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceAnnotation : GTLRObject

/** All video frames where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceFrame *> *frames;

/** All video segments where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceSegment *> *segments;

/**
 *  Thumbnail of a representative face view (in JPEG format).
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

@end


/**
 *  Face detection annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceDetectionAnnotation : GTLRObject

/**
 *  The thumbnail of a person's face.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

/** The face tracks with attributes. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceFrame : GTLRObject

/**
 *  Normalized Bounding boxes in a frame. There can be more than one boxes if
 *  the same face is detected in multiple locations within the current frame.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingBox *> *normalizedBoundingBoxes;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceSegment : GTLRObject

/** Video segment where a face was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity. For example, when the label is
 *  `Terrier`, the category is likely `dog`. And in some cases there might be
 *  more than one categories e.g., `Terrier` could also be a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment *> *segments;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

@end


/**
 *  Annotation corresponding to one detected, tracked and recognized logo class.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LogoRecognitionAnnotation : GTLRObject

/**
 *  Entity category information to specify the logo class that all the logo
 *  tracks within this LogoRecognitionAnnotation are recognized as.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *entity;

/**
 *  All video segments where the recognized logo appears. There might be
 *  multiple instances of the same logo class appearing in one VideoSegment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *> *segments;

/**
 *  All logo tracks where the recognized logo appears. Each track corresponds to
 *  one logo instance appearing in consecutive frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Track *> *tracks;

@end


/**
 *  Normalized bounding box. The normalized vertex coordinates are relative to
 *  the original image. Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Normalized bounding polygon for text (that might not be aligned with axis).
 *  Contains list of the corner points in clockwise order starting from top-left
 *  corner. For example, for a rectangular bounding box: When the text is
 *  horizontal it might look like: 0----1 | | 3----2 When it's clockwise rotated
 *  180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the
 *  vertex order will still be (0, 1, 2, 3). Note that values can be less than
 *  0, or greater than 1 due to trignometric calculations for location of the
 *  box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingPoly : GTLRObject

/** Normalized vertices of the bounding polygon. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedVertex *> *vertices;

@end


/**
 *  A vertex represents a 2D point in the image. NOTE: the normalized vertex
 *  coordinates are relative to the original image and range from 0 to 1.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedVertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end


/**
 *  Annotations corresponding to one tracked object.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ObjectTrackingAnnotation : GTLRObject

/**
 *  Object category's labeling confidence of this track.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Entity to specify the object category that this track is labeled as. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *entity;

/**
 *  Information corresponding to all frames where this object track appears.
 *  Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
 *  messages in frames. Streaming mode: it can only be one ObjectTrackingFrame
 *  message in frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ObjectTrackingFrame *> *frames;

/**
 *  Non-streaming batch mode ONLY. Each object track corresponds to one video
 *  segment where it appears.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

/**
 *  Streaming mode ONLY. In streaming mode, we do not know the end time of a
 *  tracked object before it is completed. Hence, there is no VideoSegment info
 *  returned. Instead, we provide a unique identifiable integer track_id so that
 *  the customers can correlate the results of the ongoing ObjectTrackAnnotation
 *  of the same track_id over time.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *trackId;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotations for object detection and tracking. This field
 *  stores per frame location, time offset, and confidence.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ObjectTrackingFrame : GTLRObject

/**
 *  The normalized bounding box location of this object track for the frame.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingBox *normalizedBoundingBox;

/** The timestamp of the frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Person detection annotation per video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2PersonDetectionAnnotation : GTLRObject

/** The detected tracks of a person. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechRecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is set to true, you will see all the
 *  words from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of accuracy,
 *  with the top (first) alternative being the most probable, as ranked by the
 *  recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
 *  language tag of the language in this result. This language code was detected
 *  to have the most likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Annotations related to one detected OCR text snippet. This will contain the
 *  corresponding text, confidence value, and frame level information for each
 *  detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextAnnotation : GTLRObject

/** All video segments where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextSegment *> *segments;

/** The detected text. */
@property(nonatomic, copy, nullable) NSString *text;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for text annotation (OCR). Contains
 *  information regarding timestamp and bounding box locations for the frames
 *  containing detected OCR text snippets.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextFrame : GTLRObject

/** Bounding polygon of the detected text for this frame. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingPoly *rotatedBoundingBox;

/** Timestamp of this frame. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for text detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextSegment : GTLRObject

/**
 *  Confidence for the track of detected text. It is calculated as the highest
 *  over all frames where OCR detected text appears.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Information related to the frames where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextFrame *> *frames;

/** Video segment where a text snippet was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

@end


/**
 *  For tracking related features. An object at time_offset with attributes, and
 *  located with normalized_bounding_box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TimestampedObject : GTLRObject

/** Optional. The attributes of the object in the bounding box. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedAttribute *> *attributes;

/** Optional. The detected landmarks. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedLandmark *> *landmarks;

/** Normalized Bounding box in a frame, where the object is located. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2NormalizedBoundingBox *normalizedBoundingBox;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this object.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  A track of an object instance.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Track : GTLRObject

/** Optional. Attributes in the track level. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2DetectedAttribute *> *attributes;

/**
 *  Optional. The confidence score of the tracked object.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment of a track. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

/** The object with timestamp and attributes per frame in the track. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TimestampedObject *> *timestampedObjects;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress : GTLRObject

/**
 *  Specifies which feature is being tracked if the request contains more than
 *  one feature.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_ExplicitContentDetection
 *        Explicit content detection. (Value: "EXPLICIT_CONTENT_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_FaceDetection
 *        Human face detection. (Value: "FACE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_FeatureUnspecified
 *        Unspecified. (Value: "FEATURE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_LabelDetection
 *        Label detection. Detect objects, such as dog or flower. (Value:
 *        "LABEL_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_LogoRecognition
 *        Logo detection, tracking, and recognition. (Value: "LOGO_RECOGNITION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_ObjectTracking
 *        Object detection and tracking. (Value: "OBJECT_TRACKING")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_PersonDetection
 *        Person detection. (Value: "PERSON_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_ShotChangeDetection
 *        Shot change detection. (Value: "SHOT_CHANGE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_SpeechTranscription
 *        Speech transcription. (Value: "SPEECH_TRANSCRIPTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress_Feature_TextDetection
 *        OCR text detection and tracking. (Value: "TEXT_DETECTION")
 */
@property(nonatomic, copy, nullable) NSString *feature;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be 100 when fully
 *  processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/**
 *  Specifies which segment is being tracked if the request contains more than
 *  one segment.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation *explicitAnnotation;

/** Deprecated. Please use `face_detection_annotations` instead. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceAnnotation *> *faceAnnotations;

/** Face detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2FaceDetectionAnnotation *> *faceDetectionAnnotations;

/**
 *  Label annotations on frame level. There is exactly one element for each
 *  unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Annotations for list of logos detected, tracked and recognized in video.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LogoRecognitionAnnotation *> *logoRecognitionAnnotations;

/** Annotations for list of objects detected and tracked in video. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ObjectTrackingAnnotation *> *objectAnnotations;

/** Person detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2PersonDetectionAnnotation *> *personDetectionAnnotations;

/** Video segment on which the annotation is run. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

/**
 *  Topical label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *segmentLabelAnnotations;

/**
 *  Presence label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label. Compared to the existing
 *  topical `segment_label_annotations`, this field presents more fine-grained,
 *  segment-level labels detected in video content and is made available only
 *  when the client sets `LabelDetectionConfig.model` to "builtin/latest" in the
 *  request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *segmentPresenceLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *> *shotAnnotations;

/**
 *  Topical label annotations on shot level. There is exactly one element for
 *  each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *shotLabelAnnotations;

/**
 *  Presence label annotations on shot level. There is exactly one element for
 *  each unique label. Compared to the existing topical
 *  `shot_label_annotations`, this field presents more fine-grained, shot-level
 *  labels detected in video content and is made available only when the client
 *  sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *shotPresenceLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2SpeechTranscription *> *speechTranscriptions;

/**
 *  OCR text detection and tracking. Annotations for list of detected text
 *  snippets. Each will have list of frame information associated with it.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2TextAnnotation *> *textAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such as
 *  `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  A generic detected attribute represented by name in string format.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedAttribute : GTLRObject

/**
 *  Detected attribute confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  The name of the attribute, for example, glasses, dark_glasses, mouth_open. A
 *  full list of supported type names will be provided in the document.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  Text value of the detection result. For example, the value for "HairColor"
 *  can be "black", "blonde", etc.
 */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  A generic detected landmark represented by name in string format and a 2D
 *  location.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedLandmark : GTLRObject

/**
 *  The confidence score of the detected landmark. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** The name of this landmark, for example, left_hand, right_shoulder. */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The 2D point of the detected landmark using the normalized image coordindate
 *  system. The normalized coordinates have the range from 0 to 1.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedVertex *point;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity : GTLRObject

/**
 *  Textual description, e.g., `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in [Google Knowledge Graph
 *  Search API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only). If no
 *  explicit content has been detected in a frame, no annotations are present
 *  for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame *> *frames;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Config for EXPLICIT_CONTENT_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig : GTLRObject

/**
 *  Model to use for explicit content detection. Supported values:
 *  "builtin/stable" (the default if unset) and "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceAnnotation : GTLRObject

/** All video frames where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceFrame *> *frames;

/** All video segments where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceSegment *> *segments;

/**
 *  Thumbnail of a representative face view (in JPEG format).
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

@end


/**
 *  Face detection annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceDetectionAnnotation : GTLRObject

/**
 *  The thumbnail of a person's face.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

/** The face tracks with attributes. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Config for FACE_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceDetectionConfig : GTLRObject

/**
 *  Whether to enable face attributes detection, such as glasses, dark_glasses,
 *  mouth_open etc. Ignored if 'include_bounding_boxes' is set to false.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *includeAttributes;

/**
 *  Whether bounding boxes are included in the face annotation output.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *includeBoundingBoxes;

/**
 *  Model to use for face detection. Supported values: "builtin/stable" (the
 *  default if unset) and "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceFrame : GTLRObject

/**
 *  Normalized Bounding boxes in a frame. There can be more than one boxes if
 *  the same face is detected in multiple locations within the current frame.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingBox *> *normalizedBoundingBoxes;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceSegment : GTLRObject

/** Video segment where a face was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity. For example, when the label is
 *  `Terrier`, the category is likely `dog`. And in some cases there might be
 *  more than one categories e.g., `Terrier` could also be a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment *> *segments;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Config for LABEL_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig : GTLRObject

/**
 *  The confidence threshold we perform filtering on the labels from frame-level
 *  detection. If not set, it is set to 0.4 by default. The valid range for this
 *  threshold is [0.1, 0.9]. Any value set outside of this range will be
 *  clipped. Note: For best results, follow the default threshold. We will
 *  update the default threshold everytime when we release a new model.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *frameConfidenceThreshold;

/**
 *  What labels should be detected with LABEL_DETECTION, in addition to
 *  video-level labels or segment-level labels. If unspecified, defaults to
 *  `SHOT_MODE`.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_FrameMode
 *        Detect frame-level labels. (Value: "FRAME_MODE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_LabelDetectionModeUnspecified
 *        Unspecified. (Value: "LABEL_DETECTION_MODE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotAndFrameMode
 *        Detect both shot-level and frame-level labels. (Value:
 *        "SHOT_AND_FRAME_MODE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotMode
 *        Detect shot-level labels. (Value: "SHOT_MODE")
 */
@property(nonatomic, copy, nullable) NSString *labelDetectionMode;

/**
 *  Model to use for label detection. Supported values: "builtin/stable" (the
 *  default if unset) and "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

/**
 *  Whether the video has been shot from a stationary (i.e., non-moving) camera.
 *  When set to true, might improve detection accuracy for moving objects.
 *  Should be used with `SHOT_AND_FRAME_MODE` enabled.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *stationaryCamera;

/**
 *  The confidence threshold we perform filtering on the labels from video-level
 *  and shot-level detections. If not set, it's set to 0.3 by default. The valid
 *  range for this threshold is [0.1, 0.9]. Any value set outside of this range
 *  will be clipped. Note: For best results, follow the default threshold. We
 *  will update the default threshold everytime when we release a new model.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *videoConfidenceThreshold;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

@end


/**
 *  Annotation corresponding to one detected, tracked and recognized logo class.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LogoRecognitionAnnotation : GTLRObject

/**
 *  Entity category information to specify the logo class that all the logo
 *  tracks within this LogoRecognitionAnnotation are recognized as.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *entity;

/**
 *  All video segments where the recognized logo appears. There might be
 *  multiple instances of the same logo class appearing in one VideoSegment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *segments;

/**
 *  All logo tracks where the recognized logo appears. Each track corresponds to
 *  one logo instance appearing in consecutive frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Track *> *tracks;

@end


/**
 *  Normalized bounding box. The normalized vertex coordinates are relative to
 *  the original image. Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Normalized bounding polygon for text (that might not be aligned with axis).
 *  Contains list of the corner points in clockwise order starting from top-left
 *  corner. For example, for a rectangular bounding box: When the text is
 *  horizontal it might look like: 0----1 | | 3----2 When it's clockwise rotated
 *  180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the
 *  vertex order will still be (0, 1, 2, 3). Note that values can be less than
 *  0, or greater than 1 due to trignometric calculations for location of the
 *  box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingPoly : GTLRObject

/** Normalized vertices of the bounding polygon. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedVertex *> *vertices;

@end


/**
 *  A vertex represents a 2D point in the image. NOTE: the normalized vertex
 *  coordinates are relative to the original image and range from 0 to 1.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedVertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end


/**
 *  Annotations corresponding to one tracked object.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingAnnotation : GTLRObject

/**
 *  Object category's labeling confidence of this track.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Entity to specify the object category that this track is labeled as. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *entity;

/**
 *  Information corresponding to all frames where this object track appears.
 *  Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
 *  messages in frames. Streaming mode: it can only be one ObjectTrackingFrame
 *  message in frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingFrame *> *frames;

/**
 *  Non-streaming batch mode ONLY. Each object track corresponds to one video
 *  segment where it appears.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

/**
 *  Streaming mode ONLY. In streaming mode, we do not know the end time of a
 *  tracked object before it is completed. Hence, there is no VideoSegment info
 *  returned. Instead, we provide a unique identifiable integer track_id so that
 *  the customers can correlate the results of the ongoing ObjectTrackAnnotation
 *  of the same track_id over time.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *trackId;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Config for OBJECT_TRACKING.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingConfig : GTLRObject

/**
 *  Model to use for object tracking. Supported values: "builtin/stable" (the
 *  default if unset) and "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Video frame level annotations for object detection and tracking. This field
 *  stores per frame location, time offset, and confidence.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingFrame : GTLRObject

/**
 *  The normalized bounding box location of this object track for the frame.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingBox *normalizedBoundingBox;

/** The timestamp of the frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video annotation progress. Included in the `metadata` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  A generic detected attribute represented by name in string format.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedAttribute : GTLRObject

/**
 *  Detected attribute confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  The name of the attribute, for example, glasses, dark_glasses, mouth_open. A
 *  full list of supported type names will be provided in the document.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  Text value of the detection result. For example, the value for "HairColor"
 *  can be "black", "blonde", etc.
 */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  A generic detected landmark represented by name in string format and a 2D
 *  location.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedLandmark : GTLRObject

/**
 *  The confidence score of the detected landmark. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** The name of this landmark, for example, left_hand, right_shoulder. */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The 2D point of the detected landmark using the normalized image coordindate
 *  system. The normalized coordinates have the range from 0 to 1.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedVertex *point;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity : GTLRObject

/**
 *  Textual description, e.g., `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in [Google Knowledge Graph
 *  Search API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only). If no
 *  explicit content has been detected in a frame, no annotations are present
 *  for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame *> *frames;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceAnnotation : GTLRObject

/** All video frames where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceFrame *> *frames;

/** All video segments where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceSegment *> *segments;

/**
 *  Thumbnail of a representative face view (in JPEG format).
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

@end


/**
 *  Face detection annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation : GTLRObject

/**
 *  The thumbnail of a person's face.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

/** The face tracks with attributes. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceFrame : GTLRObject

/**
 *  Normalized Bounding boxes in a frame. There can be more than one boxes if
 *  the same face is detected in multiple locations within the current frame.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox *> *normalizedBoundingBoxes;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceSegment : GTLRObject

/** Video segment where a face was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity. For example, when the label is
 *  `Terrier`, the category is likely `dog`. And in some cases there might be
 *  more than one categories e.g., `Terrier` could also be a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment *> *segments;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

@end


/**
 *  Annotation corresponding to one detected, tracked and recognized logo class.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LogoRecognitionAnnotation : GTLRObject

/**
 *  Entity category information to specify the logo class that all the logo
 *  tracks within this LogoRecognitionAnnotation are recognized as.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *entity;

/**
 *  All video segments where the recognized logo appears. There might be
 *  multiple instances of the same logo class appearing in one VideoSegment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *> *segments;

/**
 *  All logo tracks where the recognized logo appears. Each track corresponds to
 *  one logo instance appearing in consecutive frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Track *> *tracks;

@end


/**
 *  Normalized bounding box. The normalized vertex coordinates are relative to
 *  the original image. Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Normalized bounding polygon for text (that might not be aligned with axis).
 *  Contains list of the corner points in clockwise order starting from top-left
 *  corner. For example, for a rectangular bounding box: When the text is
 *  horizontal it might look like: 0----1 | | 3----2 When it's clockwise rotated
 *  180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the
 *  vertex order will still be (0, 1, 2, 3). Note that values can be less than
 *  0, or greater than 1 due to trignometric calculations for location of the
 *  box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingPoly : GTLRObject

/** Normalized vertices of the bounding polygon. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedVertex *> *vertices;

@end


/**
 *  A vertex represents a 2D point in the image. NOTE: the normalized vertex
 *  coordinates are relative to the original image and range from 0 to 1.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedVertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end


/**
 *  Annotations corresponding to one tracked object.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ObjectTrackingAnnotation : GTLRObject

/**
 *  Object category's labeling confidence of this track.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Entity to specify the object category that this track is labeled as. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *entity;

/**
 *  Information corresponding to all frames where this object track appears.
 *  Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
 *  messages in frames. Streaming mode: it can only be one ObjectTrackingFrame
 *  message in frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ObjectTrackingFrame *> *frames;

/**
 *  Non-streaming batch mode ONLY. Each object track corresponds to one video
 *  segment where it appears.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

/**
 *  Streaming mode ONLY. In streaming mode, we do not know the end time of a
 *  tracked object before it is completed. Hence, there is no VideoSegment info
 *  returned. Instead, we provide a unique identifiable integer track_id so that
 *  the customers can correlate the results of the ongoing ObjectTrackAnnotation
 *  of the same track_id over time.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *trackId;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotations for object detection and tracking. This field
 *  stores per frame location, time offset, and confidence.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ObjectTrackingFrame : GTLRObject

/**
 *  The normalized bounding box location of this object track for the frame.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox *normalizedBoundingBox;

/** The timestamp of the frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Person detection annotation per video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1PersonDetectionAnnotation : GTLRObject

/** The detected tracks of a person. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is set to true, you will see all the
 *  words from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of accuracy,
 *  with the top (first) alternative being the most probable, as ranked by the
 *  recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
 *  language tag of the language in this result. This language code was detected
 *  to have the most likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Annotations related to one detected OCR text snippet. This will contain the
 *  corresponding text, confidence value, and frame level information for each
 *  detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextAnnotation : GTLRObject

/** All video segments where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextSegment *> *segments;

/** The detected text. */
@property(nonatomic, copy, nullable) NSString *text;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for text annotation (OCR). Contains
 *  information regarding timestamp and bounding box locations for the frames
 *  containing detected OCR text snippets.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextFrame : GTLRObject

/** Bounding polygon of the detected text for this frame. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingPoly *rotatedBoundingBox;

/** Timestamp of this frame. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for text detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextSegment : GTLRObject

/**
 *  Confidence for the track of detected text. It is calculated as the highest
 *  over all frames where OCR detected text appears.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Information related to the frames where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextFrame *> *frames;

/** Video segment where a text snippet was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

@end


/**
 *  For tracking related features. An object at time_offset with attributes, and
 *  located with normalized_bounding_box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TimestampedObject : GTLRObject

/** Optional. The attributes of the object in the bounding box. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedAttribute *> *attributes;

/** Optional. The detected landmarks. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedLandmark *> *landmarks;

/** Normalized Bounding box in a frame, where the object is located. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox *normalizedBoundingBox;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this object.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  A track of an object instance.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Track : GTLRObject

/** Optional. Attributes in the track level. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1DetectedAttribute *> *attributes;

/**
 *  Optional. The confidence score of the tracked object.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment of a track. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

/** The object with timestamp and attributes per frame in the track. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TimestampedObject *> *timestampedObjects;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress : GTLRObject

/**
 *  Specifies which feature is being tracked if the request contains more than
 *  one feature.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_ExplicitContentDetection
 *        Explicit content detection. (Value: "EXPLICIT_CONTENT_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_FaceDetection
 *        Human face detection. (Value: "FACE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_FeatureUnspecified
 *        Unspecified. (Value: "FEATURE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_LabelDetection
 *        Label detection. Detect objects, such as dog or flower. (Value:
 *        "LABEL_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_LogoRecognition
 *        Logo detection, tracking, and recognition. (Value: "LOGO_RECOGNITION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_ObjectTracking
 *        Object detection and tracking. (Value: "OBJECT_TRACKING")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_PersonDetection
 *        Person detection. (Value: "PERSON_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_ShotChangeDetection
 *        Shot change detection. (Value: "SHOT_CHANGE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_SpeechTranscription
 *        Speech transcription. (Value: "SPEECH_TRANSCRIPTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress_Feature_TextDetection
 *        OCR text detection and tracking. (Value: "TEXT_DETECTION")
 */
@property(nonatomic, copy, nullable) NSString *feature;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be 100 when fully
 *  processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/**
 *  Specifies which segment is being tracked if the request contains more than
 *  one segment.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation *explicitAnnotation;

/** Deprecated. Please use `face_detection_annotations` instead. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceAnnotation *> *faceAnnotations;

/** Face detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation *> *faceDetectionAnnotations;

/**
 *  Label annotations on frame level. There is exactly one element for each
 *  unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Annotations for list of logos detected, tracked and recognized in video.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LogoRecognitionAnnotation *> *logoRecognitionAnnotations;

/** Annotations for list of objects detected and tracked in video. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ObjectTrackingAnnotation *> *objectAnnotations;

/** Person detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1PersonDetectionAnnotation *> *personDetectionAnnotations;

/** Video segment on which the annotation is run. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

/**
 *  Topical label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *segmentLabelAnnotations;

/**
 *  Presence label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label. Compared to the existing
 *  topical `segment_label_annotations`, this field presents more fine-grained,
 *  segment-level labels detected in video content and is made available only
 *  when the client sets `LabelDetectionConfig.model` to "builtin/latest" in the
 *  request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *segmentPresenceLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *> *shotAnnotations;

/**
 *  Topical label annotations on shot level. There is exactly one element for
 *  each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *shotLabelAnnotations;

/**
 *  Presence label annotations on shot level. There is exactly one element for
 *  each unique label. Compared to the existing topical
 *  `shot_label_annotations`, this field presents more fine-grained, shot-level
 *  labels detected in video content and is made available only when the client
 *  sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *shotPresenceLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription *> *speechTranscriptions;

/**
 *  OCR text detection and tracking. Annotations for list of detected text
 *  snippets. Each will have list of frame information associated with it.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1TextAnnotation *> *textAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such as
 *  `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Video annotation progress. Included in the `metadata` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  A generic detected attribute represented by name in string format.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedAttribute : GTLRObject

/**
 *  Detected attribute confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  The name of the attribute, for example, glasses, dark_glasses, mouth_open. A
 *  full list of supported type names will be provided in the document.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  Text value of the detection result. For example, the value for "HairColor"
 *  can be "black", "blonde", etc.
 */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  A generic detected landmark represented by name in string format and a 2D
 *  location.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedLandmark : GTLRObject

/**
 *  The confidence score of the detected landmark. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** The name of this landmark, for example, left_hand, right_shoulder. */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The 2D point of the detected landmark using the normalized image coordindate
 *  system. The normalized coordinates have the range from 0 to 1.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex *point;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity : GTLRObject

/**
 *  Textual description, e.g., `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in [Google Knowledge Graph
 *  Search API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only). If no
 *  explicit content has been detected in a frame, no annotations are present
 *  for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame *> *frames;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceAnnotation : GTLRObject

/** All video frames where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceFrame *> *frames;

/** All video segments where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceSegment *> *segments;

/**
 *  Thumbnail of a representative face view (in JPEG format).
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

@end


/**
 *  Face detection annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceDetectionAnnotation : GTLRObject

/**
 *  The thumbnail of a person's face.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

/** The face tracks with attributes. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceFrame : GTLRObject

/**
 *  Normalized Bounding boxes in a frame. There can be more than one boxes if
 *  the same face is detected in multiple locations within the current frame.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox *> *normalizedBoundingBoxes;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceSegment : GTLRObject

/** Video segment where a face was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity. For example, when the label is
 *  `Terrier`, the category is likely `dog`. And in some cases there might be
 *  more than one categories e.g., `Terrier` could also be a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelSegment *> *segments;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

@end


/**
 *  Annotation corresponding to one detected, tracked and recognized logo class.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LogoRecognitionAnnotation : GTLRObject

/**
 *  Entity category information to specify the logo class that all the logo
 *  tracks within this LogoRecognitionAnnotation are recognized as.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *entity;

/**
 *  All video segments where the recognized logo appears. There might be
 *  multiple instances of the same logo class appearing in one VideoSegment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *> *segments;

/**
 *  All logo tracks where the recognized logo appears. Each track corresponds to
 *  one logo instance appearing in consecutive frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Track *> *tracks;

@end


/**
 *  Normalized bounding box. The normalized vertex coordinates are relative to
 *  the original image. Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Normalized bounding polygon for text (that might not be aligned with axis).
 *  Contains list of the corner points in clockwise order starting from top-left
 *  corner. For example, for a rectangular bounding box: When the text is
 *  horizontal it might look like: 0----1 | | 3----2 When it's clockwise rotated
 *  180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the
 *  vertex order will still be (0, 1, 2, 3). Note that values can be less than
 *  0, or greater than 1 due to trignometric calculations for location of the
 *  box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingPoly : GTLRObject

/** Normalized vertices of the bounding polygon. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex *> *vertices;

@end


/**
 *  A vertex represents a 2D point in the image. NOTE: the normalized vertex
 *  coordinates are relative to the original image and range from 0 to 1.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedVertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end


/**
 *  Annotations corresponding to one tracked object.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingAnnotation : GTLRObject

/**
 *  Object category's labeling confidence of this track.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Entity to specify the object category that this track is labeled as. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Entity *entity;

/**
 *  Information corresponding to all frames where this object track appears.
 *  Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
 *  messages in frames. Streaming mode: it can only be one ObjectTrackingFrame
 *  message in frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingFrame *> *frames;

/**
 *  Non-streaming batch mode ONLY. Each object track corresponds to one video
 *  segment where it appears.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

/**
 *  Streaming mode ONLY. In streaming mode, we do not know the end time of a
 *  tracked object before it is completed. Hence, there is no VideoSegment info
 *  returned. Instead, we provide a unique identifiable integer track_id so that
 *  the customers can correlate the results of the ongoing ObjectTrackAnnotation
 *  of the same track_id over time.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *trackId;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotations for object detection and tracking. This field
 *  stores per frame location, time offset, and confidence.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingFrame : GTLRObject

/**
 *  The normalized bounding box location of this object track for the frame.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox *normalizedBoundingBox;

/** The timestamp of the frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Person detection annotation per video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1PersonDetectionAnnotation : GTLRObject

/** The detected tracks of a person. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechRecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is set to true, you will see all the
 *  words from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of accuracy,
 *  with the top (first) alternative being the most probable, as ranked by the
 *  recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
 *  language tag of the language in this result. This language code was detected
 *  to have the most likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Annotations related to one detected OCR text snippet. This will contain the
 *  corresponding text, confidence value, and frame level information for each
 *  detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextAnnotation : GTLRObject

/** All video segments where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextSegment *> *segments;

/** The detected text. */
@property(nonatomic, copy, nullable) NSString *text;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for text annotation (OCR). Contains
 *  information regarding timestamp and bounding box locations for the frames
 *  containing detected OCR text snippets.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextFrame : GTLRObject

/** Bounding polygon of the detected text for this frame. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingPoly *rotatedBoundingBox;

/** Timestamp of this frame. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for text detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextSegment : GTLRObject

/**
 *  Confidence for the track of detected text. It is calculated as the highest
 *  over all frames where OCR detected text appears.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Information related to the frames where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextFrame *> *frames;

/** Video segment where a text snippet was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

@end


/**
 *  For tracking related features. An object at time_offset with attributes, and
 *  located with normalized_bounding_box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TimestampedObject : GTLRObject

/** Optional. The attributes of the object in the bounding box. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedAttribute *> *attributes;

/** Optional. The detected landmarks. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedLandmark *> *landmarks;

/** Normalized Bounding box in a frame, where the object is located. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1NormalizedBoundingBox *normalizedBoundingBox;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this object.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  A track of an object instance.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1Track : GTLRObject

/** Optional. Attributes in the track level. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1DetectedAttribute *> *attributes;

/**
 *  Optional. The confidence score of the tracked object.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment of a track. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

/** The object with timestamp and attributes per frame in the track. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TimestampedObject *> *timestampedObjects;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress : GTLRObject

/**
 *  Specifies which feature is being tracked if the request contains more than
 *  one feature.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_ExplicitContentDetection
 *        Explicit content detection. (Value: "EXPLICIT_CONTENT_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_FaceDetection
 *        Human face detection. (Value: "FACE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_FeatureUnspecified
 *        Unspecified. (Value: "FEATURE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_LabelDetection
 *        Label detection. Detect objects, such as dog or flower. (Value:
 *        "LABEL_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_LogoRecognition
 *        Logo detection, tracking, and recognition. (Value: "LOGO_RECOGNITION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_ObjectTracking
 *        Object detection and tracking. (Value: "OBJECT_TRACKING")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_PersonDetection
 *        Person detection. (Value: "PERSON_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_ShotChangeDetection
 *        Shot change detection. (Value: "SHOT_CHANGE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_SpeechTranscription
 *        Speech transcription. (Value: "SPEECH_TRANSCRIPTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationProgress_Feature_TextDetection
 *        OCR text detection and tracking. (Value: "TEXT_DETECTION")
 */
@property(nonatomic, copy, nullable) NSString *feature;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be 100 when fully
 *  processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/**
 *  Specifies which segment is being tracked if the request contains more than
 *  one segment.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ExplicitContentAnnotation *explicitAnnotation;

/** Deprecated. Please use `face_detection_annotations` instead. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceAnnotation *> *faceAnnotations;

/** Face detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1FaceDetectionAnnotation *> *faceDetectionAnnotations;

/**
 *  Label annotations on frame level. There is exactly one element for each
 *  unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Annotations for list of logos detected, tracked and recognized in video.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LogoRecognitionAnnotation *> *logoRecognitionAnnotations;

/** Annotations for list of objects detected and tracked in video. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1ObjectTrackingAnnotation *> *objectAnnotations;

/** Person detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1PersonDetectionAnnotation *> *personDetectionAnnotations;

/** Video segment on which the annotation is run. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *segment;

/**
 *  Topical label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *segmentLabelAnnotations;

/**
 *  Presence label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label. Compared to the existing
 *  topical `segment_label_annotations`, this field presents more fine-grained,
 *  segment-level labels detected in video content and is made available only
 *  when the client sets `LabelDetectionConfig.model` to "builtin/latest" in the
 *  request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *segmentPresenceLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment *> *shotAnnotations;

/**
 *  Topical label annotations on shot level. There is exactly one element for
 *  each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *shotLabelAnnotations;

/**
 *  Presence label annotations on shot level. There is exactly one element for
 *  each unique label. Compared to the existing topical
 *  `shot_label_annotations`, this field presents more fine-grained, shot-level
 *  labels detected in video content and is made available only when the client
 *  sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1LabelAnnotation *> *shotPresenceLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1SpeechTranscription *> *speechTranscriptions;

/**
 *  OCR text detection and tracking. Annotations for list of detected text
 *  snippets. Each will have list of frame information associated with it.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1TextAnnotation *> *textAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such as
 *  `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p2beta1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Video annotation progress. Included in the `metadata` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response` field of the
 *  `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Celebrity definition.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Celebrity : GTLRObject

/**
 *  Textual description of additional information about the celebrity, if
 *  applicable.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/** The celebrity name. */
@property(nonatomic, copy, nullable) NSString *displayName;

/**
 *  The resource name of the celebrity. Have the format
 *  `video-intelligence/kg-mid` indicates a celebrity from preloaded gallery.
 *  kg-mid is the id in Google knowledge graph, which is unique for the
 *  celebrity.
 */
@property(nonatomic, copy, nullable) NSString *name;

@end


/**
 *  Celebrity recognition annotation per video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1CelebrityRecognitionAnnotation : GTLRObject

/**
 *  The tracks detected from the input video, including recognized celebrities
 *  and other detected faces in the video.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1CelebrityTrack *> *celebrityTracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  The annotation result of a celebrity face track. RecognizedCelebrity field
 *  could be empty if the face track does not have any matched celebrities.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1CelebrityTrack : GTLRObject

/** Top N match of the celebrities for the face in this track. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1RecognizedCelebrity *> *celebrities;

/** A track of a person's face. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Track *faceTrack;

@end


/**
 *  A generic detected attribute represented by name in string format.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedAttribute : GTLRObject

/**
 *  Detected attribute confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  The name of the attribute, for example, glasses, dark_glasses, mouth_open. A
 *  full list of supported type names will be provided in the document.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  Text value of the detection result. For example, the value for "HairColor"
 *  can be "black", "blonde", etc.
 */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  A generic detected landmark represented by name in string format and a 2D
 *  location.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedLandmark : GTLRObject

/**
 *  The confidence score of the detected landmark. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** The name of this landmark, for example, left_hand, right_shoulder. */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The 2D point of the detected landmark using the normalized image coordindate
 *  system. The normalized coordinates have the range from 0 to 1.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedVertex *point;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Entity : GTLRObject

/**
 *  Textual description, e.g., `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in [Google Knowledge Graph
 *  Search API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only). If no
 *  explicit content has been detected in a frame, no annotations are present
 *  for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame *> *frames;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceAnnotation : GTLRObject

/** All video frames where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceFrame *> *frames;

/** All video segments where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceSegment *> *segments;

/**
 *  Thumbnail of a representative face view (in JPEG format).
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

@end


/**
 *  Face detection annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceDetectionAnnotation : GTLRObject

/**
 *  The thumbnail of a person's face.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *thumbnail;

/** The face tracks with attributes. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Deprecated. No effect.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceFrame : GTLRObject

/**
 *  Normalized Bounding boxes in a frame. There can be more than one boxes if
 *  the same face is detected in multiple locations within the current frame.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingBox *> *normalizedBoundingBoxes;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceSegment : GTLRObject

/** Video segment where a face was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity. For example, when the label is
 *  `Terrier`, the category is likely `dog`. And in some cases there might be
 *  more than one categories e.g., `Terrier` could also be a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelSegment *> *segments;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

@end


/**
 *  Annotation corresponding to one detected, tracked and recognized logo class.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LogoRecognitionAnnotation : GTLRObject

/**
 *  Entity category information to specify the logo class that all the logo
 *  tracks within this LogoRecognitionAnnotation are recognized as.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Entity *entity;

/**
 *  All video segments where the recognized logo appears. There might be
 *  multiple instances of the same logo class appearing in one VideoSegment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *> *segments;

/**
 *  All logo tracks where the recognized logo appears. Each track corresponds to
 *  one logo instance appearing in consecutive frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Track *> *tracks;

@end


/**
 *  Normalized bounding box. The normalized vertex coordinates are relative to
 *  the original image. Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Normalized bounding polygon for text (that might not be aligned with axis).
 *  Contains list of the corner points in clockwise order starting from top-left
 *  corner. For example, for a rectangular bounding box: When the text is
 *  horizontal it might look like: 0----1 | | 3----2 When it's clockwise rotated
 *  180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the
 *  vertex order will still be (0, 1, 2, 3). Note that values can be less than
 *  0, or greater than 1 due to trignometric calculations for location of the
 *  box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingPoly : GTLRObject

/** Normalized vertices of the bounding polygon. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedVertex *> *vertices;

@end


/**
 *  A vertex represents a 2D point in the image. NOTE: the normalized vertex
 *  coordinates are relative to the original image and range from 0 to 1.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedVertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end


/**
 *  Annotations corresponding to one tracked object.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingAnnotation : GTLRObject

/**
 *  Object category's labeling confidence of this track.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Entity to specify the object category that this track is labeled as. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Entity *entity;

/**
 *  Information corresponding to all frames where this object track appears.
 *  Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
 *  messages in frames. Streaming mode: it can only be one ObjectTrackingFrame
 *  message in frames.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingFrame *> *frames;

/**
 *  Non-streaming batch mode ONLY. Each object track corresponds to one video
 *  segment where it appears.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

/**
 *  Streaming mode ONLY. In streaming mode, we do not know the end time of a
 *  tracked object before it is completed. Hence, there is no VideoSegment info
 *  returned. Instead, we provide a unique identifiable integer track_id so that
 *  the customers can correlate the results of the ongoing ObjectTrackAnnotation
 *  of the same track_id over time.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *trackId;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotations for object detection and tracking. This field
 *  stores per frame location, time offset, and confidence.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingFrame : GTLRObject

/**
 *  The normalized bounding box location of this object track for the frame.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingBox *normalizedBoundingBox;

/** The timestamp of the frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Person detection annotation per video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1PersonDetectionAnnotation : GTLRObject

/** The detected tracks of a person. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  The recognized celebrity with confidence score.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1RecognizedCelebrity : GTLRObject

/** The recognized celebrity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Celebrity *celebrity;

/**
 *  Recognition confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1SpeechRecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is set to true, you will see all the
 *  words from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of accuracy,
 *  with the top (first) alternative being the most probable, as ranked by the
 *  recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
 *  language tag of the language in this result. This language code was detected
 *  to have the most likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  `StreamingAnnotateVideoResponse` is the only message returned to the client
 *  by `StreamingAnnotateVideo`. A series of zero or more
 *  `StreamingAnnotateVideoResponse` messages are streamed back to the client.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1StreamingAnnotateVideoResponse : GTLRObject

/** Streaming annotation results. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1StreamingVideoAnnotationResults *annotationResults;

/**
 *  Google Cloud Storage URI that stores annotation results of one streaming
 *  session in JSON format. It is the annotation_result_storage_directory from
 *  the request followed by '/cloud_project_number-session_id'.
 */
@property(nonatomic, copy, nullable) NSString *annotationResultsUri;

/**
 *  If set, returns a google.rpc.Status message that specifies the error for the
 *  operation.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

@end


/**
 *  Streaming annotation results corresponding to a portion of the video that is
 *  currently being processed. Only ONE type of annotation will be specified in
 *  the response.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1StreamingVideoAnnotationResults : GTLRObject

/** Explicit content annotation results. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentAnnotation *explicitAnnotation;

/** Timestamp of the processed frame in microseconds. */
@property(nonatomic, strong, nullable) GTLRDuration *frameTimestamp;

/** Label annotation results. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation *> *labelAnnotations;

/** Object tracking results. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingAnnotation *> *objectAnnotations;

/** Shot annotation results. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *> *shotAnnotations;

@end


/**
 *  Annotations related to one detected OCR text snippet. This will contain the
 *  corresponding text, confidence value, and frame level information for each
 *  detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextAnnotation : GTLRObject

/** All video segments where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextSegment *> *segments;

/** The detected text. */
@property(nonatomic, copy, nullable) NSString *text;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Video frame level annotation results for text annotation (OCR). Contains
 *  information regarding timestamp and bounding box locations for the frames
 *  containing detected OCR text snippets.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextFrame : GTLRObject

/** Bounding polygon of the detected text for this frame. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingPoly *rotatedBoundingBox;

/** Timestamp of this frame. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for text detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextSegment : GTLRObject

/**
 *  Confidence for the track of detected text. It is calculated as the highest
 *  over all frames where OCR detected text appears.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Information related to the frames where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextFrame *> *frames;

/** Video segment where a text snippet was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

@end


/**
 *  For tracking related features. An object at time_offset with attributes, and
 *  located with normalized_bounding_box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TimestampedObject : GTLRObject

/** Optional. The attributes of the object in the bounding box. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedAttribute *> *attributes;

/** Optional. The detected landmarks. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedLandmark *> *landmarks;

/** Normalized Bounding box in a frame, where the object is located. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1NormalizedBoundingBox *normalizedBoundingBox;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this object.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  A track of an object instance.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1Track : GTLRObject

/** Optional. Attributes in the track level. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1DetectedAttribute *> *attributes;

/**
 *  Optional. The confidence score of the tracked object.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment of a track. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

/** The object with timestamp and attributes per frame in the track. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TimestampedObject *> *timestampedObjects;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress : GTLRObject

/**
 *  Specifies which feature is being tracked if the request contains more than
 *  one feature.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_CelebrityRecognition
 *        Celebrity recognition. (Value: "CELEBRITY_RECOGNITION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_ExplicitContentDetection
 *        Explicit content detection. (Value: "EXPLICIT_CONTENT_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_FaceDetection
 *        Human face detection. (Value: "FACE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_FeatureUnspecified
 *        Unspecified. (Value: "FEATURE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_LabelDetection
 *        Label detection. Detect objects, such as dog or flower. (Value:
 *        "LABEL_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_LogoRecognition
 *        Logo detection, tracking, and recognition. (Value: "LOGO_RECOGNITION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_ObjectTracking
 *        Object detection and tracking. (Value: "OBJECT_TRACKING")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_PersonDetection
 *        Person detection. (Value: "PERSON_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_ShotChangeDetection
 *        Shot change detection. (Value: "SHOT_CHANGE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_SpeechTranscription
 *        Speech transcription. (Value: "SPEECH_TRANSCRIPTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationProgress_Feature_TextDetection
 *        OCR text detection and tracking. (Value: "TEXT_DETECTION")
 */
@property(nonatomic, copy, nullable) NSString *feature;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be 100 when fully
 *  processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/**
 *  Specifies which segment is being tracked if the request contains more than
 *  one segment.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoAnnotationResults : GTLRObject

/** Celebrity recognition annotations. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1CelebrityRecognitionAnnotation *celebrityRecognitionAnnotations;

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ExplicitContentAnnotation *explicitAnnotation;

/** Deprecated. Please use `face_detection_annotations` instead. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceAnnotation *> *faceAnnotations;

/** Face detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1FaceDetectionAnnotation *> *faceDetectionAnnotations;

/**
 *  Label annotations on frame level. There is exactly one element for each
 *  unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Annotations for list of logos detected, tracked and recognized in video.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LogoRecognitionAnnotation *> *logoRecognitionAnnotations;

/** Annotations for list of objects detected and tracked in video. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1ObjectTrackingAnnotation *> *objectAnnotations;

/** Person detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1PersonDetectionAnnotation *> *personDetectionAnnotations;

/** Video segment on which the annotation is run. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *segment;

/**
 *  Topical label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation *> *segmentLabelAnnotations;

/**
 *  Presence label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label. Compared to the existing
 *  topical `segment_label_annotations`, this field presents more fine-grained,
 *  segment-level labels detected in video content and is made available only
 *  when the client sets `LabelDetectionConfig.model` to "builtin/latest" in the
 *  request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation *> *segmentPresenceLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment *> *shotAnnotations;

/**
 *  Topical label annotations on shot level. There is exactly one element for
 *  each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation *> *shotLabelAnnotations;

/**
 *  Presence label annotations on shot level. There is exactly one element for
 *  each unique label. Compared to the existing topical
 *  `shot_label_annotations`, this field presents more fine-grained, shot-level
 *  labels detected in video content and is made available only when the client
 *  sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1LabelAnnotation *> *shotPresenceLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1SpeechTranscription *> *speechTranscriptions;

/**
 *  OCR text detection and tracking. Annotations for list of detected text
 *  snippets. Each will have list of frame information associated with it.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1TextAnnotation *> *textAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such as
 *  `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p3beta1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Person detection annotation per video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1PersonDetectionAnnotation : GTLRObject

/** The detected tracks of a person. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Track *> *tracks;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Config for PERSON_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1PersonDetectionConfig : GTLRObject

/**
 *  Whether to enable person attributes detection, such as cloth color (black,
 *  blue, etc), type (coat, dress, etc), pattern (plain, floral, etc), hair,
 *  etc. Ignored if 'include_bounding_boxes' is set to false.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *includeAttributes;

/**
 *  Whether bounding boxes are included in the person detection annotation
 *  output.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *includeBoundingBoxes;

/**
 *  Whether to enable pose landmarks detection. Ignored if
 *  'include_bounding_boxes' is set to false.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *includePoseLandmarks;

@end


/**
 *  Config for SHOT_CHANGE_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig : GTLRObject

/**
 *  Model to use for shot change detection. Supported values: "builtin/stable"
 *  (the default if unset) and "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases in the results.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechContext : GTLRObject

/**
 *  Optional. A list of strings containing words and phrases "hints" so that the
 *  speech recognition is more likely to recognize them. This can be used to
 *  improve the accuracy for specific words and phrases, for example, if
 *  specific commands are typically spoken by the user. This can also be used to
 *  add additional words to the vocabulary of the recognizer. See [usage
 *  limits](https://cloud.google.com/speech/limits#content).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechRecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Transcript text representing the words that the user spoke. */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 *  Note: When `enable_speaker_diarization` is set to true, you will see all the
 *  words from the beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscription : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of accuracy,
 *  with the top (first) alternative being the most probable, as ranked by the
 *  recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechRecognitionAlternative *> *alternatives;

/**
 *  Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
 *  language tag of the language in this result. This language code was detected
 *  to have the most likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Config for SPEECH_TRANSCRIPTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscriptionConfig : GTLRObject

/**
 *  Optional. For file formats, such as MXF or MKV, supporting multiple audio
 *  tracks, specify up to two tracks. Default: track 0.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSArray<NSNumber *> *audioTracks;

/**
 *  Optional. If set, specifies the estimated number of speakers in the
 *  conversation. If not set, defaults to '2'. Ignored unless
 *  enable_speaker_diarization is set to true.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *diarizationSpeakerCount;

/**
 *  Optional. If 'true', adds punctuation to recognition result hypotheses. This
 *  feature is only available in select languages. Setting this for requests in
 *  other languages has no effect at all. The default 'false' value does not add
 *  punctuation to result hypotheses. NOTE: "This is currently offered as an
 *  experimental service, complimentary to all users. In the future this may be
 *  exclusively available as a premium feature."
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableAutomaticPunctuation;

/**
 *  Optional. If 'true', enables speaker detection for each recognized word in
 *  the top alternative of the recognition result using a speaker_tag provided
 *  in the WordInfo. Note: When this is true, we send all the words from the
 *  beginning of the audio for the top alternative in every consecutive
 *  response. This is done in order to improve our speaker tags as our models
 *  learn to identify the speakers in the conversation over time.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSpeakerDiarization;

/**
 *  Optional. If `true`, the top result includes a list of words and the
 *  confidence for those words. If `false`, no word-level confidence information
 *  is returned. The default is `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordConfidence;

/**
 *  Optional. If set to `true`, the server will attempt to filter out
 *  profanities, replacing all but the initial character in each filtered word
 *  with asterisks, e.g. "f***". If set to `false` or omitted, profanities won't
 *  be filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *filterProfanity;

/**
 *  Required. *Required* The language of the supplied audio as a
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 *  Example: "en-US". See [Language
 *  Support](https://cloud.google.com/speech/docs/languages) for a list of the
 *  currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  Optional. Maximum number of recognition hypotheses to be returned.
 *  Specifically, the maximum number of `SpeechRecognitionAlternative` messages
 *  within each `SpeechTranscription`. The server may return fewer than
 *  `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
 *  return a maximum of one. If omitted, will return a maximum of one.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/** Optional. A means to provide context to assist the speech recognition. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechContext *> *speechContexts;

@end


/**
 *  Annotations related to one detected OCR text snippet. This will contain the
 *  corresponding text, confidence value, and frame level information for each
 *  detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextAnnotation : GTLRObject

/** All video segments where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextSegment *> *segments;

/** The detected text. */
@property(nonatomic, copy, nullable) NSString *text;

/** Feature version. */
@property(nonatomic, copy, nullable) NSString *version;

@end


/**
 *  Config for TEXT_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextDetectionConfig : GTLRObject

/**
 *  Language hint can be specified if the language to be detected is known a
 *  priori. It can increase the accuracy of the detection. Language hint must be
 *  language code in BCP-47 format. Automatic language detection is performed if
 *  no hint is provided.
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *languageHints;

/**
 *  Model to use for text detection. Supported values: "builtin/stable" (the
 *  default if unset) and "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Video frame level annotation results for text annotation (OCR). Contains
 *  information regarding timestamp and bounding box locations for the frames
 *  containing detected OCR text snippets.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextFrame : GTLRObject

/** Bounding polygon of the detected text for this frame. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingPoly *rotatedBoundingBox;

/** Timestamp of this frame. */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for text detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextSegment : GTLRObject

/**
 *  Confidence for the track of detected text. It is calculated as the highest
 *  over all frames where OCR detected text appears.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Information related to the frames where OCR detected text appears. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextFrame *> *frames;

/** Video segment where a text snippet was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

@end


/**
 *  For tracking related features. An object at time_offset with attributes, and
 *  located with normalized_bounding_box.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TimestampedObject : GTLRObject

/** Optional. The attributes of the object in the bounding box. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedAttribute *> *attributes;

/** Optional. The detected landmarks. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedLandmark *> *landmarks;

/** Normalized Bounding box in a frame, where the object is located. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1NormalizedBoundingBox *normalizedBoundingBox;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this object.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  A track of an object instance.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Track : GTLRObject

/** Optional. Attributes in the track level. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1DetectedAttribute *> *attributes;

/**
 *  Optional. The confidence score of the tracked object.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment of a track. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

/** The object with timestamp and attributes per frame in the track. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TimestampedObject *> *timestampedObjects;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress : GTLRObject

/**
 *  Specifies which feature is being tracked if the request contains more than
 *  one feature.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_ExplicitContentDetection
 *        Explicit content detection. (Value: "EXPLICIT_CONTENT_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_FaceDetection
 *        Human face detection. (Value: "FACE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_FeatureUnspecified
 *        Unspecified. (Value: "FEATURE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_LabelDetection
 *        Label detection. Detect objects, such as dog or flower. (Value:
 *        "LABEL_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_LogoRecognition
 *        Logo detection, tracking, and recognition. (Value: "LOGO_RECOGNITION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_ObjectTracking
 *        Object detection and tracking. (Value: "OBJECT_TRACKING")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_PersonDetection
 *        Person detection. (Value: "PERSON_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_ShotChangeDetection
 *        Shot change detection. (Value: "SHOT_CHANGE_DETECTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_SpeechTranscription
 *        Speech transcription. (Value: "SPEECH_TRANSCRIPTION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress_Feature_TextDetection
 *        OCR text detection and tracking. (Value: "TEXT_DETECTION")
 */
@property(nonatomic, copy, nullable) NSString *feature;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far. Guaranteed to be 100 when fully
 *  processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/**
 *  Specifies which segment is being tracked if the request contains more than
 *  one segment.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation *explicitAnnotation;

/** Deprecated. Please use `face_detection_annotations` instead. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceAnnotation *> *faceAnnotations;

/** Face detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceDetectionAnnotation *> *faceDetectionAnnotations;

/**
 *  Label annotations on frame level. There is exactly one element for each
 *  unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in [Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Annotations for list of logos detected, tracked and recognized in video.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LogoRecognitionAnnotation *> *logoRecognitionAnnotations;

/** Annotations for list of objects detected and tracked in video. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingAnnotation *> *objectAnnotations;

/** Person detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1PersonDetectionAnnotation *> *personDetectionAnnotations;

/** Video segment on which the annotation is run. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

/**
 *  Topical label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *segmentLabelAnnotations;

/**
 *  Presence label annotations on video level or user-specified segment level.
 *  There is exactly one element for each unique label. Compared to the existing
 *  topical `segment_label_annotations`, this field presents more fine-grained,
 *  segment-level labels detected in video content and is made available only
 *  when the client sets `LabelDetectionConfig.model` to "builtin/latest" in the
 *  request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *segmentPresenceLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *shotAnnotations;

/**
 *  Topical label annotations on shot level. There is exactly one element for
 *  each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *shotLabelAnnotations;

/**
 *  Presence label annotations on shot level. There is exactly one element for
 *  each unique label. Compared to the existing topical
 *  `shot_label_annotations`, this field presents more fine-grained, shot-level
 *  labels detected in video content and is made available only when the client
 *  sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *shotPresenceLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscription *> *speechTranscriptions;

/**
 *  OCR text detection and tracking. Annotations for list of detected text
 *  snippets. Each will have list of frame information associated with it.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextAnnotation *> *textAnnotations;

@end


/**
 *  Video context and/or feature-specific parameters.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext : GTLRObject

/** Config for EXPLICIT_CONTENT_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig *explicitContentDetectionConfig;

/** Config for FACE_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1FaceDetectionConfig *faceDetectionConfig;

/** Config for LABEL_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig *labelDetectionConfig;

/** Config for OBJECT_TRACKING. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ObjectTrackingConfig *objectTrackingConfig;

/** Config for PERSON_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1PersonDetectionConfig *personDetectionConfig;

/**
 *  Video segments to annotate. The segments may overlap and are not required to
 *  be contiguous or span the whole video. If unspecified, each video is treated
 *  as a single segment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *segments;

/** Config for SHOT_CHANGE_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig *shotChangeDetectionConfig;

/** Config for SPEECH_TRANSCRIPTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1SpeechTranscriptionConfig *speechTranscriptionConfig;

/** Config for TEXT_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1TextDetectionConfig *textDetectionConfig;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such as
 *  `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1WordInfo : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is set only for the top alternative. This field is not
 *  guaranteed to be accurate and users should not rely on it to be always
 *  provided. The default of 0.0 is a sentinel value indicating `confidence` was
 *  not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from 1 up to diarization_speaker_count,
 *  and is only set if speaker diarization is enabled.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  The request message for Operations.CancelOperation.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningCancelOperationRequest : GTLRObject
@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleLongrunningOperation *> *operations;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress. If
 *  `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time. Some
 *  services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the `name`
 *  should be a resource name ending with `operations/{unique_id}`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx` is the
 *  original method name. For example, if the original method name is
 *  `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time. Some
 *  services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx` is the
 *  original method name. For example, if the original method name is
 *  `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response : GTLRObject
@end


/**
 *  A generic empty message that you can re-use to avoid defining duplicated
 *  empty messages in your APIs. A typical example is to use it as the request
 *  or the response type of an API method. For instance: service Foo { rpc
 *  Bar(google.protobuf.Empty) returns (google.protobuf.Empty); } The JSON
 *  representation for `Empty` is empty JSON object `{}`.
 */
@interface GTLRCloudVideoIntelligence_GoogleProtobufEmpty : GTLRObject
@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different programming environments, including REST APIs and RPC APIs. It is
 *  used by [gRPC](https://github.com/grpc). Each `Status` message contains
 *  three pieces of data: error code, error message, and error details. You can
 *  find out more about this error model and how to work with it in the [API
 *  Design Guide](https://cloud.google.com/apis/design/errors).
 */
@interface GTLRCloudVideoIntelligence_GoogleRpcStatus : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item : GTLRObject
@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
