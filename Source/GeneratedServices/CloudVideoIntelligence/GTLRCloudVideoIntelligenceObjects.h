// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Cloud Video Intelligence API (videointelligence/v1)
// Description:
//   Cloud Video Intelligence API.
// Documentation:
//   https://cloud.google.com/video-intelligence/docs/

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAttribute;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext;
@class GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata;
@class GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response;
@class GTLRCloudVideoIntelligence_GoogleRpcStatus;
@class GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest.features

/** Value: "EXPLICIT_CONTENT_DETECTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ExplicitContentDetection;
/** Value: "FEATURE_UNSPECIFIED" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_FeatureUnspecified;
/** Value: "LABEL_DETECTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_LabelDetection;
/** Value: "SHOT_CHANGE_DETECTION" */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest_Features_ShotChangeDetection;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation.level

/**
 *  Frame-level. Corresponds to a single video frame.
 *
 *  Value: "FRAME_LEVEL"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_FrameLevel;
/**
 *  Unspecified.
 *
 *  Value: "LABEL_LEVEL_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_LabelLevelUnspecified;
/**
 *  Segment-level. Corresponds to one of `AnnotateSpec.segments`.
 *
 *  Value: "SEGMENT_LEVEL"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_SegmentLevel;
/**
 *  Shot-level. Corresponds to a single shot (i.e. a series of frames
 *  without a major camera position or background change).
 *
 *  Value: "SHOT_LEVEL"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_ShotLevel;
/**
 *  Video-level. Corresponds to the whole video.
 *
 *  Value: "VIDEO_LEVEL"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_VideoLevel;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation.adult

/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Unknown;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation.medical

/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Unknown;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation.racy

/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Unknown;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation.spoof

/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Unknown;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation.violent

/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Unknown;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig.labelDetectionMode

/**
 *  Detect frame-level labels.
 *
 *  Value: "FRAME_MODE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_FrameMode;
/**
 *  Unspecified.
 *
 *  Value: "LABEL_DETECTION_MODE_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_LabelDetectionModeUnspecified;
/**
 *  Detect both shot-level and frame-level labels.
 *
 *  Value: "SHOT_AND_FRAME_MODE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotAndFrameMode;
/**
 *  Detect shot-level labels.
 *
 *  Value: "SHOT_MODE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotMode;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute.emotion

/**
 *  Amusement.
 *
 *  Value: "AMUSEMENT"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Amusement;
/**
 *  Anger.
 *
 *  Value: "ANGER"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Anger;
/**
 *  Concentration.
 *
 *  Value: "CONCENTRATION"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Concentration;
/**
 *  Contentment.
 *
 *  Value: "CONTENTMENT"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Contentment;
/**
 *  Desire.
 *
 *  Value: "DESIRE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Desire;
/**
 *  Disappointment.
 *
 *  Value: "DISAPPOINTMENT"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Disappointment;
/**
 *  Disgust.
 *
 *  Value: "DISGUST"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Disgust;
/**
 *  Elation.
 *
 *  Value: "ELATION"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Elation;
/**
 *  Embarrassment.
 *
 *  Value: "EMBARRASSMENT"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Embarrassment;
/**
 *  Unspecified emotion.
 *
 *  Value: "EMOTION_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_EmotionUnspecified;
/**
 *  Interest.
 *
 *  Value: "INTEREST"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Interest;
/**
 *  Pride.
 *
 *  Value: "PRIDE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Pride;
/**
 *  Sadness.
 *
 *  Value: "SADNESS"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Sadness;
/**
 *  Surprise.
 *
 *  Value: "SURPRISE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Surprise;

// ----------------------------------------------------------------------------
// GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame.pornographyLikelihood

/**
 *  Unspecified likelihood.
 *
 *  Value: "LIKELIHOOD_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified;
/**
 *  Likely.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Likely;
/**
 *  Possible.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Possible;
/**
 *  Unlikely.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Unlikely;
/**
 *  Very likely.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely;
/**
 *  Very unlikely.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely;

/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation request.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoRequest : GTLRObject

/** Requested video annotation features. */
@property(nonatomic, strong, nullable) NSArray<NSString *> *features;

/**
 *  The video data bytes.
 *  If unset, the input video(s) should be specified via `input_uri`.
 *  If set, `input_uri` should be unset.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *inputContent;

/**
 *  Input video location. Currently, only
 *  [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
 *  supported, which must be specified in the following format:
 *  `gs://bucket-id/object-id` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](/storage/docs/reference-uris).
 *  A video URI may include wildcards in `object-id`, and thus identify
 *  multiple videos. Supported wildcards: '*' to match 0 or more characters;
 *  '?' to match 1 character. If unset, the input video should be embedded
 *  in the request as `input_content`. If set, `input_content` should be unset.
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Optional cloud region where annotation should take place. Supported cloud
 *  regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
 *  is specified, a region will be determined based on video file location.
 */
@property(nonatomic, copy, nullable) NSString *locationId;

/**
 *  Optional location where the output (in JSON format) should be stored.
 *  Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
 *  URIs are supported, which must be specified in the following format:
 *  `gs://bucket-id/object-id` (other URI formats return
 *  google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *outputUri;

/** Additional video context and/or feature-specific parameters. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext *videoContext;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelAnnotation : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

/** Where the label was detected and with what confidence. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation *> *locations;

@end


/**
 *  Label location.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Label level.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_FrameLevel
 *        Frame-level. Corresponds to a single video frame. (Value:
 *        "FRAME_LEVEL")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_LabelLevelUnspecified
 *        Unspecified. (Value: "LABEL_LEVEL_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_SegmentLevel
 *        Segment-level. Corresponds to one of `AnnotateSpec.segments`. (Value:
 *        "SEGMENT_LEVEL")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_ShotLevel
 *        Shot-level. Corresponds to a single shot (i.e. a series of frames
 *        without a major camera position or background change). (Value:
 *        "SHOT_LEVEL")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelLocation_Level_VideoLevel
 *        Video-level. Corresponds to the whole video. (Value: "VIDEO_LEVEL")
 */
@property(nonatomic, copy, nullable) NSString *level;

/**
 *  Video segment. Set to [-1, -1] for video-level labels.
 *  Set to [timestamp, timestamp] for frame-level labels.
 *  Otherwise, corresponds to one of `AnnotateSpec.segments`
 *  (if specified) or to shot boundaries (if requested).
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoSegment *segment;

@end


/**
 *  Safe search annotation (based on per-frame visual signals only).
 *  If no unsafe content has been detected in a frame, no annotations
 *  are present for that frame. If only some types of unsafe content
 *  have been detected in a frame, the likelihood is set to `UNKNOWN`
 *  for all other types of unsafe content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation : GTLRObject

/**
 *  Likelihood of adult content.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Unknown
 *        Unknown likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Adult_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *adult;

/**
 *  Likelihood of medical content.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Unknown
 *        Unknown likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Medical_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *medical;

/**
 *  Likelihood of racy content.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Unknown
 *        Unknown likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Racy_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *racy;

/**
 *  Likelihood that an obvious modification was made to the original
 *  version to make it appear funny or offensive.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Unknown
 *        Unknown likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Spoof_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *spoof;

/**
 *  Video time offset in microseconds.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *timeOffset;

/**
 *  Likelihood of violent content.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Unknown
 *        Unknown likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation_Violent_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *violent;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far.
 *  Guaranteed to be 100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/** Label annotations. There is exactly one element for each unique label. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1LabelAnnotation *> *labelAnnotations;

/** Safe search annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1SafeSearchAnnotation *> *safeSearchAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoSegment *> *shotAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta1VideoSegment : GTLRObject

/**
 *  End offset in microseconds (inclusive). Unset means 0.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *endTimeOffset;

/**
 *  Start offset in microseconds (inclusive). Unset means 0.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *startTimeOffset;

@end


/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame *> *frames;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment *> *segments;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *segment;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far.
 *  Guaranteed to be 100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2ExplicitContentAnnotation *explicitAnnotation;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2LabelAnnotation *> *shotLabelAnnotations;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1beta2VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame *> *frames;

@end


/**
 *  Config for EXPLICIT_CONTENT_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig : GTLRObject

/**
 *  Model to use for explicit content detection.
 *  Supported values: "builtin/stable" (the default if unset) and
 *  "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment *> *segments;

@end


/**
 *  Config for LABEL_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig : GTLRObject

/**
 *  What labels should be detected with LABEL_DETECTION, in addition to
 *  video-level labels or segment-level labels.
 *  If unspecified, defaults to `SHOT_MODE`.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_FrameMode
 *        Detect frame-level labels. (Value: "FRAME_MODE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_LabelDetectionModeUnspecified
 *        Unspecified. (Value: "LABEL_DETECTION_MODE_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotAndFrameMode
 *        Detect both shot-level and frame-level labels. (Value:
 *        "SHOT_AND_FRAME_MODE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig_LabelDetectionMode_ShotMode
 *        Detect shot-level labels. (Value: "SHOT_MODE")
 */
@property(nonatomic, copy, nullable) NSString *labelDetectionMode;

/**
 *  Model to use for label detection.
 *  Supported values: "builtin/stable" (the default if unset) and
 *  "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

/**
 *  Whether the video has been shot from a stationary (i.e. non-moving) camera.
 *  When set to true, might improve detection accuracy for moving objects.
 *  Should be used with `SHOT_AND_FRAME_MODE` enabled.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *stationaryCamera;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *segment;

@end


/**
 *  Video annotation progress. Included in the `metadata`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1AnnotateVideoProgress : GTLRObject

/** Progress metadata for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress *> *annotationProgress;

@end


/**
 *  Video annotation response. Included in the `response`
 *  field of the `Operation` returned by the `GetOperation`
 *  call of the `google::longrunning::Operations` service.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1AnnotateVideoResponse : GTLRObject

/** Annotation results for all videos specified in `AnnotateVideoRequest`. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults *> *annotationResults;

@end


/**
 *  Emotion attribute.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute : GTLRObject

/**
 *  Emotion entry.
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Amusement
 *        Amusement. (Value: "AMUSEMENT")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Anger
 *        Anger. (Value: "ANGER")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Concentration
 *        Concentration. (Value: "CONCENTRATION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Contentment
 *        Contentment. (Value: "CONTENTMENT")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Desire
 *        Desire. (Value: "DESIRE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Disappointment
 *        Disappointment. (Value: "DISAPPOINTMENT")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Disgust
 *        Disgust. (Value: "DISGUST")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Elation
 *        Elation. (Value: "ELATION")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Embarrassment
 *        Embarrassment. (Value: "EMBARRASSMENT")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_EmotionUnspecified
 *        Unspecified emotion. (Value: "EMOTION_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Interest
 *        Interest. (Value: "INTEREST")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Pride
 *        Pride. (Value: "PRIDE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Sadness
 *        Sadness. (Value: "SADNESS")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute_Emotion_Surprise
 *        Surprise. (Value: "SURPRISE")
 */
@property(nonatomic, copy, nullable) NSString *emotion;

/**
 *  Confidence score.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *score;

@end


/**
 *  Detected entity from video analysis.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity : GTLRObject

/**
 *  Textual description, e.g. `Fixed-gear bicycle`.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  Opaque entity ID. Some IDs may be available in
 *  [Google Knowledge Graph Search
 *  API](https://developers.google.com/knowledge-graph/).
 */
@property(nonatomic, copy, nullable) NSString *entityId;

/** Language code for `description` in BCP-47 format. */
@property(nonatomic, copy, nullable) NSString *languageCode;

@end


/**
 *  Explicit content annotation (based on per-frame visual signals only).
 *  If no explicit content has been detected in a frame, no annotations are
 *  present for that frame.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation : GTLRObject

/** All video frames where explicit content was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame *> *frames;

@end


/**
 *  Video frame level annotation results for explicit content.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame : GTLRObject

/**
 *  Likelihood of the pornography content..
 *
 *  Likely values:
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_LikelihoodUnspecified
 *        Unspecified likelihood. (Value: "LIKELIHOOD_UNSPECIFIED")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Likely
 *        Likely. (Value: "LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Possible
 *        Possible. (Value: "POSSIBLE")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_Unlikely
 *        Unlikely. (Value: "UNLIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryLikely
 *        Very likely. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentFrame_PornographyLikelihood_VeryUnlikely
 *        Very unlikely. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *pornographyLikelihood;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Face detection annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation : GTLRObject

/** All video frames where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionFrame *> *frames;

/** All video segments where a face was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceSegment *> *segments;

@end


/**
 *  Face detection attribute.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAttribute : GTLRObject

/** Emotion attributes. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1EmotionAttribute *> *emotions;

/** Normalized Bounding box. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox *normalizedBoundingBox;

@end


/**
 *  Video frame level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionFrame : GTLRObject

/**
 *  Face attributes in a frame.
 *  There can be more than one attributes if the same face is detected in
 *  multiple locations within the current frame.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAttribute *> *attributes;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for face detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceSegment : GTLRObject

/** Video segment where a face was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

@end


/**
 *  Label annotation.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation : GTLRObject

/**
 *  Common categories for the detected entity.
 *  E.g. when the label is `Terrier` the category is likely `dog`. And in some
 *  cases there might be more than one categories e.g. `Terrier` could also be
 *  a `pet`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *> *categoryEntities;

/** Detected entity. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1Entity *entity;

/** All video frames where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame *> *frames;

/** All video segments where a label was detected. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment *> *segments;

@end


/**
 *  Video frame level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelFrame : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time-offset, relative to the beginning of the video, corresponding to the
 *  video frame for this location.
 */
@property(nonatomic, strong, nullable) GTLRDuration *timeOffset;

@end


/**
 *  Video segment level annotation results for label detection.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelSegment : GTLRObject

/**
 *  Confidence that the label is accurate. Range: [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/** Video segment where a label was detected. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *segment;

@end


/**
 *  Normalized bounding box.
 *  The normalized vertex coordinates are relative to the original image.
 *  Range: [0, 1].
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1NormalizedBoundingBox : GTLRObject

/**
 *  Bottom Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *bottom;

/**
 *  Left X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *left;

/**
 *  Right X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *right;

/**
 *  Top Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *top;

@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative : GTLRObject

/**
 *  Output only. The confidence estimate between 0.0 and 1.0. A higher number
 *  indicates an estimated greater likelihood that the recognized words are
 *  correct. This field is typically provided only for the top hypothesis, and
 *  only for `is_final=true` results. Clients should not rely on the
 *  `confidence` field as it is not guaranteed to be accurate or consistent.
 *  The default of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Output only. Transcript text representing the words that the user spoke.
 */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  Output only. A list of word-specific information for each recognized word.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo *> *words;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription : GTLRObject

/**
 *  Output only. May contain one or more recognition hypotheses (up to the
 *  maximum specified in `max_alternatives`).
 *  These alternatives are ordered in terms of accuracy, with the top (first)
 *  alternative being the most probable, as ranked by the recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechRecognitionAlternative *> *alternatives;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far.
 *  Guaranteed to be 100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation *explicitAnnotation;

/** Face detection annotations. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation *> *faceDetectionAnnotations;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1LabelAnnotation *> *shotLabelAnnotations;

/** Speech transcription. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1SpeechTranscription *> *speechTranscriptions;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  Word-specific information for recognized words. Word information is only
 *  included in the response when certain request parameters are set, such
 *  as `enable_word_time_offsets`.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1p1beta1WordInfo : GTLRObject

/**
 *  Output only. Time offset relative to the beginning of the audio, and
 *  corresponding to the end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. Time offset relative to the beginning of the audio, and
 *  corresponding to the start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** Output only. The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end


/**
 *  Config for SHOT_CHANGE_DETECTION.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig : GTLRObject

/**
 *  Model to use for shot change detection.
 *  Supported values: "builtin/stable" (the default if unset) and
 *  "builtin/latest".
 */
@property(nonatomic, copy, nullable) NSString *model;

@end


/**
 *  Annotation progress for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationProgress : GTLRObject

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Approximate percentage processed thus far.
 *  Guaranteed to be 100 when fully processed.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/** Time of the most recent update. */
@property(nonatomic, strong, nullable) GTLRDateTime *updateTime;

@end


/**
 *  Annotation results for a single video.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoAnnotationResults : GTLRObject

/**
 *  If set, indicates an error. Note that for a single `AnnotateVideoRequest`
 *  some videos may succeed and some may fail.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/** Explicit content annotation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentAnnotation *explicitAnnotation;

/**
 *  Label annotations on frame level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *frameLabelAnnotations;

/**
 *  Video file location in
 *  [Google Cloud Storage](https://cloud.google.com/storage/).
 */
@property(nonatomic, copy, nullable) NSString *inputUri;

/**
 *  Label annotations on video level or user specified segment level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *segmentLabelAnnotations;

/** Shot annotations. Each shot is represented as a video segment. */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *shotAnnotations;

/**
 *  Label annotations on shot level.
 *  There is exactly one element for each unique label.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelAnnotation *> *shotLabelAnnotations;

@end


/**
 *  Video context and/or feature-specific parameters.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoContext : GTLRObject

/** Config for EXPLICIT_CONTENT_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ExplicitContentDetectionConfig *explicitContentDetectionConfig;

/** Config for LABEL_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1LabelDetectionConfig *labelDetectionConfig;

/**
 *  Video segments to annotate. The segments may overlap and are not required
 *  to be contiguous or span the whole video. If unspecified, each video
 *  is treated as a single segment.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment *> *segments;

/** Config for SHOT_CHANGE_DETECTION. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1ShotChangeDetectionConfig *shotChangeDetectionConfig;

@end


/**
 *  Video segment.
 */
@interface GTLRCloudVideoIntelligence_GoogleCloudVideointelligenceV1VideoSegment : GTLRObject

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the end of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTimeOffset;

/**
 *  Time-offset, relative to the beginning of the video,
 *  corresponding to the start of the segment (inclusive).
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTimeOffset;

@end


/**
 *  The request message for Operations.CancelOperation.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningCancelOperationRequest : GTLRObject
@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleLongrunningOperation *> *operations;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress.
 *  If `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleRpcStatus *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the
 *  `name` should have the format of `operations/some/unique/name`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time.
 *  Some services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Metadata : GTLRObject
@end


/**
 *  The normal response of the operation in case of success. If the original
 *  method returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx`
 *  is the original method name. For example, if the original method name
 *  is `TakeSnapshot()`, the inferred response type is
 *  `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleLongrunningOperation_Response : GTLRObject
@end


/**
 *  A generic empty message that you can re-use to avoid defining duplicated
 *  empty messages in your APIs. A typical example is to use it as the request
 *  or the response type of an API method. For instance:
 *  service Foo {
 *  rpc Bar(google.protobuf.Empty) returns (google.protobuf.Empty);
 *  }
 *  The JSON representation for `Empty` is empty JSON object `{}`.
 */
@interface GTLRCloudVideoIntelligence_GoogleProtobufEmpty : GTLRObject
@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different
 *  programming environments, including REST APIs and RPC APIs. It is used by
 *  [gRPC](https://github.com/grpc). The error model is designed to be:
 *  - Simple to use and understand for most users
 *  - Flexible enough to meet unexpected needs
 *  # Overview
 *  The `Status` message contains three pieces of data: error code, error
 *  message,
 *  and error details. The error code should be an enum value of
 *  google.rpc.Code, but it may accept additional error codes if needed. The
 *  error message should be a developer-facing English message that helps
 *  developers *understand* and *resolve* the error. If a localized user-facing
 *  error message is needed, put the localized message in the error details or
 *  localize it in the client. The optional error details may contain arbitrary
 *  information about the error. There is a predefined set of error detail types
 *  in the package `google.rpc` that can be used for common error conditions.
 *  # Language mapping
 *  The `Status` message is the logical representation of the error model, but
 *  it
 *  is not necessarily the actual wire format. When the `Status` message is
 *  exposed in different client libraries and different wire protocols, it can
 *  be
 *  mapped differently. For example, it will likely be mapped to some exceptions
 *  in Java, but more likely mapped to some error codes in C.
 *  # Other uses
 *  The error model and the `Status` message can be used in a variety of
 *  environments, either with or without APIs, to provide a
 *  consistent developer experience across different environments.
 *  Example uses of this error model include:
 *  - Partial errors. If a service needs to return partial errors to the client,
 *  it may embed the `Status` in the normal response to indicate the partial
 *  errors.
 *  - Workflow errors. A typical workflow has multiple steps. Each step may
 *  have a `Status` message for error reporting.
 *  - Batch operations. If a client uses batch request and batch response, the
 *  `Status` message should be used directly inside batch response, one for
 *  each error sub-response.
 *  - Asynchronous operations. If an API call embeds asynchronous operation
 *  results in its response, the status of those operations should be
 *  represented directly using the `Status` message.
 *  - Logging. If some API errors are stored in logs, the message `Status` could
 *  be used directly after any stripping needed for security/privacy reasons.
 */
@interface GTLRCloudVideoIntelligence_GoogleRpcStatus : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRCloudVideoIntelligence_GoogleRpcStatus_Details_Item : GTLRObject
@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
