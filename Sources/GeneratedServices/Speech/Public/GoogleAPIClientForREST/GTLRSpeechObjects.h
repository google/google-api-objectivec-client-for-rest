// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Cloud Speech-to-Text API (speech/v1)
// Description:
//   Converts audio to text by applying powerful neural network models.
// Documentation:
//   https://cloud.google.com/speech-to-text/docs/quickstart-protocol

#import <GoogleAPIClientForREST/GTLRObject.h>

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRSpeech_ABNFGrammar;
@class GTLRSpeech_Adaptation;
@class GTLRSpeech_AdaptationInfo;
@class GTLRSpeech_ClassItem;
@class GTLRSpeech_Context;
@class GTLRSpeech_CustomClass;
@class GTLRSpeech_CustomClass_Annotations;
@class GTLRSpeech_Operation;
@class GTLRSpeech_Operation_Metadata;
@class GTLRSpeech_Operation_Response;
@class GTLRSpeech_Phrase;
@class GTLRSpeech_PhraseSet;
@class GTLRSpeech_PhraseSet_Annotations;
@class GTLRSpeech_RecognitionAlternative;
@class GTLRSpeech_RecognitionAudio;
@class GTLRSpeech_RecognitionConfig;
@class GTLRSpeech_RecognitionMetadata;
@class GTLRSpeech_RecognitionResult;
@class GTLRSpeech_SpeakerDiarizationConfig;
@class GTLRSpeech_Status;
@class GTLRSpeech_Status_Details_Item;
@class GTLRSpeech_TranscriptOutputConfig;
@class GTLRSpeech_WordInfo;

// Generated comments include content from the discovery document; avoid them
// causing warnings since clang's checks are some what arbitrary.
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdocumentation"
#pragma clang diagnostic ignored "-Wdeprecated-declarations"

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRSpeech_CustomClass.state

/**
 *  The normal and active state.
 *
 *  Value: "ACTIVE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_CustomClass_State_Active;
/**
 *  This CustomClass has been deleted.
 *
 *  Value: "DELETED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_CustomClass_State_Deleted;
/**
 *  Unspecified state. This is only used/useful for distinguishing unset values.
 *
 *  Value: "STATE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_CustomClass_State_StateUnspecified;

// ----------------------------------------------------------------------------
// GTLRSpeech_PhraseSet.state

/**
 *  The normal and active state.
 *
 *  Value: "ACTIVE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_PhraseSet_State_Active;
/**
 *  This CustomClass has been deleted.
 *
 *  Value: "DELETED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_PhraseSet_State_Deleted;
/**
 *  Unspecified state. This is only used/useful for distinguishing unset values.
 *
 *  Value: "STATE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_PhraseSet_State_StateUnspecified;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionConfig.encoding

/**
 *  Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
 *
 *  Value: "AMR"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Amr;
/**
 *  Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
 *
 *  Value: "AMR_WB"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_AmrWb;
/**
 *  Not specified.
 *
 *  Value: "ENCODING_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified;
/**
 *  `FLAC` (Free Lossless Audio Codec) is the recommended encoding because it is
 *  lossless--therefore recognition is not compromised--and requires only about
 *  half the bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
 *  24-bit samples, however, not all fields in `STREAMINFO` are supported.
 *
 *  Value: "FLAC"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Flac;
/**
 *  Uncompressed 16-bit signed little-endian samples (Linear PCM).
 *
 *  Value: "LINEAR16"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Linear16;
/**
 *  8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
 *
 *  Value: "MULAW"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_Mulaw;
/**
 *  Opus encoded audio frames in Ogg container
 *  ([OggOpus](https://wiki.xiph.org/OggOpus)). `sample_rate_hertz` must be one
 *  of 8000, 12000, 16000, 24000, or 48000.
 *
 *  Value: "OGG_OPUS"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_OggOpus;
/**
 *  Although the use of lossy encodings is not recommended, if a very low
 *  bitrate encoding is required, `OGG_OPUS` is highly preferred over Speex
 *  encoding. The [Speex](https://speex.org/) encoding supported by Cloud Speech
 *  API has a header byte in each block, as in MIME type
 *  `audio/x-speex-with-header-byte`. It is a variant of the RTP Speex encoding
 *  defined in [RFC 5574](https://tools.ietf.org/html/rfc5574). The stream is a
 *  sequence of blocks, one block per RTP packet. Each block starts with a byte
 *  containing the length of the block, in bytes, followed by one or more frames
 *  of Speex data, padded to an integral number of bytes (octets) as specified
 *  in RFC 5574. In other words, each RTP header is replaced with a single byte
 *  containing the block length. Only Speex wideband is supported.
 *  `sample_rate_hertz` must be 16000.
 *
 *  Value: "SPEEX_WITH_HEADER_BYTE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte;
/**
 *  Opus encoded audio frames in WebM container
 *  ([OggOpus](https://wiki.xiph.org/OggOpus)). `sample_rate_hertz` must be one
 *  of 8000, 12000, 16000, 24000, or 48000.
 *
 *  Value: "WEBM_OPUS"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionConfig_Encoding_WebmOpus;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.interactionType

/**
 *  Transcribe speech to text to create a written document, such as a
 *  text-message, email or report.
 *
 *  Value: "DICTATION"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Dictation;
/**
 *  Multiple people in a conversation or discussion. For example in a meeting
 *  with two or more people actively participating. Typically all the primary
 *  people speaking would be in the same room (if not, see PHONE_CALL)
 *
 *  Value: "DISCUSSION"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Discussion;
/**
 *  Use case is either unknown or is something other than one of the other
 *  values below.
 *
 *  Value: "INTERACTION_TYPE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_InteractionTypeUnspecified;
/**
 *  A phone-call or video-conference in which two or more people, who are not in
 *  the same room, are actively participating.
 *
 *  Value: "PHONE_CALL"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_PhoneCall;
/**
 *  One or more persons lecturing or presenting to others, mostly uninterrupted.
 *
 *  Value: "PRESENTATION"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Presentation;
/**
 *  Professionally produced audio (eg. TV Show, Podcast).
 *
 *  Value: "PROFESSIONALLY_PRODUCED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_ProfessionallyProduced;
/**
 *  Transcribe voice commands, such as for controlling a device.
 *
 *  Value: "VOICE_COMMAND"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceCommand;
/**
 *  A recorded message intended for another person to listen to.
 *
 *  Value: "VOICEMAIL"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_Voicemail;
/**
 *  Transcribe spoken questions and queries into text.
 *
 *  Value: "VOICE_SEARCH"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceSearch;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.microphoneDistance

/**
 *  The speaker is more than 3 meters away from the microphone.
 *
 *  Value: "FARFIELD"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Farfield;
/**
 *  Audio type is not known.
 *
 *  Value: "MICROPHONE_DISTANCE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified;
/**
 *  The speaker if within 3 meters of the microphone.
 *
 *  Value: "MIDFIELD"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Midfield;
/**
 *  The audio was captured from a closely placed microphone. Eg. phone,
 *  dictaphone, or handheld microphone. Generally if there speaker is within 1
 *  meter of the microphone.
 *
 *  Value: "NEARFIELD"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Nearfield;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.originalMediaType

/**
 *  The speech data is an audio recording.
 *
 *  Value: "AUDIO"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Audio;
/**
 *  Unknown original media type.
 *
 *  Value: "ORIGINAL_MEDIA_TYPE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified;
/**
 *  The speech data originally recorded on a video.
 *
 *  Value: "VIDEO"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Video;

// ----------------------------------------------------------------------------
// GTLRSpeech_RecognitionMetadata.recordingDeviceType

/**
 *  Speech was recorded indoors.
 *
 *  Value: "OTHER_INDOOR_DEVICE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice;
/**
 *  Speech was recorded outdoors.
 *
 *  Value: "OTHER_OUTDOOR_DEVICE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice;
/**
 *  Speech was recorded using a personal computer or tablet.
 *
 *  Value: "PC"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Pc;
/**
 *  Speech was recorded over a phone line.
 *
 *  Value: "PHONE_LINE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_PhoneLine;
/**
 *  The recording device is unknown.
 *
 *  Value: "RECORDING_DEVICE_TYPE_UNSPECIFIED"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified;
/**
 *  Speech was recorded on a smartphone.
 *
 *  Value: "SMARTPHONE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Smartphone;
/**
 *  Speech was recorded in a vehicle.
 *
 *  Value: "VEHICLE"
 */
FOUNDATION_EXTERN NSString * const kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Vehicle;

/**
 *  GTLRSpeech_ABNFGrammar
 */
@interface GTLRSpeech_ABNFGrammar : GTLRObject

/**
 *  All declarations and rules of an ABNF grammar broken up into multiple
 *  strings that will end up concatenated.
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *abnfStrings;

@end


/**
 *  Speech adaptation configuration.
 */
@interface GTLRSpeech_Adaptation : GTLRObject

/**
 *  Augmented Backus-Naur form (ABNF) is a standardized grammar notation
 *  comprised by a set of derivation rules. See specifications:
 *  https://www.w3.org/TR/speech-grammar
 */
@property(nonatomic, strong, nullable) GTLRSpeech_ABNFGrammar *abnfGrammar;

/**
 *  A collection of custom classes. To specify the classes inline, leave the
 *  class' `name` blank and fill in the rest of its fields, giving it a unique
 *  `custom_class_id`. Refer to the inline defined class in phrase hints by its
 *  `custom_class_id`.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_CustomClass *> *customClasses;

/** A collection of phrase set resource names to use. */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phraseSetReferences;

/**
 *  A collection of phrase sets. To specify the hints inline, leave the phrase
 *  set's `name` blank and fill in the rest of its fields. Any phrase set can
 *  use any custom class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_PhraseSet *> *phraseSets;

@end


/**
 *  Information on speech adaptation use in results
 */
@interface GTLRSpeech_AdaptationInfo : GTLRObject

/**
 *  Whether there was a timeout when applying speech adaptation. If true,
 *  adaptation had no effect in the response transcript.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *adaptationTimeout;

/**
 *  If set, returns a message specifying which part of the speech adaptation
 *  request timed out.
 */
@property(nonatomic, copy, nullable) NSString *timeoutMessage;

@end


/**
 *  An item of the class.
 */
@interface GTLRSpeech_ClassItem : GTLRObject

/** The class item's value. */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases in the results.
 */
@interface GTLRSpeech_Context : GTLRObject

/**
 *  Hint Boost. Positive value will increase the probability that a specific
 *  phrase will be recognized over other similar sounding phrases. The higher
 *  the boost, the higher the chance of false positive recognition as well.
 *  Negative boost values would correspond to anti-biasing. Anti-biasing is not
 *  enabled, so negative boost will simply be ignored. Though `boost` can accept
 *  a wide range of positive values, most use cases are best served with values
 *  between 0 and 20. We recommend using a binary search approach to finding the
 *  optimal value for your use case.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *boost;

/**
 *  A list of strings containing words and phrases "hints" so that the speech
 *  recognition is more likely to recognize them. This can be used to improve
 *  the accuracy for specific words and phrases, for example, if specific
 *  commands are typically spoken by the user. This can also be used to add
 *  additional words to the vocabulary of the recognizer. See [usage
 *  limits](https://cloud.google.com/speech-to-text/quotas#content). List items
 *  can also be set to classes for groups of words that represent common
 *  concepts that occur in natural language. For example, rather than providing
 *  phrase hints for every month of the year, using the $MONTH class improves
 *  the likelihood of correctly transcribing audio that includes months.
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *phrases;

@end


/**
 *  Message sent by the client for the `CreateCustomClass` method.
 */
@interface GTLRSpeech_CreateCustomClassRequest : GTLRObject

/** Required. The custom class to create. */
@property(nonatomic, strong, nullable) GTLRSpeech_CustomClass *customClass;

/**
 *  Required. The ID to use for the custom class, which will become the final
 *  component of the custom class' resource name. This value should restrict to
 *  letters, numbers, and hyphens, with the first character a letter, the last a
 *  letter or a number, and be 4-63 characters.
 */
@property(nonatomic, copy, nullable) NSString *customClassId;

@end


/**
 *  Message sent by the client for the `CreatePhraseSet` method.
 */
@interface GTLRSpeech_CreatePhraseSetRequest : GTLRObject

/** Required. The phrase set to create. */
@property(nonatomic, strong, nullable) GTLRSpeech_PhraseSet *phraseSet;

/**
 *  Required. The ID to use for the phrase set, which will become the final
 *  component of the phrase set's resource name. This value should restrict to
 *  letters, numbers, and hyphens, with the first character a letter, the last a
 *  letter or a number, and be 4-63 characters.
 */
@property(nonatomic, copy, nullable) NSString *phraseSetId;

@end


/**
 *  A set of words or phrases that represents a common concept likely to appear
 *  in your audio, for example a list of passenger ship names. CustomClass items
 *  can be substituted into placeholders that you set in PhraseSet phrases.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "items" property.
 */
@interface GTLRSpeech_CustomClass : GTLRCollectionObject

/**
 *  Output only. Allows users to store small amounts of arbitrary data. Both the
 *  key and the value must be 63 characters or less each. At most 100
 *  annotations. This field is not used.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_CustomClass_Annotations *annotations;

/**
 *  If this custom class is a resource, the custom_class_id is the resource id
 *  of the CustomClass. Case sensitive.
 */
@property(nonatomic, copy, nullable) NSString *customClassId;

/**
 *  Output only. The time at which this resource was requested for deletion.
 *  This field is not used.
 */
@property(nonatomic, strong, nullable) GTLRDateTime *deleteTime;

/**
 *  Output only. User-settable, human-readable name for the CustomClass. Must be
 *  63 characters or less. This field is not used.
 */
@property(nonatomic, copy, nullable) NSString *displayName;

/**
 *  Output only. This checksum is computed by the server based on the value of
 *  other fields. This may be sent on update, undelete, and delete requests to
 *  ensure the client has an up-to-date value before proceeding. This field is
 *  not used.
 */
@property(nonatomic, copy, nullable) NSString *ETag;

/**
 *  Output only. The time at which this resource will be purged. This field is
 *  not used.
 */
@property(nonatomic, strong, nullable) GTLRDateTime *expireTime;

/**
 *  A collection of class items.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_ClassItem *> *items;

/**
 *  Output only. The [KMS key
 *  name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which
 *  the content of the ClassItem is encrypted. The expected format is
 *  `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
 */
@property(nonatomic, copy, nullable) NSString *kmsKeyName;

/**
 *  Output only. The [KMS key version
 *  name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions)
 *  with which content of the ClassItem is encrypted. The expected format is
 *  `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.
 */
@property(nonatomic, copy, nullable) NSString *kmsKeyVersionName;

/** The resource name of the custom class. */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  Output only. Whether or not this CustomClass is in the process of being
 *  updated. This field is not used.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *reconciling;

/**
 *  Output only. The CustomClass lifecycle state. This field is not used.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_CustomClass_State_Active The normal and active state.
 *        (Value: "ACTIVE")
 *    @arg @c kGTLRSpeech_CustomClass_State_Deleted This CustomClass has been
 *        deleted. (Value: "DELETED")
 *    @arg @c kGTLRSpeech_CustomClass_State_StateUnspecified Unspecified state.
 *        This is only used/useful for distinguishing unset values. (Value:
 *        "STATE_UNSPECIFIED")
 */
@property(nonatomic, copy, nullable) NSString *state;

/**
 *  Output only. System-assigned unique identifier for the CustomClass. This
 *  field is not used.
 */
@property(nonatomic, copy, nullable) NSString *uid;

@end


/**
 *  Output only. Allows users to store small amounts of arbitrary data. Both the
 *  key and the value must be 63 characters or less each. At most 100
 *  annotations. This field is not used.
 *
 *  @note This class is documented as having more properties of NSString. Use @c
 *        -additionalJSONKeys and @c -additionalPropertyForName: to get the list
 *        of properties and then fetch them; or @c -additionalProperties to
 *        fetch them all at once.
 */
@interface GTLRSpeech_CustomClass_Annotations : GTLRObject
@end


/**
 *  A generic empty message that you can re-use to avoid defining duplicated
 *  empty messages in your APIs. A typical example is to use it as the request
 *  or the response type of an API method. For instance: service Foo { rpc
 *  Bar(google.protobuf.Empty) returns (google.protobuf.Empty); }
 */
@interface GTLRSpeech_Empty : GTLRObject
@end


/**
 *  Message returned to the client by the `ListCustomClasses` method.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "customClasses" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListCustomClassesResponse : GTLRCollectionObject

/**
 *  The custom classes.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_CustomClass *> *customClasses;

/**
 *  A token, which can be sent as `page_token` to retrieve the next page. If
 *  this field is omitted, there are no subsequent pages.
 */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

@end


/**
 *  The response message for Operations.ListOperations.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "operations" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListOperationsResponse : GTLRCollectionObject

/** The standard List next-page token. */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  A list of operations that matches the specified filter in the request.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Operation *> *operations;

@end


/**
 *  Message returned to the client by the `ListPhraseSet` method.
 *
 *  @note This class supports NSFastEnumeration and indexed subscripting over
 *        its "phraseSets" property. If returned as the result of a query, it
 *        should support automatic pagination (when @c shouldFetchNextPages is
 *        enabled).
 */
@interface GTLRSpeech_ListPhraseSetResponse : GTLRCollectionObject

/**
 *  A token, which can be sent as `page_token` to retrieve the next page. If
 *  this field is omitted, there are no subsequent pages.
 */
@property(nonatomic, copy, nullable) NSString *nextPageToken;

/**
 *  The phrase set.
 *
 *  @note This property is used to support NSFastEnumeration and indexed
 *        subscripting on this class.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_PhraseSet *> *phraseSets;

@end


/**
 *  Describes the progress of a long-running `LongRunningRecognize` call. It is
 *  included in the `metadata` field of the `Operation` returned by the
 *  `GetOperation` call of the `google::longrunning::Operations` service.
 */
@interface GTLRSpeech_LongRunningRecognizeMetadata : GTLRObject

/** Time of the most recent processing update. */
@property(nonatomic, strong, nullable) GTLRDateTime *lastUpdateTime;

/**
 *  Approximate percentage of audio processed thus far. Guaranteed to be 100
 *  when the audio is fully processed and the results are available.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *progressPercent;

/** Time when the request was received. */
@property(nonatomic, strong, nullable) GTLRDateTime *startTime;

/**
 *  Output only. The URI of the audio file being transcribed. Empty if the audio
 *  was sent as byte content.
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  The top-level message sent by the client for the `LongRunningRecognize`
 *  method.
 */
@interface GTLRSpeech_LongRunningRecognizeRequest : GTLRObject

/** Required. The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  Required. Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

/**
 *  Optional. Specifies an optional destination for the recognition results.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_TranscriptOutputConfig *outputConfig;

@end


/**
 *  The only message returned to the client by the `LongRunningRecognize`
 *  method. It contains the result as zero or more sequential
 *  `SpeechRecognitionResult` messages. It is included in the `result.response`
 *  field of the `Operation` returned by the `GetOperation` call of the
 *  `google::longrunning::Operations` service.
 */
@interface GTLRSpeech_LongRunningRecognizeResponse : GTLRObject

/** Original output config if present in the request. */
@property(nonatomic, strong, nullable) GTLRSpeech_TranscriptOutputConfig *outputConfig;

/** If the transcript output fails this field contains the relevant error. */
@property(nonatomic, strong, nullable) GTLRSpeech_Status *outputError;

/**
 *  The ID associated with the request. This is a unique ID specific only to the
 *  given request.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *requestId;

/**
 *  Sequential list of transcription results corresponding to sequential
 *  portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

/** Provides information on speech adaptation behavior in response */
@property(nonatomic, strong, nullable) GTLRSpeech_AdaptationInfo *speechAdaptationInfo;

/** When available, billed audio seconds for the corresponding request. */
@property(nonatomic, strong, nullable) GTLRDuration *totalBilledTime;

@end


/**
 *  This resource represents a long-running operation that is the result of a
 *  network API call.
 */
@interface GTLRSpeech_Operation : GTLRObject

/**
 *  If the value is `false`, it means the operation is still in progress. If
 *  `true`, the operation is completed, and either `error` or `response` is
 *  available.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *done;

/** The error result of the operation in case of failure or cancellation. */
@property(nonatomic, strong, nullable) GTLRSpeech_Status *error;

/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time. Some
 *  services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Metadata *metadata;

/**
 *  The server-assigned name, which is only unique within the same service that
 *  originally returns it. If you use the default HTTP mapping, the `name`
 *  should be a resource name ending with `operations/{unique_id}`.
 */
@property(nonatomic, copy, nullable) NSString *name;

/**
 *  The normal, successful response of the operation. If the original method
 *  returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx` is the
 *  original method name. For example, if the original method name is
 *  `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Operation_Response *response;

@end


/**
 *  Service-specific metadata associated with the operation. It typically
 *  contains progress information and common metadata such as create time. Some
 *  services might not provide such metadata. Any method that returns a
 *  long-running operation should document the metadata type, if any.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Metadata : GTLRObject
@end


/**
 *  The normal, successful response of the operation. If the original method
 *  returns no data on success, such as `Delete`, the response is
 *  `google.protobuf.Empty`. If the original method is standard
 *  `Get`/`Create`/`Update`, the response should be the resource. For other
 *  methods, the response should have the type `XxxResponse`, where `Xxx` is the
 *  original method name. For example, if the original method name is
 *  `TakeSnapshot()`, the inferred response type is `TakeSnapshotResponse`.
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Operation_Response : GTLRObject
@end


/**
 *  A phrases containing words and phrase "hints" so that the speech recognition
 *  is more likely to recognize them. This can be used to improve the accuracy
 *  for specific words and phrases, for example, if specific commands are
 *  typically spoken by the user. This can also be used to add additional words
 *  to the vocabulary of the recognizer. See [usage
 *  limits](https://cloud.google.com/speech-to-text/quotas#content). List items
 *  can also include pre-built or custom classes containing groups of words that
 *  represent common concepts that occur in natural language. For example,
 *  rather than providing a phrase hint for every month of the year (e.g. "i was
 *  born in january", "i was born in febuary", ...), use the pre-built `$MONTH`
 *  class improves the likelihood of correctly transcribing audio that includes
 *  months (e.g. "i was born in $month"). To refer to pre-built classes, use the
 *  class' symbol prepended with `$` e.g. `$MONTH`. To refer to custom classes
 *  that were defined inline in the request, set the class's `custom_class_id`
 *  to a string unique to all class resources and inline classes. Then use the
 *  class' id wrapped in $`{...}` e.g. "${my-months}". To refer to custom
 *  classes resources, use the class' id wrapped in `${}` (e.g. `${my-months}`).
 *  Speech-to-Text supports three locations: `global`, `us` (US North America),
 *  and `eu` (Europe). If you are calling the `speech.googleapis.com` endpoint,
 *  use the `global` location. To specify a region, use a [regional
 *  endpoint](https://cloud.google.com/speech-to-text/docs/endpoints) with
 *  matching `us` or `eu` location value.
 */
@interface GTLRSpeech_Phrase : GTLRObject

/**
 *  Hint Boost. Overrides the boost set at the phrase set level. Positive value
 *  will increase the probability that a specific phrase will be recognized over
 *  other similar sounding phrases. The higher the boost, the higher the chance
 *  of false positive recognition as well. Negative boost will simply be
 *  ignored. Though `boost` can accept a wide range of positive values, most use
 *  cases are best served with values between 0 and 20. We recommend using a
 *  binary search approach to finding the optimal value for your use case as
 *  well as adding phrases both with and without boost to your requests.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *boost;

/** The phrase itself. */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  Provides "hints" to the speech recognizer to favor specific words and
 *  phrases in the results.
 */
@interface GTLRSpeech_PhraseSet : GTLRObject

/**
 *  Output only. Allows users to store small amounts of arbitrary data. Both the
 *  key and the value must be 63 characters or less each. At most 100
 *  annotations. This field is not used.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_PhraseSet_Annotations *annotations;

/**
 *  Hint Boost. Positive value will increase the probability that a specific
 *  phrase will be recognized over other similar sounding phrases. The higher
 *  the boost, the higher the chance of false positive recognition as well.
 *  Negative boost values would correspond to anti-biasing. Anti-biasing is not
 *  enabled, so negative boost will simply be ignored. Though `boost` can accept
 *  a wide range of positive values, most use cases are best served with values
 *  between 0 (exclusive) and 20. We recommend using a binary search approach to
 *  finding the optimal value for your use case as well as adding phrases both
 *  with and without boost to your requests.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *boost;

/**
 *  Output only. The time at which this resource was requested for deletion.
 *  This field is not used.
 */
@property(nonatomic, strong, nullable) GTLRDateTime *deleteTime;

/**
 *  Output only. User-settable, human-readable name for the PhraseSet. Must be
 *  63 characters or less. This field is not used.
 */
@property(nonatomic, copy, nullable) NSString *displayName;

/**
 *  Output only. This checksum is computed by the server based on the value of
 *  other fields. This may be sent on update, undelete, and delete requests to
 *  ensure the client has an up-to-date value before proceeding. This field is
 *  not used.
 */
@property(nonatomic, copy, nullable) NSString *ETag;

/**
 *  Output only. The time at which this resource will be purged. This field is
 *  not used.
 */
@property(nonatomic, strong, nullable) GTLRDateTime *expireTime;

/**
 *  Output only. The [KMS key
 *  name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which
 *  the content of the PhraseSet is encrypted. The expected format is
 *  `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.
 */
@property(nonatomic, copy, nullable) NSString *kmsKeyName;

/**
 *  Output only. The [KMS key version
 *  name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions)
 *  with which content of the PhraseSet is encrypted. The expected format is
 *  `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.
 */
@property(nonatomic, copy, nullable) NSString *kmsKeyVersionName;

/** The resource name of the phrase set. */
@property(nonatomic, copy, nullable) NSString *name;

/** A list of word and phrases. */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Phrase *> *phrases;

/**
 *  Output only. Whether or not this PhraseSet is in the process of being
 *  updated. This field is not used.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *reconciling;

/**
 *  Output only. The CustomClass lifecycle state. This field is not used.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_PhraseSet_State_Active The normal and active state.
 *        (Value: "ACTIVE")
 *    @arg @c kGTLRSpeech_PhraseSet_State_Deleted This CustomClass has been
 *        deleted. (Value: "DELETED")
 *    @arg @c kGTLRSpeech_PhraseSet_State_StateUnspecified Unspecified state.
 *        This is only used/useful for distinguishing unset values. (Value:
 *        "STATE_UNSPECIFIED")
 */
@property(nonatomic, copy, nullable) NSString *state;

/**
 *  Output only. System-assigned unique identifier for the PhraseSet. This field
 *  is not used.
 */
@property(nonatomic, copy, nullable) NSString *uid;

@end


/**
 *  Output only. Allows users to store small amounts of arbitrary data. Both the
 *  key and the value must be 63 characters or less each. At most 100
 *  annotations. This field is not used.
 *
 *  @note This class is documented as having more properties of NSString. Use @c
 *        -additionalJSONKeys and @c -additionalPropertyForName: to get the list
 *        of properties and then fetch them; or @c -additionalProperties to
 *        fetch them all at once.
 */
@interface GTLRSpeech_PhraseSet_Annotations : GTLRObject
@end


/**
 *  Alternative hypotheses (a.k.a. n-best list).
 */
@interface GTLRSpeech_RecognitionAlternative : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number indicates an
 *  estimated greater likelihood that the recognized words are correct. This
 *  field is set only for the top alternative of a non-streaming result or, of a
 *  streaming result where `is_final=true`. This field is not guaranteed to be
 *  accurate and users should not rely on it to be always provided. The default
 *  of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Transcript text representing the words that the user spoke. In languages
 *  that use spaces to separate words, the transcript might have a leading space
 *  if it isn't the first result. You can concatenate each result to obtain the
 *  full transcript without using a separator.
 */
@property(nonatomic, copy, nullable) NSString *transcript;

/**
 *  A list of word-specific information for each recognized word. Note: When
 *  `enable_speaker_diarization` is true, you will see all the words from the
 *  beginning of the audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_WordInfo *> *words;

@end


/**
 *  Contains audio data in the encoding specified in the `RecognitionConfig`.
 *  Either `content` or `uri` must be supplied. Supplying both or neither
 *  returns google.rpc.Code.INVALID_ARGUMENT. See [content
 *  limits](https://cloud.google.com/speech-to-text/quotas#content).
 */
@interface GTLRSpeech_RecognitionAudio : GTLRObject

/**
 *  The audio data bytes encoded as specified in `RecognitionConfig`. Note: as
 *  with all bytes fields, proto buffers use a pure binary representation,
 *  whereas JSON representations use base64.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *content;

/**
 *  URI that points to a file that contains audio data bytes as specified in
 *  `RecognitionConfig`. The file must not be compressed (for example, gzip).
 *  Currently, only Google Cloud Storage URIs are supported, which must be
 *  specified in the following format: `gs://bucket_name/object_name` (other URI
 *  formats return google.rpc.Code.INVALID_ARGUMENT). For more information, see
 *  [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
 */
@property(nonatomic, copy, nullable) NSString *uri;

@end


/**
 *  Provides information to the recognizer that specifies how to process the
 *  request.
 */
@interface GTLRSpeech_RecognitionConfig : GTLRObject

/**
 *  Speech adaptation configuration improves the accuracy of speech recognition.
 *  For more information, see the [speech
 *  adaptation](https://cloud.google.com/speech-to-text/docs/adaptation)
 *  documentation. When speech adaptation is set it supersedes the
 *  `speech_contexts` field.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_Adaptation *adaptation;

/**
 *  A list of up to 3 additional
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tags,
 *  listing possible alternative languages of the supplied audio. See [Language
 *  Support](https://cloud.google.com/speech-to-text/docs/languages) for a list
 *  of the currently supported language codes. If alternative languages are
 *  listed, recognition result will contain recognition in the most likely
 *  language detected including the main language_code. The recognition result
 *  will include the language tag of the language detected in the audio. Note:
 *  This feature is only supported for Voice Command and Voice Search use cases
 *  and performance may vary for other use cases (e.g., phone call
 *  transcription).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *alternativeLanguageCodes;

/**
 *  The number of channels in the input audio data. ONLY set this for
 *  MULTI-CHANNEL recognition. Valid values for LINEAR16, OGG_OPUS and FLAC are
 *  `1`-`8`. Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is
 *  only `1`. If `0` or omitted, defaults to one channel (mono). Note: We only
 *  recognize the first channel by default. To perform independent recognition
 *  on each channel set `enable_separate_recognition_per_channel` to 'true'.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *audioChannelCount;

/**
 *  Config to enable speaker diarization and set additional parameters to make
 *  diarization better suited for your application. Note: When this is enabled,
 *  we send all the words from the beginning of the audio for the top
 *  alternative in every consecutive STREAMING responses. This is done in order
 *  to improve our speaker tags as our models learn to identify the speakers in
 *  the conversation over time. For non-streaming requests, the diarization
 *  results will be provided only in the top alternative of the FINAL
 *  SpeechRecognitionResult.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_SpeakerDiarizationConfig *diarizationConfig;

/**
 *  If 'true', adds punctuation to recognition result hypotheses. This feature
 *  is only available in select languages. Setting this for requests in other
 *  languages has no effect at all. The default 'false' value does not add
 *  punctuation to result hypotheses.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableAutomaticPunctuation;

/**
 *  This needs to be set to `true` explicitly and `audio_channel_count` > 1 to
 *  get each channel recognized separately. The recognition result will contain
 *  a `channel_tag` field to state which channel that result belongs to. If this
 *  is not true, we will only recognize the first channel. The request is billed
 *  cumulatively for all channels recognized: `audio_channel_count` multiplied
 *  by the length of the audio.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSeparateRecognitionPerChannel;

/**
 *  The spoken emoji behavior for the call If not set, uses default behavior
 *  based on model of choice If 'true', adds spoken emoji formatting for the
 *  request. This will replace spoken emojis with the corresponding Unicode
 *  symbols in the final transcript. If 'false', spoken emojis are not replaced.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSpokenEmojis;

/**
 *  The spoken punctuation behavior for the call If not set, uses default
 *  behavior based on model of choice e.g. command_and_search will enable spoken
 *  punctuation by default If 'true', replaces spoken punctuation with the
 *  corresponding symbols in the request. For example, "how are you question
 *  mark" becomes "how are you?". See
 *  https://cloud.google.com/speech-to-text/docs/spoken-punctuation for support.
 *  If 'false', spoken punctuation is not replaced.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSpokenPunctuation;

/**
 *  If `true`, the top result includes a list of words and the confidence for
 *  those words. If `false`, no word-level confidence information is returned.
 *  The default is `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordConfidence;

/**
 *  If `true`, the top result includes a list of words and the start and end
 *  time offsets (timestamps) for those words. If `false`, no word-level time
 *  offset information is returned. The default is `false`.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableWordTimeOffsets;

/**
 *  Encoding of audio data sent in all `RecognitionAudio` messages. This field
 *  is optional for `FLAC` and `WAV` audio files and required for all other
 *  audio formats. For details, see AudioEncoding.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Amr Adaptive Multi-Rate
 *        Narrowband codec. `sample_rate_hertz` must be 8000. (Value: "AMR")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_AmrWb Adaptive Multi-Rate
 *        Wideband codec. `sample_rate_hertz` must be 16000. (Value: "AMR_WB")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_EncodingUnspecified Not
 *        specified. (Value: "ENCODING_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Flac `FLAC` (Free Lossless
 *        Audio Codec) is the recommended encoding because it is
 *        lossless--therefore recognition is not compromised--and requires only
 *        about half the bandwidth of `LINEAR16`. `FLAC` stream encoding
 *        supports 16-bit and 24-bit samples, however, not all fields in
 *        `STREAMINFO` are supported. (Value: "FLAC")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Linear16 Uncompressed
 *        16-bit signed little-endian samples (Linear PCM). (Value: "LINEAR16")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_Mulaw 8-bit samples that
 *        compand 14-bit audio samples using G.711 PCMU/mu-law. (Value: "MULAW")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_OggOpus Opus encoded audio
 *        frames in Ogg container ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *        `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or
 *        48000. (Value: "OGG_OPUS")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_SpeexWithHeaderByte
 *        Although the use of lossy encodings is not recommended, if a very low
 *        bitrate encoding is required, `OGG_OPUS` is highly preferred over
 *        Speex encoding. The [Speex](https://speex.org/) encoding supported by
 *        Cloud Speech API has a header byte in each block, as in MIME type
 *        `audio/x-speex-with-header-byte`. It is a variant of the RTP Speex
 *        encoding defined in [RFC 5574](https://tools.ietf.org/html/rfc5574).
 *        The stream is a sequence of blocks, one block per RTP packet. Each
 *        block starts with a byte containing the length of the block, in bytes,
 *        followed by one or more frames of Speex data, padded to an integral
 *        number of bytes (octets) as specified in RFC 5574. In other words,
 *        each RTP header is replaced with a single byte containing the block
 *        length. Only Speex wideband is supported. `sample_rate_hertz` must be
 *        16000. (Value: "SPEEX_WITH_HEADER_BYTE")
 *    @arg @c kGTLRSpeech_RecognitionConfig_Encoding_WebmOpus Opus encoded audio
 *        frames in WebM container ([OggOpus](https://wiki.xiph.org/OggOpus)).
 *        `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or
 *        48000. (Value: "WEBM_OPUS")
 */
@property(nonatomic, copy, nullable) NSString *encoding;

/**
 *  Required. The language of the supplied audio as a
 *  [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 *  Example: "en-US". See [Language
 *  Support](https://cloud.google.com/speech-to-text/docs/languages) for a list
 *  of the currently supported language codes.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  Maximum number of recognition hypotheses to be returned. Specifically, the
 *  maximum number of `SpeechRecognitionAlternative` messages within each
 *  `SpeechRecognitionResult`. The server may return fewer than
 *  `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
 *  return a maximum of one. If omitted, will return a maximum of one.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxAlternatives;

/** Metadata regarding this request. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionMetadata *metadata;

/**
 *  Which model to select for the given request. Select the model best suited to
 *  your domain to get best results. If a model is not explicitly specified,
 *  then we auto-select a model based on the parameters in the
 *  RecognitionConfig. *Model* *Description* latest_long Best for long form
 *  content like media or conversation. latest_short Best for short form content
 *  like commands or single shot directed speech. command_and_search Best for
 *  short queries such as voice commands or voice search. phone_call Best for
 *  audio that originated from a phone call (typically recorded at an 8khz
 *  sampling rate). video Best for audio that originated from video or includes
 *  multiple speakers. Ideally the audio is recorded at a 16khz or greater
 *  sampling rate. This is a premium model that costs more than the standard
 *  rate. default Best for audio that is not one of the specific audio models.
 *  For example, long-form audio. Ideally the audio is high-fidelity, recorded
 *  at a 16khz or greater sampling rate. medical_conversation Best for audio
 *  that originated from a conversation between a medical provider and patient.
 *  medical_dictation Best for audio that originated from dictation notes by a
 *  medical provider.
 */
@property(nonatomic, copy, nullable) NSString *model;

/**
 *  If set to `true`, the server will attempt to filter out profanities,
 *  replacing all but the initial character in each filtered word with
 *  asterisks, e.g. "f***". If set to `false` or omitted, profanities won't be
 *  filtered out.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *profanityFilter;

/**
 *  Sample rate in Hertz of the audio data sent in all `RecognitionAudio`
 *  messages. Valid values are: 8000-48000. 16000 is optimal. For best results,
 *  set the sampling rate of the audio source to 16000 Hz. If that's not
 *  possible, use the native sample rate of the audio source (instead of
 *  re-sampling). This field is optional for FLAC and WAV audio files, but is
 *  required for all other audio formats. For details, see AudioEncoding.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *sampleRateHertz;

/**
 *  Array of SpeechContext. A means to provide context to assist the speech
 *  recognition. For more information, see [speech
 *  adaptation](https://cloud.google.com/speech-to-text/docs/adaptation).
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Context *> *speechContexts;

/**
 *  Set to true to use an enhanced model for speech recognition. If
 *  `use_enhanced` is set to true and the `model` field is not set, then an
 *  appropriate enhanced model is chosen if an enhanced model exists for the
 *  audio. If `use_enhanced` is true and an enhanced version of the specified
 *  model does not exist, then the speech is recognized using the standard
 *  version of the specified model.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *useEnhanced;

@end


/**
 *  Description of audio data to be recognized.
 */
GTLR_DEPRECATED
@interface GTLRSpeech_RecognitionMetadata : GTLRObject

/**
 *  Description of the content. Eg. "Recordings of federal supreme court
 *  hearings from 2012".
 */
@property(nonatomic, copy, nullable) NSString *audioTopic;

/**
 *  The industry vertical to which this speech recognition request most closely
 *  applies. This is most indicative of the topics contained in the audio. Use
 *  the 6-digit NAICS code to identify the industry vertical - see
 *  https://www.naics.com/search/.
 *
 *  Uses NSNumber of unsignedIntValue.
 */
@property(nonatomic, strong, nullable) NSNumber *industryNaicsCodeOfAudio;

/**
 *  The use case most closely describing the audio content to be recognized.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Dictation
 *        Transcribe speech to text to create a written document, such as a
 *        text-message, email or report. (Value: "DICTATION")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Discussion
 *        Multiple people in a conversation or discussion. For example in a
 *        meeting with two or more people actively participating. Typically all
 *        the primary people speaking would be in the same room (if not, see
 *        PHONE_CALL) (Value: "DISCUSSION")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_InteractionTypeUnspecified
 *        Use case is either unknown or is something other than one of the other
 *        values below. (Value: "INTERACTION_TYPE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_PhoneCall A
 *        phone-call or video-conference in which two or more people, who are
 *        not in the same room, are actively participating. (Value:
 *        "PHONE_CALL")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Presentation One
 *        or more persons lecturing or presenting to others, mostly
 *        uninterrupted. (Value: "PRESENTATION")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_ProfessionallyProduced
 *        Professionally produced audio (eg. TV Show, Podcast). (Value:
 *        "PROFESSIONALLY_PRODUCED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceCommand
 *        Transcribe voice commands, such as for controlling a device. (Value:
 *        "VOICE_COMMAND")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_Voicemail A
 *        recorded message intended for another person to listen to. (Value:
 *        "VOICEMAIL")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_InteractionType_VoiceSearch
 *        Transcribe spoken questions and queries into text. (Value:
 *        "VOICE_SEARCH")
 */
@property(nonatomic, copy, nullable) NSString *interactionType;

/**
 *  The audio type that most closely describes the audio being recognized.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Farfield The
 *        speaker is more than 3 meters away from the microphone. (Value:
 *        "FARFIELD")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_MicrophoneDistanceUnspecified
 *        Audio type is not known. (Value: "MICROPHONE_DISTANCE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Midfield The
 *        speaker if within 3 meters of the microphone. (Value: "MIDFIELD")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_MicrophoneDistance_Nearfield The
 *        audio was captured from a closely placed microphone. Eg. phone,
 *        dictaphone, or handheld microphone. Generally if there speaker is
 *        within 1 meter of the microphone. (Value: "NEARFIELD")
 */
@property(nonatomic, copy, nullable) NSString *microphoneDistance;

/**
 *  The original media the speech was recorded on.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Audio The speech
 *        data is an audio recording. (Value: "AUDIO")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_OriginalMediaType_OriginalMediaTypeUnspecified
 *        Unknown original media type. (Value:
 *        "ORIGINAL_MEDIA_TYPE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_OriginalMediaType_Video The speech
 *        data originally recorded on a video. (Value: "VIDEO")
 */
@property(nonatomic, copy, nullable) NSString *originalMediaType;

/**
 *  Mime type of the original audio file. For example `audio/m4a`,
 *  `audio/x-alaw-basic`, `audio/mp3`, `audio/3gpp`. A list of possible audio
 *  mime types is maintained at
 *  http://www.iana.org/assignments/media-types/media-types.xhtml#audio
 */
@property(nonatomic, copy, nullable) NSString *originalMimeType;

/**
 *  The device used to make the recording. Examples 'Nexus 5X' or 'Polycom
 *  SoundStation IP 6000' or 'POTS' or 'VoIP' or 'Cardioid Microphone'.
 */
@property(nonatomic, copy, nullable) NSString *recordingDeviceName;

/**
 *  The type of device the speech was recorded with.
 *
 *  Likely values:
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherIndoorDevice
 *        Speech was recorded indoors. (Value: "OTHER_INDOOR_DEVICE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_OtherOutdoorDevice
 *        Speech was recorded outdoors. (Value: "OTHER_OUTDOOR_DEVICE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Pc Speech was
 *        recorded using a personal computer or tablet. (Value: "PC")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_PhoneLine
 *        Speech was recorded over a phone line. (Value: "PHONE_LINE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_RecordingDeviceTypeUnspecified
 *        The recording device is unknown. (Value:
 *        "RECORDING_DEVICE_TYPE_UNSPECIFIED")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Smartphone
 *        Speech was recorded on a smartphone. (Value: "SMARTPHONE")
 *    @arg @c kGTLRSpeech_RecognitionMetadata_RecordingDeviceType_Vehicle Speech
 *        was recorded in a vehicle. (Value: "VEHICLE")
 */
@property(nonatomic, copy, nullable) NSString *recordingDeviceType;

@end


/**
 *  A speech recognition result corresponding to a portion of the audio.
 */
@interface GTLRSpeech_RecognitionResult : GTLRObject

/**
 *  May contain one or more recognition hypotheses (up to the maximum specified
 *  in `max_alternatives`). These alternatives are ordered in terms of accuracy,
 *  with the top (first) alternative being the most probable, as ranked by the
 *  recognizer.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionAlternative *> *alternatives;

/**
 *  For multi-channel audio, this is the channel number corresponding to the
 *  recognized result for the audio from that channel. For audio_channel_count =
 *  N, its output values can range from '1' to 'N'.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *channelTag;

/**
 *  Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
 *  language tag of the language in this result. This language code was detected
 *  to have the most likelihood of being spoken in the audio.
 */
@property(nonatomic, copy, nullable) NSString *languageCode;

/**
 *  Time offset of the end of this result relative to the beginning of the
 *  audio.
 */
@property(nonatomic, strong, nullable) GTLRDuration *resultEndTime;

@end


/**
 *  The top-level message sent by the client for the `Recognize` method.
 */
@interface GTLRSpeech_RecognizeRequest : GTLRObject

/** Required. The audio data to be recognized. */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionAudio *audio;

/**
 *  Required. Provides information to the recognizer that specifies how to
 *  process the request.
 */
@property(nonatomic, strong, nullable) GTLRSpeech_RecognitionConfig *config;

@end


/**
 *  The only message returned to the client by the `Recognize` method. It
 *  contains the result as zero or more sequential `SpeechRecognitionResult`
 *  messages.
 */
@interface GTLRSpeech_RecognizeResponse : GTLRObject

/**
 *  The ID associated with the request. This is a unique ID specific only to the
 *  given request.
 *
 *  Uses NSNumber of longLongValue.
 */
@property(nonatomic, strong, nullable) NSNumber *requestId;

/**
 *  Sequential list of transcription results corresponding to sequential
 *  portions of audio.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_RecognitionResult *> *results;

/** Provides information on adaptation behavior in response */
@property(nonatomic, strong, nullable) GTLRSpeech_AdaptationInfo *speechAdaptationInfo;

/** When available, billed audio seconds for the corresponding request. */
@property(nonatomic, strong, nullable) GTLRDuration *totalBilledTime;

@end


/**
 *  Config to enable speaker diarization.
 */
@interface GTLRSpeech_SpeakerDiarizationConfig : GTLRObject

/**
 *  If 'true', enables speaker detection for each recognized word in the top
 *  alternative of the recognition result using a speaker_label provided in the
 *  WordInfo.
 *
 *  Uses NSNumber of boolValue.
 */
@property(nonatomic, strong, nullable) NSNumber *enableSpeakerDiarization;

/**
 *  Maximum number of speakers in the conversation. This range gives you more
 *  flexibility by allowing the system to automatically determine the correct
 *  number of speakers. If not set, the default value is 6.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxSpeakerCount;

/**
 *  Minimum number of speakers in the conversation. This range gives you more
 *  flexibility by allowing the system to automatically determine the correct
 *  number of speakers. If not set, the default value is 2.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *minSpeakerCount;

/**
 *  Output only. Unused.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag GTLR_DEPRECATED;

@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different programming environments, including REST APIs and RPC APIs. It is
 *  used by [gRPC](https://github.com/grpc). Each `Status` message contains
 *  three pieces of data: error code, error message, and error details. You can
 *  find out more about this error model and how to work with it in the [API
 *  Design Guide](https://cloud.google.com/apis/design/errors).
 */
@interface GTLRSpeech_Status : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There is a common set of
 *  message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRSpeech_Status_Details_Item *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRSpeech_Status_Details_Item
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRSpeech_Status_Details_Item : GTLRObject
@end


/**
 *  Specifies an optional destination for the recognition results.
 */
@interface GTLRSpeech_TranscriptOutputConfig : GTLRObject

/**
 *  Specifies a Cloud Storage URI for the recognition results. Must be specified
 *  in the format: `gs://bucket_name/object_name`, and the bucket must already
 *  exist.
 */
@property(nonatomic, copy, nullable) NSString *gcsUri;

@end


/**
 *  Word-specific information for recognized words.
 */
@interface GTLRSpeech_WordInfo : GTLRObject

/**
 *  The confidence estimate between 0.0 and 1.0. A higher number indicates an
 *  estimated greater likelihood that the recognized words are correct. This
 *  field is set only for the top alternative of a non-streaming result or, of a
 *  streaming result where `is_final=true`. This field is not guaranteed to be
 *  accurate and users should not rely on it to be always provided. The default
 *  of 0.0 is a sentinel value indicating `confidence` was not set.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  end of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *endTime;

/**
 *  Output only. A label value assigned for every unique speaker within the
 *  audio. This field specifies which speaker was detected to have spoken this
 *  word. For some models, like medical_conversation this can be actual speaker
 *  role, for example "patient" or "provider", but generally this would be a
 *  number identifying a speaker. This field is only set if
 *  enable_speaker_diarization = 'true' and only for the top alternative.
 */
@property(nonatomic, copy, nullable) NSString *speakerLabel;

/**
 *  Output only. A distinct integer value is assigned for every speaker within
 *  the audio. This field specifies which one of those speakers was detected to
 *  have spoken this word. Value ranges from '1' to diarization_speaker_count.
 *  speaker_tag is set if enable_speaker_diarization = 'true' and only for the
 *  top alternative. Note: Use speaker_label instead.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *speakerTag GTLR_DEPRECATED;

/**
 *  Time offset relative to the beginning of the audio, and corresponding to the
 *  start of the spoken word. This field is only set if
 *  `enable_word_time_offsets=true` and only in the top hypothesis. This is an
 *  experimental feature and the accuracy of the time offset can vary.
 */
@property(nonatomic, strong, nullable) GTLRDuration *startTime;

/** The word corresponding to this set of information. */
@property(nonatomic, copy, nullable) NSString *word;

@end

NS_ASSUME_NONNULL_END

#pragma clang diagnostic pop
